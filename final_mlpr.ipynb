{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-05-15T19:41:40.569444Z","iopub.status.busy":"2025-05-15T19:41:40.569073Z","iopub.status.idle":"2025-05-15T19:41:45.250134Z","shell.execute_reply":"2025-05-15T19:41:45.249333Z","shell.execute_reply.started":"2025-05-15T19:41:40.569426Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/spotify-challenge/md5sums\n","/kaggle/input/spotify-challenge/README.md\n","/kaggle/input/spotify-challenge/license.txt\n","/kaggle/input/spotify-challenge/stats.txt\n","/kaggle/input/spotify-challenge/src/check.py\n","/kaggle/input/spotify-challenge/src/descriptions.py\n","/kaggle/input/spotify-challenge/src/stats.py\n","/kaggle/input/spotify-challenge/src/show.py\n","/kaggle/input/spotify-challenge/src/deeper_stats.py\n","/kaggle/input/spotify-challenge/src/print.py\n","/kaggle/input/spotify-challenge/data/mpd.slice.35000-35999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.98000-98999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.405000-405999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.601000-601999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.567000-567999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.421000-421999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.983000-983999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.434000-434999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.315000-315999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.797000-797999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.194000-194999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.981000-981999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.548000-548999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.618000-618999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.943000-943999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.907000-907999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.805000-805999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.481000-481999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.208000-208999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.574000-574999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.352000-352999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.850000-850999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.716000-716999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.667000-667999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.691000-691999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.706000-706999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.811000-811999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.484000-484999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.901000-901999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.899000-899999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.628000-628999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.76000-76999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.178000-178999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.401000-401999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.843000-843999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.426000-426999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.919000-919999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.156000-156999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.733000-733999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.550000-550999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.737000-737999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.323000-323999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.344000-344999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.928000-928999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.917000-917999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.579000-579999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.662000-662999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.788000-788999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.837000-837999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.244000-244999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.228000-228999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.66000-66999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.814000-814999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.476000-476999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.975000-975999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.866000-866999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.12000-12999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.389000-389999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.522000-522999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.276000-276999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.894000-894999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.286000-286999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.589000-589999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.397000-397999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.684000-684999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.590000-590999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.530000-530999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.415000-415999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.65000-65999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.185000-185999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.255000-255999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.715000-715999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.16000-16999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.468000-468999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.408000-408999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.61000-61999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.923000-923999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.930000-930999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.433000-433999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.927000-927999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.639000-639999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.841000-841999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.263000-263999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.501000-501999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.772000-772999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.39000-39999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.206000-206999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.467000-467999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.746000-746999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.36000-36999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.182000-182999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.536000-536999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.248000-248999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.158000-158999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.830000-830999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.474000-474999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.625000-625999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.30000-30999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.488000-488999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.168000-168999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.766000-766999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.931000-931999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.480000-480999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.6000-6999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.880000-880999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.317000-317999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.151000-151999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.821000-821999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.606000-606999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.465000-465999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.319000-319999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.149000-149999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.277000-277999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.493000-493999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.51000-51999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.782000-782999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.45000-45999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.575000-575999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.701000-701999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.130000-130999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.387000-387999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.534000-534999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.90000-90999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.661000-661999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.867000-867999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.815000-815999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.925000-925999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.101000-101999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.489000-489999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.325000-325999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.190000-190999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.174000-174999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.940000-940999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.68000-68999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.833000-833999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.611000-611999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.21000-21999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.472000-472999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.403000-403999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.637000-637999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.392000-392999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.793000-793999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.115000-115999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.338000-338999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.494000-494999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.912000-912999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.463000-463999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.761000-761999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.300000-300999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.18000-18999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.70000-70999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.592000-592999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.95000-95999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.41000-41999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.496000-496999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.170000-170999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.596000-596999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.138000-138999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.394000-394999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.165000-165999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.942000-942999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.395000-395999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.273000-273999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.11000-11999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.124000-124999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.343000-343999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.752000-752999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.703000-703999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.355000-355999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.961000-961999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.603000-603999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.279000-279999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.322000-322999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.60000-60999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.721000-721999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.23000-23999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.898000-898999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.47000-47999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.308000-308999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.908000-908999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.340000-340999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.346000-346999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.264000-264999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.498000-498999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.462000-462999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.213000-213999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.617000-617999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.295000-295999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.386000-386999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.431000-431999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.172000-172999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.836000-836999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.456000-456999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.585000-585999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.845000-845999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.597000-597999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.763000-763999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.102000-102999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.204000-204999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.676000-676999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.240000-240999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.541000-541999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.741000-741999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.309000-309999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.525000-525999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.687000-687999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.513000-513999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.681000-681999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.835000-835999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.486000-486999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.85000-85999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.742000-742999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.116000-116999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.977000-977999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.54000-54999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.207000-207999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.283000-283999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.396000-396999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.407000-407999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.285000-285999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.479000-479999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.164000-164999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.477000-477999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.932000-932999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.473000-473999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.379000-379999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.311000-311999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.851000-851999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.274000-274999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.900000-900999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.97000-97999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.307000-307999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.239000-239999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.495000-495999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.34000-34999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.233000-233999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.935000-935999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.840000-840999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.441000-441999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.289000-289999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.200000-200999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.188000-188999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.863000-863999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.695000-695999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.2000-2999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.374000-374999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.388000-388999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.936000-936999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.816000-816999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.973000-973999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.784000-784999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.946000-946999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.380000-380999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.951000-951999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.682000-682999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.189000-189999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.658000-658999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.826000-826999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.129000-129999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.152000-152999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.173000-173999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.375000-375999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.985000-985999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.870000-870999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.139000-139999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.67000-67999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.160000-160999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.105000-105999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.15000-15999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.879000-879999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.680000-680999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.938000-938999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.336000-336999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.48000-48999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.780000-780999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.600000-600999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.719000-719999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.393000-393999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.112000-112999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.728000-728999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.711000-711999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.783000-783999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.664000-664999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.586000-586999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.881000-881999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.669000-669999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.491000-491999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.459000-459999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.247000-247999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.492000-492999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.410000-410999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.490000-490999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.842000-842999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.996000-996999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.87000-87999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.631000-631999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.339000-339999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.428000-428999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.828000-828999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.621000-621999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.690000-690999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.651000-651999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.958000-958999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.895000-895999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.786000-786999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.419000-419999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.259000-259999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.972000-972999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.824000-824999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.937000-937999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.191000-191999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.581000-581999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.148000-148999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.893000-893999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.673000-673999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.512000-512999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.332000-332999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.499000-499999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.533000-533999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.107000-107999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.347000-347999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.636000-636999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.150000-150999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.423000-423999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.402000-402999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.758000-758999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.686000-686999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.373000-373999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.358000-358999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.452000-452999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.561000-561999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.872000-872999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.294000-294999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.745000-745999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.378000-378999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.406000-406999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.335000-335999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.740000-740999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.608000-608999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.926000-926999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.445000-445999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.794000-794999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.649000-649999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.757000-757999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.33000-33999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.398000-398999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.210000-210999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.100000-100999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.692000-692999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.950000-950999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.110000-110999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.221000-221999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.578000-578999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.483000-483999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.650000-650999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.969000-969999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.226000-226999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.648000-648999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.235000-235999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.17000-17999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.595000-595999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.251000-251999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.759000-759999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.242000-242999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.573000-573999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.8000-8999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.520000-520999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.714000-714999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.22000-22999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.861000-861999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.455000-455999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.704000-704999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.72000-72999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.427000-427999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.790000-790999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.121000-121999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.232000-232999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.1000-1999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.847000-847999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.132000-132999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.787000-787999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.183000-183999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.382000-382999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.180000-180999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.652000-652999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.644000-644999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.372000-372999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.125000-125999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.554000-554999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.442000-442999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.362000-362999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.822000-822999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.987000-987999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.889000-889999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.509000-509999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.819000-819999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.911000-911999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.73000-73999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.877000-877999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.91000-91999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.507000-507999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.453000-453999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.197000-197999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.516000-516999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.959000-959999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.464000-464999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.506000-506999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.478000-478999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.166000-166999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.69000-69999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.713000-713999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.990000-990999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.576000-576999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.807000-807999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.290000-290999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.196000-196999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.743000-743999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.978000-978999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.450000-450999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.718000-718999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.216000-216999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.93000-93999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.953000-953999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.979000-979999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.635000-635999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.813000-813999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.560000-560999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.440000-440999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.710000-710999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.921000-921999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.135000-135999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.668000-668999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.771000-771999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.510000-510999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.329000-329999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.528000-528999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.717000-717999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.224000-224999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.839000-839999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.602000-602999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.466000-466999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.992000-992999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.137000-137999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.432000-432999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.820000-820999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.734000-734999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.655000-655999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.910000-910999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.858000-858999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.88000-88999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.524000-524999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.157000-157999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.689000-689999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.237000-237999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.366000-366999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.747000-747999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.57000-57999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.141000-141999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.957000-957999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.63000-63999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.542000-542999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.678000-678999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.795000-795999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.103000-103999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.750000-750999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.342000-342999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.883000-883999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.890000-890999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.954000-954999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.818000-818999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.708000-708999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.696000-696999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.976000-976999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.591000-591999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.767000-767999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.693000-693999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.250000-250999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.475000-475999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.660000-660999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.897000-897999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.860000-860999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.694000-694999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.885000-885999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.312000-312999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.632000-632999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.412000-412999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.646000-646999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.161000-161999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.179000-179999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.518000-518999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.892000-892999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.143000-143999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.424000-424999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.59000-59999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.965000-965999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.918000-918999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.723000-723999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.460000-460999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.298000-298999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.540000-540999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.284000-284999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.131000-131999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.557000-557999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.38000-38999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.914000-914999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.571000-571999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.345000-345999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.75000-75999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.359000-359999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.305000-305999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.884000-884999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.281000-281999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.607000-607999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.364000-364999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.827000-827999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.871000-871999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.855000-855999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.296000-296999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.4000-4999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.857000-857999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.500000-500999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.609000-609999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.549000-549999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.211000-211999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.348000-348999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.56000-56999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.799000-799999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.633000-633999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.454000-454999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.989000-989999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.967000-967999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.155000-155999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.266000-266999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.920000-920999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.654000-654999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.470000-470999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.854000-854999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.82000-82999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.809000-809999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.626000-626999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.555000-555999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.126000-126999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.209000-209999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.58000-58999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.683000-683999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.439000-439999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.192000-192999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.656000-656999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.665000-665999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.760000-760999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.874000-874999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.186000-186999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.218000-218999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.882000-882999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.988000-988999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.262000-262999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.371000-371999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.62000-62999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.326000-326999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.825000-825999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.217000-217999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.328000-328999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.363000-363999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.229000-229999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.545000-545999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.754000-754999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.258000-258999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.136000-136999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.865000-865999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.773000-773999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.588000-588999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.13000-13999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.413000-413999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.796000-796999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.120000-120999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.781000-781999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.679000-679999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.356000-356999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.634000-634999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.698000-698999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.587000-587999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.0-999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.598000-598999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.32000-32999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.171000-171999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.823000-823999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.429000-429999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.736000-736999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.341000-341999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.176000-176999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.354000-354999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.3000-3999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.219000-219999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.471000-471999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.624000-624999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.720000-720999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.616000-616999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.117000-117999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.142000-142999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.903000-903999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.891000-891999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.7000-7999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.739000-739999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.436000-436999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.777000-777999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.544000-544999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.243000-243999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.612000-612999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.147000-147999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.685000-685999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.697000-697999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.337000-337999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.52000-52999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.769000-769999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.159000-159999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.20000-20999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.205000-205999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.482000-482999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.133000-133999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.556000-556999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.838000-838999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.569000-569999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.225000-225999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.700000-700999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.154000-154999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.615000-615999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.726000-726999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.44000-44999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.623000-623999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.280000-280999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.119000-119999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.275000-275999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.849000-849999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.956000-956999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.198000-198999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.418000-418999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.566000-566999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.803000-803999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.238000-238999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.638000-638999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.504000-504999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.709000-709999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.933000-933999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.568000-568999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.707000-707999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.949000-949999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.875000-875999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.924000-924999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.390000-390999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.399000-399999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.267000-267999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.175000-175999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.986000-986999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.31000-31999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.705000-705999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.834000-834999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.580000-580999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.812000-812999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.888000-888999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.302000-302999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.118000-118999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.547000-547999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.134000-134999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.293000-293999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.722000-722999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.546000-546999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.203000-203999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.552000-552999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.447000-447999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.952000-952999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.214000-214999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.966000-966999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.485000-485999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.330000-330999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.313000-313999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.74000-74999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.808000-808999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.369000-369999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.448000-448999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.77000-77999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.785000-785999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.301000-301999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.944000-944999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.994000-994999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.642000-642999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.659000-659999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.806000-806999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.619000-619999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.400000-400999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.564000-564999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.420000-420999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.941000-941999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.457000-457999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.677000-677999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.572000-572999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.458000-458999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.645000-645999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.461000-461999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.270000-270999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.231000-231999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.265000-265999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.334000-334999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.543000-543999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.181000-181999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.385000-385999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.627000-627999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.577000-577999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.271000-271999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.798000-798999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.853000-853999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.81000-81999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.71000-71999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.878000-878999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.109000-109999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.753000-753999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.89000-89999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.111000-111999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.469000-469999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.905000-905999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.643000-643999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.187000-187999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.776000-776999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.43000-43999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.127000-127999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.360000-360999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.246000-246999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.435000-435999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.80000-80999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.384000-384999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.641000-641999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.487000-487999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.153000-153999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.327000-327999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.599000-599999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.306000-306999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.272000-272999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.862000-862999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.802000-802999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.523000-523999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.896000-896999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.653000-653999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.916000-916999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.446000-446999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.532000-532999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.922000-922999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.411000-411999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.570000-570999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.729000-729999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.702000-702999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.657000-657999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.292000-292999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.558000-558999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.236000-236999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.998000-998999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.497000-497999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.438000-438999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.377000-377999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.26000-26999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.261000-261999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.451000-451999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.948000-948999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.947000-947999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.288000-288999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.86000-86999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.223000-223999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.829000-829999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.997000-997999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.755000-755999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.324000-324999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.144000-144999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.304000-304999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.778000-778999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.256000-256999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.671000-671999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.314000-314999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.970000-970999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.848000-848999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.422000-422999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.614000-614999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.146000-146999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.869000-869999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.333000-333999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.367000-367999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.887000-887999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.27000-27999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.113000-113999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.128000-128999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.449000-449999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.252000-252999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.425000-425999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.630000-630999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.831000-831999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.559000-559999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.800000-800999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.565000-565999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.906000-906999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.605000-605999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.699000-699999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.351000-351999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.177000-177999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.804000-804999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.610000-610999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.42000-42999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.613000-613999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.383000-383999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.92000-92999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.712000-712999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.902000-902999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.775000-775999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.53000-53999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.416000-416999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.647000-647999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.14000-14999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.868000-868999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.688000-688999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.964000-964999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.94000-94999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.84000-84999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.349000-349999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.675000-675999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.663000-663999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.376000-376999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.527000-527999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.521000-521999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.582000-582999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.414000-414999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.55000-55999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.320000-320999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.350000-350999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.201000-201999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.404000-404999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.437000-437999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.508000-508999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.5000-5999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.748000-748999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.672000-672999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.10000-10999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.444000-444999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.505000-505999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.995000-995999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.791000-791999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.299000-299999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.49000-49999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.331000-331999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.886000-886999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.913000-913999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.391000-391999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.980000-980999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.727000-727999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.24000-24999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.409000-409999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.792000-792999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.368000-368999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.859000-859999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.939000-939999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.844000-844999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.594000-594999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.670000-670999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.725000-725999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.291000-291999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.764000-764999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.999000-999999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.535000-535999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.852000-852999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.78000-78999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.738000-738999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.982000-982999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.96000-96999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.321000-321999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.873000-873999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.318000-318999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.876000-876999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.583000-583999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.562000-562999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.955000-955999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.245000-245999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.960000-960999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.303000-303999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.732000-732999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.79000-79999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.365000-365999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.526000-526999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.361000-361999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.751000-751999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.539000-539999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.162000-162999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.106000-106999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.316000-316999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.193000-193999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.765000-765999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.584000-584999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.909000-909999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.762000-762999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.934000-934999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.756000-756999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.620000-620999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.184000-184999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.370000-370999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.269000-269999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.503000-503999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.553000-553999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.50000-50999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.227000-227999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.64000-64999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.310000-310999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.215000-215999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.234000-234999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.381000-381999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.531000-531999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.604000-604999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.199000-199999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.260000-260999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.254000-254999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.40000-40999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.122000-122999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.993000-993999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.297000-297999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.169000-169999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.443000-443999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.417000-417999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.108000-108999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.749000-749999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.537000-537999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.768000-768999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.278000-278999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.640000-640999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.145000-145999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.230000-230999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.28000-28999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.353000-353999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.99000-99999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.163000-163999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.856000-856999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.37000-37999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.357000-357999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.904000-904999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.929000-929999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.735000-735999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.83000-83999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.222000-222999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.832000-832999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.801000-801999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.123000-123999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.915000-915999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.202000-202999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.789000-789999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.514000-514999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.253000-253999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.46000-46999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.195000-195999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.674000-674999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.167000-167999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.963000-963999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.810000-810999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.538000-538999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.268000-268999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.770000-770999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.241000-241999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.817000-817999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.515000-515999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.529000-529999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.249000-249999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.730000-730999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.629000-629999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.974000-974999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.212000-212999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.622000-622999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.724000-724999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.114000-114999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.846000-846999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.593000-593999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.519000-519999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.744000-744999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.104000-104999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.511000-511999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.984000-984999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.287000-287999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.864000-864999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.430000-430999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.731000-731999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.29000-29999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.220000-220999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.945000-945999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.517000-517999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.563000-563999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.502000-502999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.25000-25999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.19000-19999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.774000-774999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.257000-257999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.962000-962999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.9000-9999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.666000-666999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.779000-779999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.991000-991999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.282000-282999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.140000-140999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.968000-968999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.551000-551999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.971000-971999.json\n","/kaggle/input/-spotify-tracks-dataset/dataset.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T19:42:12.486471Z","iopub.status.busy":"2025-05-15T19:42:12.485948Z","iopub.status.idle":"2025-05-15T19:42:22.285232Z","shell.execute_reply":"2025-05-15T19:42:22.284504Z","shell.execute_reply.started":"2025-05-15T19:42:12.486450Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting cornac\n","  Downloading cornac-2.3.3-cp311-cp311-manylinux1_x86_64.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy>2.0.0 (from cornac)\n","  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from cornac) (1.15.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cornac) (4.67.1)\n","Collecting powerlaw (from cornac)\n","  Downloading powerlaw-1.5-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from powerlaw->cornac) (3.7.2)\n","Requirement already satisfied: mpmath in /usr/local/lib/python3.11/dist-packages (from powerlaw->cornac) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (25.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (11.1.0)\n","Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->powerlaw->cornac) (1.17.0)\n","Downloading cornac-2.3.3-cp311-cp311-manylinux1_x86_64.whl (31.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading powerlaw-1.5-py3-none-any.whl (24 kB)\n","Installing collected packages: numpy, powerlaw, cornac\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n","gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\n","mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.5 which is incompatible.\n","mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.5 which is incompatible.\n","mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.5 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n","datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n","ydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.5 which is incompatible.\n","google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\n","google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n","bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n","plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n","pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cornac-2.3.3 numpy-2.2.5 powerlaw-1.5\n"]}],"source":["!pip install cornac"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T21:20:59.944939Z","iopub.status.busy":"2025-05-15T21:20:59.944096Z","iopub.status.idle":"2025-05-15T22:23:50.411037Z","shell.execute_reply":"2025-05-15T22:23:50.410336Z","shell.execute_reply.started":"2025-05-15T21:20:59.944906Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","Loading data...\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|██████████| 5/5 [00:01<00:00,  4.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 333697 total interactions.\n","\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['popularity', 'duration_ms', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'mode']\n","Loaded and processed 89741 items with 13 features.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","Warning: Item features not available or aligned. Hybrid NCF training will likely fail or run without features.\n","\n","--- Splitting interaction data into train, validation, and test sets ---\n","Interactions for training: 226913\n","Interactions for validation: 40044\n","Interactions for testing: 66740\n","\n","--- Training Hybrid NCF Model ---\n","   Mapped 5000 users and 91484 items.\n","   Aligning item features with internal item ID map (13 features)...\n","   Aligned features for 3085 items. Internal feature array shape: (91484, 13).\n","   Calculated popularity for 91484 items.\n","   Hybrid NCF model architecture built and compiled with regularization.\n","   Created separate model for extracting NCF item embeddings from MLP path.\n","   Generating positive and negative samples for Hybrid NCF training (8 negatives per positive)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["   Prepared 2026649 training samples for Hybrid NCF.\n","   Prepared 359948 validation samples for Hybrid NCF.\n","   Configured Early Stopping with patience=5.\n","   Fitting Hybrid NCF model for up to 30 epochs with Early Stopping (Batch Size: 512)...\n","Epoch 1/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 7ms/step - accuracy: 0.9683 - loss: 0.2468 - val_accuracy: 0.9886 - val_loss: 0.0961\n","Epoch 2/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.0974 - val_accuracy: 0.9886 - val_loss: 0.0828\n","Epoch 3/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0887 - val_accuracy: 0.9886 - val_loss: 0.0795\n","Epoch 4/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.0877 - val_accuracy: 0.9886 - val_loss: 0.0791\n","Epoch 5/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.0872 - val_accuracy: 0.9886 - val_loss: 0.0814\n","Epoch 6/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0876 - val_accuracy: 0.9886 - val_loss: 0.0788\n","Epoch 7/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.0872 - val_accuracy: 0.9886 - val_loss: 0.0799\n","Epoch 8/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0874 - val_accuracy: 0.9886 - val_loss: 0.0786\n","Epoch 9/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0877 - val_accuracy: 0.9886 - val_loss: 0.0785\n","Epoch 10/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0874 - val_accuracy: 0.9886 - val_loss: 0.0802\n","Epoch 11/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0874 - val_accuracy: 0.9886 - val_loss: 0.0786\n","Epoch 12/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.0870 - val_accuracy: 0.9886 - val_loss: 0.0783\n","Epoch 13/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0875 - val_accuracy: 0.9886 - val_loss: 0.0777\n","Epoch 14/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0876 - val_accuracy: 0.9886 - val_loss: 0.0782\n","Epoch 15/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.0874 - val_accuracy: 0.9886 - val_loss: 0.0797\n","Epoch 16/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0872 - val_accuracy: 0.9886 - val_loss: 0.0807\n","Epoch 17/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0875 - val_accuracy: 0.9886 - val_loss: 0.0782\n","Epoch 18/30\n","\u001b[1m3959/3959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0873 - val_accuracy: 0.9886 - val_loss: 0.0786\n","\n","Hybrid NCF model training complete (possibly stopped early).\n","\n","--- Evaluating Hybrid NCF Model on Main Test Set ---\n","\n","--- Starting Evaluation (n=10) ---\n","Evaluating for 4759 test users with valid interactions.\n","   Extracting and caching NCF item embeddings...\n","   Extracted and cached embeddings of shape: (91484, 128)\n","\n","  Evaluating with reranking method: None\n"]},{"name":"stderr","output_type":"stream","text":["                                                                    \r"]},{"name":"stdout","output_type":"stream","text":["\n","  Evaluating with reranking method: Smooth Xquad\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"name":"stdout","output_type":"stream","text":["\n","Evaluation finished in 3240.76 seconds.\n","\n","--- Evaluating Hybrid NCF Model on User Study Data (/kaggle/working/user_study_interactions.csv) ---\n","Loading user study data from /kaggle/working/user_study_interactions.csv...\n","Loaded 641 interactions from user study.\n","\n","--- Starting Evaluation (n=10) ---\n","No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\n","\n","--- Final Evaluation Results ---\n","\n","Results on Main MPD Test Set:\n","\n","Hybrid NCF Model:\n","  Reranking: None\n","    Precision@10: 0.0008\n","    Recall@10: 0.0007\n","    NDCG@10: 0.0009\n","    Average Diversity (Inverse Popularity): 0.1599\n","  Reranking: Smooth Xquad\n","    Precision@10: 0.0012\n","    Recall@10: 0.0010\n","    NDCG@10: 0.0012\n","    Average Diversity (Inverse Popularity): 0.2062\n","\n","--- Hybrid NCF Reranking Comparison (Smooth XQuAD vs None) on Main Test Set (%) ---\n","  Precision@10: +51.28% change\n","  Recall@10: +38.26% change\n","  NDCG@10: +33.88% change\n","  Average Diversity (Inverse Popularity): +28.94% change\n","\n","Results on User Study Test Data:\n","\n","Hybrid NCF Model:\n","\n","--- Example Recommendations ---\n","Using users from Main Test Set for examples.\n","\n","Recommendations for User: 12358\n","  Hybrid NCF (None Reranking): ['spotify:track:6ngavex4sZrVTif1wwRof0', 'spotify:track:3eqT34f3rxmm4dR7madDIM', 'spotify:track:08GSP3de4vNyAkPkJirmhi', 'spotify:track:4llYMkhHa1w7UAdDFRQcEZ', 'spotify:track:0GhuwiwcAuw3zU5QHCmBiP', 'spotify:track:5dFoWIiJ2814hRwMYDcFiU', 'spotify:track:7ySUcLPVX7KudhnmNcgY2D', 'spotify:track:1rdreHH1v6kyrx2mFQvLR4', 'spotify:track:3bi5bTs4vAyOqpKIZ2dP1N', 'spotify:track:0HZhYMZOcUzZKSFwPOti6m']\n","  Hybrid NCF (Smooth XQuAD Reranking): ['spotify:track:6ngavex4sZrVTif1wwRof0', 'spotify:track:3eqT34f3rxmm4dR7madDIM', 'spotify:track:08GSP3de4vNyAkPkJirmhi', 'spotify:track:4llYMkhHa1w7UAdDFRQcEZ', 'spotify:track:0GhuwiwcAuw3zU5QHCmBiP', 'spotify:track:3bi5bTs4vAyOqpKIZ2dP1N', 'spotify:track:7kMfu3KUydmrFVGEAhjtyl', 'spotify:track:6waHiq6DtId8t1RODPVSgP', 'spotify:track:2EEeOnHehOozLq4aS0n6SL', 'spotify:track:3pNm2hVHhTiZZ2bKY4vL00']\n","\n","Recommendations for User: 12188\n","  Hybrid NCF (None Reranking): ['spotify:track:6ngavex4sZrVTif1wwRof0', 'spotify:track:3eqT34f3rxmm4dR7madDIM', 'spotify:track:08GSP3de4vNyAkPkJirmhi', 'spotify:track:4llYMkhHa1w7UAdDFRQcEZ', 'spotify:track:0GhuwiwcAuw3zU5QHCmBiP', 'spotify:track:5dFoWIiJ2814hRwMYDcFiU', 'spotify:track:7ySUcLPVX7KudhnmNcgY2D', 'spotify:track:1rdreHH1v6kyrx2mFQvLR4', 'spotify:track:3bi5bTs4vAyOqpKIZ2dP1N', 'spotify:track:0HZhYMZOcUzZKSFwPOti6m']\n","  Hybrid NCF (Smooth XQuAD Reranking): ['spotify:track:6ngavex4sZrVTif1wwRof0', 'spotify:track:3eqT34f3rxmm4dR7madDIM', 'spotify:track:08GSP3de4vNyAkPkJirmhi', 'spotify:track:4llYMkhHa1w7UAdDFRQcEZ', 'spotify:track:0GhuwiwcAuw3zU5QHCmBiP', 'spotify:track:3bi5bTs4vAyOqpKIZ2dP1N', 'spotify:track:7kMfu3KUydmrFVGEAhjtyl', 'spotify:track:6waHiq6DtId8t1RODPVSgP', 'spotify:track:3pNm2hVHhTiZZ2bKY4vL00', 'spotify:track:4LK1zq7x8Xxd7Nc32anUoH']\n","\n","Recommendations for User: 11412\n","  Hybrid NCF (None Reranking): ['spotify:track:6ngavex4sZrVTif1wwRof0', 'spotify:track:3eqT34f3rxmm4dR7madDIM', 'spotify:track:08GSP3de4vNyAkPkJirmhi', 'spotify:track:4llYMkhHa1w7UAdDFRQcEZ', 'spotify:track:0GhuwiwcAuw3zU5QHCmBiP', 'spotify:track:5dFoWIiJ2814hRwMYDcFiU', 'spotify:track:7ySUcLPVX7KudhnmNcgY2D', 'spotify:track:1rdreHH1v6kyrx2mFQvLR4', 'spotify:track:3bi5bTs4vAyOqpKIZ2dP1N', 'spotify:track:0HZhYMZOcUzZKSFwPOti6m']\n","  Hybrid NCF (Smooth XQuAD Reranking): ['spotify:track:6ngavex4sZrVTif1wwRof0', 'spotify:track:3eqT34f3rxmm4dR7madDIM', 'spotify:track:08GSP3de4vNyAkPkJirmhi', 'spotify:track:4llYMkhHa1w7UAdDFRQcEZ', 'spotify:track:0GhuwiwcAuw3zU5QHCmBiP', 'spotify:track:3bi5bTs4vAyOqpKIZ2dP1N', 'spotify:track:7kMfu3KUydmrFVGEAhjtyl', 'spotify:track:6waHiq6DtId8t1RODPVSgP', 'spotify:track:2EEeOnHehOozLq4aS0n6SL', 'spotify:track:3pNm2hVHhTiZZ2bKY4vL00']\n","\n","--- Starting Explicit Feature Importance Analysis ---\n","Preparing data for feature importance model...\n","Generated 34578 positive interaction samples with features.\n","Generating negative samples for feature importance analysis...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                              \r"]},{"name":"stdout","output_type":"stream","text":["Generated 34102 negative samples for feature importance analysis.\n","Linked 34102 negative samples with features for analysis.\n","Combined positive and negative samples (features only) for analysis: 68680\n","Training RandomForestClassifier for feature importance...\n","RandomForestClassifier training complete.\n","\n","Feature Importance Scores (from RandomForestClassifier):\n","             Feature  Importance\n","0         popularity    0.608127\n","1        duration_ms    0.054477\n","2   instrumentalness    0.053629\n","3       acousticness    0.044969\n","4            valence    0.036858\n","5           loudness    0.034412\n","6       danceability    0.033766\n","7             energy    0.033125\n","8        speechiness    0.032162\n","9              tempo    0.029945\n","10          liveness    0.029643\n","11          explicit    0.004574\n","12              mode    0.004311\n","\n","Evaluating Classifier Model for Feature Importance Analysis...\n","Accuracy: 0.9533\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.95      0.95      8526\n","           1       0.95      0.96      0.95      8644\n","\n","    accuracy                           0.95     17170\n","   macro avg       0.95      0.95      0.95     17170\n","weighted avg       0.95      0.95      0.95     17170\n","\n","ROC AUC Score: 0.9810\n","Average Precision Score: 0.9768\n","\n","Confusion Matrix:\n","[[8075  451]\n"," [ 350 8294]]\n","\n","Interpretation of Feature Importance Scores:\n","- Higher scores indicate features that were more influential in predicting whether a user interacted with an item.\n","- This gives you insight into which explicit track characteristics are associated with user engagement in the MPD dataset.\n","- Compare these scores to the implicit features learned by NCF and how they relate to the embedding space.\n","\n","--- Further Suggestions for Improving Accuracy and Diversity ---\n","Based on the evaluation results and the potential for overfitting:\n","1. **Analyze Feature Importance Results:** Understand which explicit features are most predictive. This can guide feature engineering or selection.\n","2. **Hyperparameter Tuning:** Experiment rigorously with NCF embedding size, MLP layer sizes, learning rate, batch size, L2 regularization strength, and dropout rates. Use the validation set performance (e.g., validation loss, validation accuracy) as the primary guide, not just training metrics.\n","3. **Experiment with Negative Sampling:** The ratio of negative samples (NEG_SAMPLES_RATIO) can significantly impact training and generalization. Try different ratios.\n","4. **Explore Categorical Features:** Implement handling for categorical features like 'track_genre', 'key', 'time_signature' using one-hot encoding or embedding layers within the NCF model.\n","5. **Advanced Hybrid Architectures:** Investigate more complex ways to combine collaborative filtering and feature data within the NCF structure (e.g., parallel towers, attention mechanisms).\n","6. **Alternative Regularization:** Explore other regularization techniques like Dropout (already increased slightly, but could tune further), or Batch Normalization.\n","7. **Different Diversity Metrics & Reranking:** While Smooth XQuAD increased diversity, it decreased relevance on the test set. Experiment with the MMR_LAMBDA and potentially other diversity metrics or reranking algorithms.\n","8. **Cross-Validation:** For more robust evaluation and hyperparameter tuning, consider using cross-validation if computational resources allow.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn \n","from sklearn.utils import resample \n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","\n","tf.get_logger().setLevel('ERROR') \n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training, and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   feature_columns: Optional[List[str]] = None, # List of explicit feature columns to use\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 \n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            feature_columns: List of column names from the item features dataset to use as input.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training.\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.feature_columns = feature_columns if feature_columns is not None else []\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.num_features = len(self.feature_columns)\n","        self.item_internal_features: Optional[np.ndarray] = None # Scaled features as numpy array, aligned by internal item ID\n","\n","        # Models\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None # Hybrid NCF model\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the hybrid model\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        # The input is now expected to be the full file path, not just the directory\n","        full_path = features_filepath # Use the path directly\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_features = None # Ensure internal features are also reset\n","            self.num_features = 0 # Reset feature count\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            # Drop duplicates based on track_id if any\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","\n","            # Create a Spotify URI from track_id for linking with MPD data\n","            # MPD uses 'spotify:track:<track_id>', need to create this format\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 print(\"Error: 'track_id' column not found in features data. Cannot create track_uri for linking. Skipping feature processing.\")\n","                 self.item_features_df = None\n","                 self.item_internal_features = None\n","                 self.num_features = 0\n","                 return\n","\n","            # Select only the specified feature columns\n","            if not all(col in features_df.columns for col in self.feature_columns):\n","                 missing_cols = [col for col in self.feature_columns if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.feature_columns = [col for col in self.feature_columns if col in features_df.columns]\n","                 self.num_features = len(self.feature_columns)\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining after checking CSV columns. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_features = None\n","                      return\n","\n","            # Select features and the linking column (track_uri)\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle potential missing values in feature columns (simple fillna for now)\n","            # Consider a more sophisticated strategy if needed\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling with 0.\")\n","                 self.item_features_df[self.feature_columns] = self.item_features_df[self.feature_columns].fillna(0)\n","\n","\n","            # Scale numerical features\n","            # Identify numerical columns among the feature columns\n","            numerical_features = self.item_features_df[self.feature_columns].select_dtypes(include=np.number).columns.tolist()\n","            if numerical_features:\n","                 print(f\"Scaling numerical features: {numerical_features}\")\n","                 self.feature_scaler = StandardScaler()\n","                 self.item_features_df[numerical_features] = self.feature_scaler.fit_transform(self.item_features_df[numerical_features])\n","            else:\n","                 print(\"No numerical features found among specified feature columns for scaling.\")\n","\n","\n","            print(f\"Loaded and processed {len(self.item_features_df)} items with {self.num_features} features.\")\n","\n","            # Prepare internal feature array, aligned with item_id_map\n","            # This requires the item_id_map to be created first (usually during interaction matrix creation)\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_features = None\n","            self.num_features = 0\n","\n","\n","    def _align_item_features_with_mapping(self):\n","        \"\"\"Align item features dataframe with the internal item ID mapping.\"\"\"\n","        # Only align if features are loaded and item map exists\n","        if self.item_features_df is None or not self.item_id_map or not self.feature_columns or self.num_features == 0:\n","            self.item_internal_features = None\n","            # print(\"Cannot align item features: features not loaded, item map not ready, or no feature columns defined.\") # Uncomment for debug\n","            return\n","\n","        print(f\"   Aligning item features with internal item ID map ({len(self.feature_columns)} features)...\")\n","        # Use the item_id_map to create an 'internal_item_id' column in the features dataframe\n","        # Keep only items present in both the features dataframe and the interaction matrix mapping\n","        features_mapped_df = self.item_features_df.copy()\n","        features_mapped_df['internal_item_id'] = features_mapped_df['track_uri'].map(self.item_id_map)\n","        features_mapped_df = features_mapped_df.dropna(subset=['internal_item_id']) # Drop items not in interaction data\n","        features_mapped_df['internal_item_id'] = features_mapped_df['internal_item_id'].astype(int)\n","\n","        if features_mapped_df.empty:\n","             print(\"Warning: No items with features found in the interaction matrix mapping after alignment.\")\n","             self.item_internal_features = None\n","             return\n","\n","        # Sort by internal item ID to ensure alignment with arrays indexed by internal ID\n","        features_mapped_df = features_mapped_df.sort_values('internal_item_id').reset_index(drop=True)\n","\n","        # Create the numpy array of features, aligned by internal item ID\n","        # Initialize with zeros for items in interaction matrix but not in features data\n","        num_total_items = len(self.item_id_map)\n","        self.item_internal_features = np.zeros((num_total_items, self.num_features), dtype=np.float32)\n","\n","        # Fill the feature array with data from the aligned features dataframe\n","        # Ensure the columns are in the correct order if the original dataframe columns were reordered somehow\n","        self.item_internal_features[features_mapped_df['internal_item_id'].values] = features_mapped_df[self.feature_columns].values\n","\n","        print(f\"   Aligned features for {len(features_mapped_df)} items. Internal feature array shape: {self.item_internal_features.shape}.\")\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            if self.item_features_df is not None and self.item_internal_features is None:\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        # Handle invalid indices (items/users in df but not in map - should not happen if map was built from df)\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        # Further check for out-of-bounds indices (redundant if map was built from df, but safe)\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_range_mask], ratings[valid_range_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            \n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)} # Ensure counts are int\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame, # Accept train and validation DataFrames\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None: # Added early stopping\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model with features.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model ---\")\n","\n","        \n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping) # Mappings created here\n","\n","        # Calculate popularity if needed (used by diversity metric)\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        \n","        if self.item_features_df is not None and (self.item_internal_features is None or self.item_internal_features.shape[0] != len(self.item_id_map) or self.item_internal_features.shape[1] != self.num_features):\n","             self._align_item_features_with_mapping()\n","\n","\n","        # Check if essential data is available for training\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            self.item_internal_features is None or self.num_features == 0:\n","             print(\"Interaction data or item features not ready/aligned. Cannot train Hybrid NCF.\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Ready: {self.item_internal_features is not None}, Num Features: {self.num_features}\")\n","\n","             self.hybrid_ncf_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        # Create model architecture\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        num_features = self.num_features # Use the number of specified features\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","        item_features_input = tf.keras.layers.Input(shape=(num_features,), dtype='float32', name='item_features_input') # Feature input\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            # Added L2 regularization on embeddings\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            # Added L2 regularization on embeddings\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            # Added L2 regularization on embeddings\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            # Added L2 regularization on embeddings\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # Concatenate MLP embeddings with item features\n","        mlp_features_concat = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec, item_features_input])\n","\n","\n","        # MLP layers\n","        mlp_output = mlp_features_concat\n","        # Added L2 regularization and Dropout to Dense layers\n","        for dim in [256, 128, 64]: # Using hardcoded mlp_layer_dims for now, can make configurable later\n","            mlp_dense = tf.keras.layers.Dense(\n","                dim,\n","                activation='relu',\n","                kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","            )(mlp_output)\n","            mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense) # Increased dropout for demonstration\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer with L2 regularization\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='sigmoid',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Create and compile model\n","        self.hybrid_ncf_model = tf.keras.models.Model( # Renamed\n","            inputs=[user_input, item_input, item_features_input], # Added feature input\n","            outputs=output\n","        )\n","\n","        self.hybrid_ncf_model.compile( # Renamed\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), # Using hardcoded LR\n","            loss='binary_crossentropy',\n","            metrics=['accuracy'] # We monitor accuracy during training\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with regularization.\")\n","\n","        # Create a separate model to get item embeddings from the MLP path (for Smooth XQuAD)\n","        # Note: This model only takes item_input but outputs the embedding learned from interactions\n","        self._item_embedding_model = tf.keras.models.Model(\n","            inputs=item_input,\n","            outputs=mlp_item_embedding_layer(item_input) # Extract embeddings from MLP item embedding layer\n","        )\n","        print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","\n","\n","        # Prepare training and validation data for Hybrid NCF\n","        print(f\"   Generating positive and negative samples for Hybrid NCF training ({self.neg_samples_ratio} negatives per positive)...\")\n","\n","       \n","        train_user_indices_pos = train_df['user'].map(self.user_id_map).values.astype(int)\n","        train_item_indices_pos = train_df['item'].map(self.item_id_map).values.astype(int)\n","\n","        val_user_indices_pos = val_df['user'].map(self.user_id_map).values.astype(int)\n","        val_item_indices_pos = val_df['item'].map(self.item_id_map).values.astype(int)\n","\n","        \n","        train_labels_pos = np.ones(len(train_user_indices_pos), dtype=np.float32)\n","        val_labels_pos = np.ones(len(val_user_indices_pos), dtype=np.float32)\n","\n","        \n","        valid_train_pos_mask = train_item_indices_pos < len(self.id_item_map)\n","        if not np.all(valid_train_pos_mask):\n","             print(f\"Warning: Filtering {np.sum(~valid_train_pos_mask)} train positive interactions with items not in item map.\")\n","             train_user_indices_pos = train_user_indices_pos[valid_train_pos_mask]\n","             train_item_indices_pos = train_item_indices_pos[valid_train_pos_mask]\n","             train_labels_pos = train_labels_pos[valid_train_pos_mask]\n","\n","        valid_val_pos_mask = val_item_indices_pos < len(self.id_item_map)\n","        if not np.all(valid_val_pos_mask):\n","             print(f\"Warning: Filtering {np.sum(~valid_val_pos_mask)} val positive interactions with items not in item map.\")\n","             val_user_indices_pos = val_user_indices_pos[valid_val_pos_mask]\n","             val_item_indices_pos = val_item_indices_pos[valid_val_pos_mask]\n","             val_labels_pos = val_labels_pos[valid_val_pos_mask]\n","\n","\n","        train_pos_item_features = self.item_internal_features[train_item_indices_pos]\n","        val_pos_item_features = self.item_internal_features[val_item_indices_pos]\n","\n","\n","        # Generate negative samples for training and validation\n","        train_neg_user_indices, train_neg_item_indices = self._generate_negative_samples(train_user_indices_pos, train_item_indices_pos, neg_samples_per_positive=self.neg_samples_ratio)\n","        val_neg_user_indices, val_neg_item_indices = self._generate_negative_samples(val_user_indices_pos, val_item_indices_pos, neg_samples_per_positive=self.neg_samples_ratio)\n","\n","\n","        \n","        valid_train_neg_mask = train_neg_item_indices < len(self.id_item_map)\n","        if not np.all(valid_train_neg_mask):\n","             print(f\"Warning: Filtering {np.sum(~valid_train_neg_mask)} train negative samples with items not in item map.\")\n","             train_neg_user_indices = train_neg_user_indices[valid_train_neg_mask]\n","             train_neg_item_indices = train_neg_item_indices[valid_train_neg_mask]\n","\n","        valid_val_neg_mask = val_neg_item_indices < len(self.id_item_map)\n","        if not np.all(valid_val_neg_mask):\n","             print(f\"Warning: Filtering {np.sum(~valid_val_neg_mask)} val negative samples with items not in item map.\")\n","             val_neg_user_indices = val_neg_user_indices[valid_val_neg_mask]\n","             val_neg_item_indices = val_neg_item_indices[valid_val_neg_mask]\n","\n","\n","        train_neg_item_features = self.item_internal_features[train_neg_item_indices]\n","        val_neg_item_features = self.item_internal_features[val_neg_item_indices]\n","\n","        # Labels for negative samples are 0\n","        train_labels_neg = np.zeros(len(train_neg_user_indices), dtype=np.float32)\n","        val_labels_neg = np.zeros(len(val_neg_user_indices), dtype=np.float32)\n","\n","\n","        \n","        all_train_user_indices = np.concatenate([train_user_indices_pos, train_neg_user_indices])\n","        all_train_item_indices = np.concatenate([train_item_indices_pos, train_neg_item_indices])\n","        all_train_item_features = np.concatenate([train_pos_item_features, train_neg_item_features])\n","        all_train_labels = np.concatenate([train_labels_pos, train_labels_neg]) # Use correct negative labels array\n","\n","        all_val_user_indices = np.concatenate([val_user_indices_pos, val_neg_user_indices])\n","        all_val_item_indices = np.concatenate([val_item_indices_pos, val_neg_item_indices])\n","        all_val_item_features = np.concatenate([val_pos_item_features, val_neg_item_features])\n","        all_val_labels = np.concatenate([val_labels_pos, val_labels_neg]) # Use correct negative labels array\n","\n","\n","        # Shuffle training data\n","        train_indices = np.arange(len(all_train_user_indices))\n","        np.random.shuffle(train_indices)\n","        all_train_user_indices = all_train_user_indices[train_indices]\n","        all_train_item_indices = all_train_item_indices[train_indices]\n","        all_train_item_features = all_train_item_features[train_indices]\n","        all_train_labels = all_train_labels[train_indices]\n","        print(f\"   Prepared {len(all_train_labels)} training samples for Hybrid NCF.\")\n","\n","        # Shuffle validation data\n","        val_indices = np.arange(len(all_val_user_indices))\n","        np.random.shuffle(val_indices)\n","        all_val_user_indices = all_val_user_indices[val_indices]\n","        all_val_item_indices = all_val_item_indices[val_indices]\n","        all_val_item_features = all_val_item_features[val_indices]\n","        all_val_labels = all_val_labels[val_indices]\n","        print(f\"   Prepared {len(all_val_labels)} validation samples for Hybrid NCF.\")\n","\n","\n","        # Create tf.data.Datasets for efficient training and validation\n","        train_dataset = tf.data.Dataset.from_tensor_slices(\n","            ({'user_input': all_train_user_indices, 'item_input': all_train_item_indices, 'item_features_input': all_train_item_features}, all_train_labels)\n","        ).shuffle(buffer_size=100000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset = tf.data.Dataset.from_tensor_slices(\n","            ({'user_input': all_val_user_indices, 'item_input': all_val_item_indices, 'item_features_input': all_val_item_features}, all_val_labels)\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE) # No need to shuffle validation data\n","\n","\n","        # Define Early Stopping Callback\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss', # Monitor validation loss\n","            patience=early_stopping_patience, # Number of epochs with no improvement after which training will be stopped\n","            mode='min', # Stop when validation loss is minimized\n","            restore_best_weights=True \n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        # Train model\n","        print(f\"   Fitting Hybrid NCF model for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.hybrid_ncf_model.fit(\n","                train_dataset,\n","                epochs=epochs,\n","                validation_data=val_dataset, # Provide validation dataset\n","                callbacks=[early_stopping], # Add early stopping callback\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None # Clear cache after training\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns.\")\n","             self.hybrid_ncf_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model training failed.\")\n","\n","\n","    def _generate_negative_samples(self, user_indices_pos: np.ndarray,\n","                                  item_indices_pos: np.ndarray, # Use positive indices as input\n","                                  neg_samples_per_positive: int = 4) -> Tuple[np.ndarray, np.ndarray]:\n","        \"\"\"Generate negative samples for training or validation from the pool of items with features.\"\"\"\n","        neg_user_indices, neg_item_indices = [], []\n","        num_items = len(self.item_id_map) # Use length of mapped items\n","\n","        # Get the set of all items that have features and are in the overall item_id_map\n","        items_with_features_internal_ids_set = set()\n","        if self.item_features_df is not None and 'track_uri' in self.item_features_df.columns and self.item_id_map:\n","             items_with_features_orig_uris = self.item_features_df['track_uri'].unique()\n","             items_with_features_internal_ids_set = {self.item_id_map[uri] for uri in items_with_features_orig_uris if uri in self.item_id_map}\n","\n","        if not items_with_features_internal_ids_set:\n","             print(\"Warning: No items with features found in the item map. Cannot generate negatives from featured items.\")\n","             # Fallback: sample from all items in the interaction matrix mapping\n","             items_with_features_internal_ids_set = set(range(num_items))\n","\n","\n","        \n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        # Corrected: Iterate through the input user_indices_pos and item_indices_pos\n","        for u, i in zip(user_indices_pos, item_indices_pos):\n","             user_positive_items[u].add(i)\n","\n","\n","        # Only generate negatives for users who had at least one positive interaction in this subset (train/val)\n","        unique_users_with_positives = np.unique(user_indices_pos)\n","\n","        for u in tqdm(unique_users_with_positives, desc=f\"Generating Negatives ({neg_samples_per_positive}x)\", leave=False):\n","             positive_items_for_user = user_positive_items.get(u, set()) # Items this specific user interacted with in this subset\n","\n","             # Candidate negative items must have features AND not be positive for this user\n","             negative_candidates_for_user_with_features = list(items_with_features_internal_ids_set - positive_items_for_user)\n","\n","\n","             if negative_candidates_for_user_with_features:\n","                 # Sample negative items for this user\n","                 num_pos_for_user = len(positive_items_for_user)\n","                 num_neg_to_sample = min(len(negative_candidates_for_user_with_features),\n","                                    num_pos_for_user * neg_samples_per_positive)\n","\n","                 if num_neg_to_sample > 0:\n","                     neg_items = random.sample(negative_candidates_for_user_with_features, num_neg_to_sample)\n","                     neg_user_indices.extend([u] * len(neg_items))\n","                     neg_item_indices.extend(neg_items)\n","\n","        if not neg_user_indices:\n","            print(\"Warning: No negative samples were successfully generated.\")\n","\n","\n","        return np.array(neg_user_indices, dtype=np.int32), np.array(neg_item_indices, dtype=np.int32) # Ensure int32 dtype\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        # Embeddings are only available if the hybrid NCF model trained successfully\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32) # Use int32\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024 # Batch size for prediction\n","\n","        try:\n","            \n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            # Predict using the separate item embedding model\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            # The separate model outputs the embedding directly, shape should be (num_items, embedding_size)\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 # Handle cases where Keras might return shape (None, 1, embedding_size)\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        # print(\"   Applying Smooth XQuAD reranking...\") # Uncomment for debug\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        # Smooth XQuAD requires item embeddings for similarity calculation\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             # Fallback to just returning top k by original score\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","      \n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            # Return top items from the valid ones if fewer than k\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","\n","        # Sort initial recommendations by relevance score (descending)\n","        # We need to work with indices to pop efficiently\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True) # (original_index_in_valid, (item_id, score))\n","\n","        selected_ids_internal = [] # Internal IDs of selected items\n","        # Create a list of (internal_item_id, relevance_score) tuples for easier processing\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","\n","        # Select the first item (highest relevance)\n","        # The very first item is chosen purely on relevance\n","        if remaining_candidates_data: # Ensure there's at least one candidate\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return [] # No candidates to select\n","\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1 # Index within the remaining_candidates_data list\n","\n","            # Embeddings of items already selected\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","\n","            # Calculate similarity between remaining candidates and the set of selected items\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break # No candidates left\n","\n","            # Ensure candidate_internal_ids are valid indices for item_embeddings\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 # print(\"   Smooth XQuAD Reranking: No remaining candidates with valid embeddings. Stopping.\")\n","                 break # No candidates left with embeddings\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","\n","            # Calculate cosine similarity matrix\n","            # Shape (num_valid_remaining, num_selected)\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            # Find the maximum similarity of each remaining candidate to any item in the selected set\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","\n","            # Evaluate XQuAD score for each remaining candidate\n","            # Iterate through the remaining_candidates_data list which contains (item_id, relevance_score)\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 # Check if the candidate_id had valid embeddings and was included in similarity calculation\n","                 if candidate_id in valid_id_to_list_index:\n","                     # Get the correct similarity score using the map\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i # Index in the current list of remaining candidates\n","                 # Else: This candidate did not have valid embeddings or was already selected/invalid, skip it for XQuAD scoring\n","\n","\n","            if best_candidate_list_index != -1:\n","                # Select the best item based on XQuAD score\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 # Should not happen if remaining_candidates is not empty AND at least one has valid embeddings, but as a safeguard\n","                 # If best_candidate_list_index is still -1, it means no remaining candidate had valid embeddings\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","\n","        # print(f\"   Finished Smooth XQuAD reranking, selected {len(selected_ids_internal)} items.\") # Uncomment for debug\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             # print(f\"Warning: User {user_id} not found in user map. Cannot generate recommendations.\") # Uncomment for debug\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None: # Check Hybrid NCF model availability\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        # Need item IDs and their features for prediction\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        # Features are required for the Hybrid NCF model's prediction input\n","        if self.item_internal_features is None or self.item_internal_features.shape[0] != num_items or self.num_features == 0:\n","             print(\"Error: Item features not loaded or aligned correctly for prediction. Cannot generate recommendations using Hybrid NCF.\")\n","             # You might want to add logic here to fallback to a non-hybrid recommendation method if features are missing\n","             # For now, return empty list\n","             return []\n","\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32) # Use int32\n","        item_array_internal = all_items_internal.astype(np.int32) # Use int32\n","        item_features_array = self.item_internal_features # Use the aligned internal features\n","\n","\n","        # Reshape for Keras input\n","        user_array_internal = user_array_internal.reshape(-1, 1)\n","        item_array_internal = item_array_internal.reshape(-1, 1)\n","        # Features are already (num_items, num_features)\n","\n","\n","        predictions = np.array([])\n","        try:\n","             # Predict using the Hybrid NCF model with all inputs\n","             # Create a tf.data.Dataset for prediction as well\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(\n","                 ({'user_input': user_array_internal, 'item_input': item_array_internal, 'item_features_input': item_features_array})\n","             ).batch(1024).prefetch(tf.data.AUTOTUNE) # Use a reasonable batch size for prediction\n","\n","             predictions = self.hybrid_ncf_model.predict(\n","                 predict_dataset,\n","                 verbose=0\n","             ).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        # Combine internal item IDs with scores\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        # Get items the user has already interacted with to exclude them\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             # Filter out interacted items from candidates\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","\n","        # Apply reranking\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             # Take more candidates than needed for Smooth XQuAD reranking\n","             # XQuAD needs candidates to select from. Taking top M where M > N\n","             # Common practice is M = 100*N or similar, but let's use max(N, 500) as before for efficiency\n","             rerank_candidates_count = max(n * 10, 500) # Increased candidate pool size for better diversity selection\n","             # Sort by relevance score and take top M candidates for reranking\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","\n","        elif rerank_method_lower == 'none':\n","             # No reranking, just take top N by relevance\n","             # print(\"   Applying None reranking (relevance only)...\")\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","\n","        # Convert internal indices back to original item IDs\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        # print(f\"   Generated {len(recommended_items_original)} recommendations for user {user_id} with method '{rerank_method}'.\") # Uncomment for debug\n","        return recommended_items_original\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        # Only evaluate the Hybrid NCF model with specified reranking methods\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {} # Results for the Hybrid NCF model under different reranking\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        # Prepare test data - filter to users/items seen during training\n","        # Ensure test users and items are in the overall mappings created during training\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","\n","        # Create ground truth dictionary using original IDs\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        # Define reranking methods to evaluate for the Hybrid NCF model\n","        evaluation_methods = ['none', 'smooth_xquad'] # Only evaluate these two for Hybrid NCF\n","\n","\n","        # Pre-calculate NCF embeddings if needed for Smooth XQuAD evaluation\n","        # This uses the _item_embedding_model which extracts from the MLP path of the hybrid model\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","        # Metrics for each reranking method\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","\n","                 # Get recommendations using the Hybrid NCF model and the specified reranking method\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","\n","                 if recs_orig:\n","                      # Ensure recommended list has length n for precision/recall/ndcg calculation\n","                      recs_at_n_orig = recs_orig[:n]\n","\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      # Calculate diversity on the generated recommendations (could be more than n before truncation)\n","                      diversities_val = self._calculate_diversity(recs_orig)\n","                      if diversities_val is not None: # Handle cases where diversity can't be calculated\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","\n","        # Calculate average metrics for each method\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        # Truncate recommendations to k\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        # Calculate DCG\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","\n","        \n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        # Filter recommendations to those in the item_id_map and get their internal IDs\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        \n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","    \n","    \n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity() # Calculate if interaction matrix is available and not empty\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 # print(\"Warning: Cannot calculate item popularity for diversity metric.\") # Uncomment for debug\n","                 return 0.0 # Return 0 if popularity still can't be calculated\n","\n","\n","        max_pop = max(self.item_popularity.values()) if self.item_popularity else 0 # Handle case where popularity is empty\n","        if max_pop == 0: return 0.0 # Should be handled before, but safe check\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","           \n","             pop = self.item_popularity.get(item_internal_id, 0) # Default popularity to 0 if somehow missing\n","             # Use a smoothing factor or add epsilon to avoid division by zero/infinity for unseen items\n","             inverse_pop = 1.0 / (pop + 1.0) # Add 1 to popularity to avoid div by zero\n","             inverse_pop_scores.append(inverse_pop)\n","\n","\n","        # Calculate average inverse popularity as the diversity metric\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","\n","        return avg_inverse_popularity\n","\n","\n","    # Removed the demonstrate_feature_importance_analysis method from the class\n","    # The code is now a standalone block in the main function\n","\n","\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","    tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","    # --- Configuration Parameters ---\n","    # Define paths to your data\n","    # MPD Interaction Data: likely in '/kaggle/input/spotify-challenge/data'\n","    MPD_INPUT_DIR = '/kaggle/input/spotify-challenge/data' # Path for MPD JSON slices - *** SET TO THIS PATH ***\n","\n","    # Spotify Tracks Feature Data: Specify the FULL FILE PATH to the CSV\n","    FEATURES_FILE_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv' # *** CORRECTED FULL PATH TO THE FEATURES CSV FILE ***\n","\n","\n","    # Model Hyperparameters\n","    EMBEDDING_SIZE = 128 # Embedding size for NCF\n","    L2_REGULARIZATION = 0.001 # L2 regularization strength (Added)\n","\n","\n","    # Hybrid NCF Model Parameters\n","    NCF_EPOCHS = 30 # Number of training epochs for Hybrid NCF (Max epochs with early stopping)\n","    NCF_BATCH_SIZE = 512\n","    EARLY_STOPPING_PATIENCE = 5 # Early stopping patience (Added)\n","    NEG_SAMPLES_RATIO = 8 # Negative samples per positive interaction during training (Increased)\n","\n","    # Reranking Parameters\n","    # MMR_LAMBDA is the trade-off for Smooth XQuAD (0-1). Higher means more relevance.\n","    SMOOTH_XQUAD_LAMBDA = 0.7 # Lambda for Smooth XQuAD\n","\n","\n","    # Explicit Feature Columns to use in the Hybrid Model\n","    # Choose numerical and potentially one-hot encoded categorical features\n","    # 'popularity', 'duration_ms', 'explicit' (needs mapping), 'danceability', 'energy', 'key',\n","    # 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n","    # 'valence', 'tempo', 'time_signature'\n","    # 'track_genre' is categorical with many values, harder to use directly without encoding/embedding\n","\n","    # Let's select some common numerical features and 'explicit', 'mode'.\n","    # 'key', 'time_signature', 'track_genre' would require more sophisticated handling (e.g., one-hot or embedding layers in NCF).\n","    # For simplicity, we'll use numerical and binary/ordinal features directly.\n","    SELECTED_FEATURE_COLUMNS = [\n","        'popularity', 'duration_ms', 'danceability', 'energy', 'loudness',\n","        'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n","        'valence', 'tempo', 'mode', 'explicit' # 'mode' and 'explicit' are 0/1 or boolean, can be treated as numerical for scaling\n","        # Consider 'key', 'time_signature', 'track_genre' as next steps requiring different handling\n","    ]\n","\n","\n","    # Data Loading Parameters\n","    NUM_MPD_FILES_TO_LOAD = 5 # Number of MPD slice files to load from the directory\n","\n","    # Data Splitting Parameters (Manual split for train, validation, test)\n","    VAL_SET_FRACTION = 0.15 # Fraction of (train+val) data for validation set\n","    TEST_SET_FRACTION = 0.20 # Fraction of total data for the main test set\n","\n","\n","    # User Study Data File Path (where you save your collected data)\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv' # *** Update this path if needed ***\n","    # Note: The code will check if this file exists and only run the evaluation if it does.\n","\n","\n","    # Recommendation & Evaluation Parameters\n","    N_RECOMMENDATIONS = 10\n","\n","\n","    # --- Data Loading ---\n","    print(\"Loading data...\")\n","    # Initialize recommender\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=EMBEDDING_SIZE,\n","        mmr_lambda=SMOOTH_XQUAD_LAMBDA,\n","        feature_columns=SELECTED_FEATURE_COLUMNS, # Pass feature columns to initializer\n","        l2_reg=L2_REGULARIZATION, # Pass L2 regularization parameter\n","        neg_samples_ratio=NEG_SAMPLES_RATIO # Pass negative samples ratio\n","    )\n","\n","\n","    # Load interaction data first\n","    interactions_df = recommender.load_mpd_data(\n","        input_dir=MPD_INPUT_DIR,\n","        num_files=NUM_MPD_FILES_TO_LOAD\n","    )\n","\n","    if interactions_df.empty:\n","        print(\"Error: Failed to load interaction data. Cannot proceed.\")\n","        return\n","\n","    print(f\"Loaded {len(interactions_df)} total interactions.\")\n","\n","\n","    # Load and process item feature data using the corrected file path\n","    recommender.load_item_features(\n","        features_filepath=FEATURES_FILE_PATH # Pass the full file path\n","    )\n","\n","    # If item features didn't load or align, print a warning and the reason\n","    if recommender.item_internal_features is None or recommender.item_internal_features.shape[1] == 0:\n","         print(\"\\nWarning: Item features not available or aligned. Hybrid NCF training will likely fail or run without features.\")\n","         # The train_hybrid_ncf_model method has checks to gracefully handle this.\n","         # If you want a non-hybrid fallback, you'd implement it here.\n","\n","\n","    # --- Data Splitting (Train, Validation, Test) ---\n","    print(\"\\n--- Splitting interaction data into train, validation, and test sets ---\")\n","    try:\n","        # Split into train+validation and test first\n","        train_val_df, test_df = train_test_split(\n","            interactions_df,\n","            test_size=TEST_SET_FRACTION,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        # Then split train+validation into train and validation\n","        train_df, val_df = train_test_split(\n","            train_val_df,\n","            test_size=VAL_SET_FRACTION, # This fraction is *of the train_val_df*\n","            random_state=42,\n","            shuffle=True\n","        )\n","\n","        print(f\"Interactions for training: {len(train_df)}\")\n","        print(f\"Interactions for validation: {len(val_df)}\")\n","        print(f\"Interactions for testing: {len(test_df)}\")\n","\n","    except ValueError as e:\n","        print(f\"Error splitting data: {e}. Make sure you have enough interactions.\")\n","        return\n","    except Exception as e:\n","         print(f\"An unexpected error occurred during data splitting: {e}\")\n","         return\n","\n","\n","    # --- Train Hybrid NCF Model ---\n","    # Pass train and validation DataFrames to the training method\n","    recommender.train_hybrid_ncf_model(\n","        train_df,\n","        val_df, # Pass validation data\n","        epochs=NCF_EPOCHS, # Use the configured epoch count (max epochs)\n","        batch_size=NCF_BATCH_SIZE,\n","        early_stopping_patience=EARLY_STOPPING_PATIENCE # Pass patience\n","    )\n","\n","\n","    # --- Evaluate Model on Main Test Set ---\n","    # Evaluate the trained Hybrid NCF model on the main test set\n","    main_test_evaluation_results = {}\n","    if recommender.hybrid_ncf_model is not None: # Check if Hybrid NCF model trained successfully\n","         print(\"\\n--- Evaluating Hybrid NCF Model on Main Test Set ---\")\n","         # The evaluate method uses the entire test_df directly\n","         main_test_evaluation_results = recommender.evaluate(test_df, n=N_RECOMMENDATIONS)\n","    else:\n","         print(\"\\nSkipping main test set evaluation as the Hybrid NCF model did not train successfully.\")\n","\n","\n","    # --- Evaluate Model on User Study Data (if available) ---\n","    # This section is included but will only run if the file exists, as requested.\n","    user_study_test_results = {}\n","    print(f\"\\n--- Evaluating Hybrid NCF Model on User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","    if os.path.exists(USER_STUDY_DATA_PATH):\n","         print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","         try:\n","              user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","              print(f\"Loaded {len(user_study_df)} interactions from user study.\")\n","              if not user_study_df.empty:\n","                   # Evaluate the SAME trained hybrid model on the user study data\n","                   # Note: User study users/items might not be in the original MPD mappings.\n","                   # The evaluate method filters to users/items in the mappings.\n","                   user_study_test_results = recommender.evaluate(user_study_df, n=N_RECOMMENDATIONS)\n","              else:\n","                   print(\"User study data file is empty. Skipping evaluation.\")\n","         except Exception as e:\n","              print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","              print(\"Skipping user study evaluation.\")\n","    else:\n","         print(f\"User study data file not found at {USER_STUDY_DATA_PATH}. Skipping evaluation.\")\n","         print(\"Please collect data from your 25 users and save it to this path in a compatible CSV format (at least 'user' and 'item' columns).\")\n","\n","\n","    # --- Print Evaluation Results (Combined) ---\n","    print(\"\\n--- Final Evaluation Results ---\")\n","\n","    if main_test_evaluation_results:\n","         print(\"\\nResults on Main MPD Test Set:\")\n","         if 'Hybrid NCF' in main_test_evaluation_results:\n","              ncf_main_results = main_test_evaluation_results['Hybrid NCF']\n","              print(\"\\nHybrid NCF Model:\")\n","              for method_name, metrics in ncf_main_results.items():\n","                   print(f\"  Reranking: {method_name.replace('_', ' ').title()}\")\n","                   for metric, value in metrics.items():\n","                        print(f\"    {metric}: {value:.4f}\")\n","\n","              if 'none' in ncf_main_results and 'smooth_xquad' in main_test_evaluation_results['Hybrid NCF']:\n","                   print(\"\\n--- Hybrid NCF Reranking Comparison (Smooth XQuAD vs None) on Main Test Set (%) ---\")\n","                   none_metrics = ncf_main_results['none']\n","                   smooth_xquad_metrics = main_test_evaluation_results['Hybrid NCF']['smooth_xquad']\n","                   for metric in none_metrics.keys():\n","                        if metric in smooth_xquad_metrics:\n","                             none_value = none_metrics[metric]\n","                             smooth_xquad_value = smooth_xquad_metrics[metric]\n","                             if none_value != 0:\n","                                  improvement = ((smooth_xquad_value - none_value) / none_value) * 100\n","                                  # Ensure percentage change is not misleading for diversity if baseline is 0\n","                                  if 'Diversity' in metric and none_value == 0:\n","                                       print(f\"  {metric}: None baseline is 0. Smooth XQuAD value: {smooth_xquad_value:.4f}\")\n","                                  else:\n","                                       print(f\"  {metric}: {improvement:+.2f}% change\")\n","                             else:\n","                                  # Handle division by zero for percentage change if baseline is 0\n","                                  print(f\"  {metric}: 'None' baseline value is 0. Smooth XQuAD value: {smooth_xquad_value:.4f}\")\n","\n","\n","    if user_study_test_results:\n","         print(\"\\nResults on User Study Test Data:\")\n","         if 'Hybrid NCF' in user_study_test_results:\n","              ncf_user_study_results = user_study_test_results['Hybrid NCF']\n","              print(\"\\nHybrid NCF Model:\")\n","              for method_name, metrics in ncf_user_study_results.items():\n","                   print(f\"  Reranking: {method_name.replace('_', ' ').title()}\")\n","                   for metric, value in metrics.items():\n","                        print(f\"    {metric}: {value:.4f}\")\n","\n","              if 'none' in ncf_user_study_results and 'smooth_xquad' in user_study_test_results['Hybrid NCF']:\n","                   print(\"\\n--- Hybrid NCF Reranking Comparison (Smooth XQuAD vs None) on User Study Data (%) ---\")\n","                   none_metrics = ncf_user_study_results['none']\n","                   smooth_xquad_metrics = user_study_test_results['Hybrid NCF']['smooth_xquad']\n","                   for metric in none_metrics.keys():\n","                        if metric in smooth_xquad_metrics:\n","                             none_value = none_metrics[metric]\n","                             smooth_xquad_value = smooth_xquad_metrics[metric]\n","                             if none_value != 0:\n","                                  improvement = ((smooth_xquad_value - none_value) / none_value) * 100\n","                                  # Ensure percentage change is not misleading for diversity if baseline is 0\n","                                  if 'Diversity' in metric and none_value == 0:\n","                                       print(f\"  {metric}: None baseline is 0. Smooth XQuAD value: {smooth_xquad_value:.4f}\")\n","                                  else:\n","                                       print(f\"  {metric}: {improvement:+.2f}% change\")\n","                             else:\n","                                  # Handle division by zero for percentage change if baseline is 0\n","                                  print(f\"  {metric}: 'None' baseline value is 0. Smooth XQuAD value: {smooth_xquad_value:.4f}\")\n","\n","\n","    if not main_test_evaluation_results and not user_study_test_results:\n","        print(\"No evaluation results available.\")\n","\n","\n","    # --- Generate Example Recommendations (Optional) ---\n","    print(\"\\n--- Example Recommendations ---\")\n","    # Use test users from main test set or user study if available\n","    example_users_df = None\n","    if test_df is not None and not test_df.empty:\n","        example_users_df = test_df\n","        print(\"Using users from Main Test Set for examples.\")\n","    elif user_study_df is not None and not user_study_df.empty: # Use user study users if main test set is empty\n","        example_users_df = user_study_df\n","        print(\"Using users from User Study Data for examples.\")\n","    else:\n","         print(\"No test users available for example recommendations.\")\n","\n","\n","    # Ensure there are users to sample from and get original user IDs\n","    example_users_orig = []\n","    if example_users_df is not None and not example_users_df['user'].empty:\n","        # Get unique users who are also present in the recommender's user_id_map\n","        valid_example_users = example_users_df['user'].unique()\n","        valid_example_users_in_map = [user for user in valid_example_users if user in recommender.user_id_map]\n","        example_users_orig = valid_example_users_in_map[:min(3, len(valid_example_users_in_map))]\n","\n","\n","    if example_users_orig and recommender.hybrid_ncf_model is not None: # Only generate if users and model are available\n","        for user in example_users_orig:\n","             print(f\"\\nRecommendations for User: {user}\")\n","             # Example using different reranking methods\n","             recs_none = recommender.recommend(user, n=N_RECOMMENDATIONS, rerank_method='none')\n","             print(f\"  Hybrid NCF (None Reranking): {recs_none}\")\n","\n","             # Only try Smooth XQuAD if embeddings are available\n","             if recommender._get_ncf_item_embeddings() is not None:\n","                 recs_smooth_xquad = recommender.recommend(user, n=N_RECOMMENDATIONS, rerank_method='smooth_xquad')\n","                 print(f\"  Hybrid NCF (Smooth XQuAD Reranking): {recs_smooth_xquad}\")\n","             else:\n","                 print(\"  Smooth XQuAD Reranking skipped: NCF item embeddings not available.\")\n","\n","    else:\n","        print(\"No users or trained Hybrid NCF model available for example recommendations.\")\n","\n","\n","    # --- Explicit Feature Importance Analysis (Uncommented Block) ---\n","    # This code block performs the analysis using a separate classifier.\n","    # It relies on the 'recommender' object and 'interactions_df' from the main execution.\n","    # Make sure these variables are accessible in your notebook environment.\n","\n","    print(\"\\n--- Starting Explicit Feature Importance Analysis ---\")\n","\n","    # Check if necessary data is available before proceeding with analysis\n","    if 'recommender' not in locals() or recommender is None or recommender.item_features_df is None or recommender.item_features_df.empty or not recommender.feature_columns:\n","         print(\"Item features or recommender object not ready. Cannot perform feature importance analysis.\")\n","    elif 'interactions_df' not in locals() or interactions_df is None or interactions_df.empty:\n","         print(\"Interactions data not ready. Cannot perform feature importance analysis.\")\n","    else:\n","        try:\n","            print('Preparing data for feature importance model...')\n","            # 1. Prepare a dataset of (Item Features) -> Interaction Label (1/0)\n","            #    - Positive Samples: Features of items from interactions_df (Label 1)\n","            #    - Negative Samples: Features of randomly sampled items that *were not* interacted with (Label 0)\n","\n","            # Link positive interactions to item features\n","            # Ensure we only take features for items that were interacted with AND are in the features dataset\n","            positive_interactions_with_features = interactions_df.merge(\n","                recommender.item_features_df[['track_uri'] + recommender.feature_columns],\n","                left_on='item', right_on='track_uri', how='inner'\n","            ).drop(columns=['track_uri', 'track_name', 'artist_name']) # Keep user and item columns for sampling\n","            positive_interactions_with_features['label'] = 1\n","\n","            if positive_interactions_with_features.empty:\n","                 print('No positive interaction samples with features found. Cannot perform this analysis.')\n","            else:\n","                print(f'Generated {len(positive_interactions_with_features)} positive interaction samples with features.')\n","\n","                # 2. Generate negative samples\n","                \n","\n","                all_item_uris_with_features = recommender.item_features_df['track_uri'].unique() # Pool of all items *with features*\n","                items_with_features_set = set(all_item_uris_with_features)\n","\n","                # Get the set of all (user, item) pairs that ARE positive interactions\n","                positive_pairs = set(zip(positive_interactions_with_features['user'], positive_interactions_with_features['item']))\n","\n","                users_for_neg_sampling = positive_interactions_with_features['user'].unique()\n","\n","                neg_samples_ratio_fi = 1 # Ratio for Feature Importance analysis (can be different from NCF training)\n","                negative_samples_list_fi = []\n","\n","                print('Generating negative samples for feature importance analysis...')\n","                \n","\n","                for user_id in tqdm(users_for_neg_sampling, desc='Generating Negatives (FI)', leave=False):\n","                    interacted_items_by_user = {item for u, item in positive_pairs if u == user_id}\n","                    # Candidate negative items: all items *with features* MINUS items this user interacted with\n","                    negative_candidates_for_user_fi = list(items_with_features_set - interacted_items_by_user)\n","\n","                    if negative_candidates_for_user_fi:\n","                        # Sample negative items for this user\n","                        num_pos_for_user_fi = len(interacted_items_by_user)\n","                        num_neg_to_sample_fi = min(len(negative_candidates_for_user_fi),\n","                                                   num_pos_for_user_fi * neg_samples_ratio_fi)\n","\n","                        if num_neg_to_sample_fi > 0:\n","                            neg_items_fi = random.sample(negative_candidates_for_user_fi, num_neg_to_sample_fi)\n","                            for item_id in neg_items_fi:\n","                                negative_samples_list_fi.append({'user': user_id, 'item': item_id, 'label': 0})\n","\n","                if not negative_samples_list_fi:\n","                     print('No negative samples generated for feature importance analysis. Cannot proceed.')\n","                else:\n","                    negative_samples_df_fi = pd.DataFrame(negative_samples_list_fi)\n","\n","                    print(f'Generated {len(negative_samples_df_fi)} negative samples for feature importance analysis.')\n","\n","                    # Link negative samples to item features\n","                    negative_samples_with_features_fi = negative_samples_df_fi.merge(\n","                        recommender.item_features_df[['track_uri'] + recommender.feature_columns],\n","                        left_on='item', right_on='track_uri', how='inner' # Ensure negative samples have features\n","                    ).drop(columns=['track_uri', 'user', 'item']) # Drop user/item as we predict from features\n","\n","                    if negative_samples_with_features_fi.empty:\n","                         print('No negative samples found with linked item features for analysis. Cannot proceed.')\n","                    else:\n","                        negative_samples_with_features_fi['label'] = 0 # Ensure label is 0\n","                        print(f'Linked {len(negative_samples_with_features_fi)} negative samples with features for analysis.')\n","\n","\n","                        \n","                        positive_features_only_fi = positive_interactions_with_features[recommender.feature_columns + ['label']]\n","                        negative_features_only_fi = negative_samples_with_features_fi[recommender.feature_columns + ['label']]\n","\n","                        \n","                        all_samples_features_fi = pd.concat([positive_features_only_fi, negative_features_only_fi], ignore_index=True)\n","\n","                        if all_samples_features_fi.empty:\n","                             print('Combined samples dataset (features only) is empty for analysis. Cannot proceed.')\n","                        else:\n","                            print(f'Combined positive and negative samples (features only) for analysis: {len(all_samples_features_fi)}')\n","\n","                            \n","                            X_fi = all_samples_features_fi[recommender.feature_columns]\n","                            y_fi = all_samples_features_fi['label']\n","\n","                            \n","                            X_train_clf_fi, X_test_clf_fi, y_train_clf_fi, y_test_clf_fi = train_test_split_sklearn(X_fi, y_fi, test_size=0.25, random_state=42, stratify=y_fi)\n","\n","                            # 3. Initialize and train a classifier model \n","                            print('Training RandomForestClassifier for feature importance...')\n","                           \n","                            clf_model_fi = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n","                            clf_model_fi.fit(X_train_clf_fi, y_train_clf_fi)\n","                            print('RandomForestClassifier training complete.')\n","\n","                            # 4. Get feature importance scores\n","                            feature_importances_fi = clf_model_fi.feature_importances_\n","                            feature_names_fi = recommender.feature_columns\n","                            importance_df_fi = pd.DataFrame({'Feature': feature_names_fi, 'Importance': feature_importances_fi})\n","                            importance_df_fi = importance_df_fi.sort_values('Importance', ascending=False).reset_index(drop=True)\n","\n","                            \n","                            print('\\nFeature Importance Scores (from RandomForestClassifier):')\n","                            print(importance_df_fi)\n","\n","                           \n","                            print('\\nEvaluating Classifier Model for Feature Importance Analysis...')\n","                           \n","                            y_pred_clf_fi = clf_model_fi.predict(X_test_clf_fi)\n","                            y_pred_proba_clf_fi = clf_model_fi.predict_proba(X_test_clf_fi)[:, 1] \n","\n","                           \n","                            accuracy_fi = accuracy_score(y_test_clf_fi, y_pred_clf_fi) \n","                            print(f'Accuracy: {accuracy_fi:.4f}')\n","\n","                            print('\\nClassification Report:')\n","                            print(classification_report(y_test_clf_fi, y_pred_clf_fi))\n","\n","                            # Check AUC and Average Precision \n","                            try:\n","                                roc_auc_fi = roc_auc_score(y_test_clf_fi, y_pred_proba_clf_fi)\n","                                print(f'ROC AUC Score: {roc_auc_fi:.4f}')\n","                            except ValueError: # Handles case where only one class is present in y_test\n","                                print('ROC AUC score cannot be calculated as only one class is present in test samples for FI analysis.')\n","\n","                            try:\n","                                avg_precision_fi = average_precision_score(y_test_clf_fi, y_pred_proba_clf_fi)\n","                                print(f'Average Precision Score: {avg_precision_fi:.4f}')\n","                            except ValueError: # Handles case where only one class is present in y_test\n","                                print('Average Precision score cannot be calculated as only one class is present in test samples for FI analysis.')\n","\n","                            # Confusion Matrix\n","                            cm_fi = confusion_matrix(y_test_clf_fi, y_pred_clf_fi)\n","                            print('\\nConfusion Matrix:')\n","                            print(cm_fi)\n","\n","                            print('\\nInterpretation of Feature Importance Scores:')\n","                            print('- Higher scores indicate features that were more influential in predicting whether a user interacted with an item.')\n","                            print('- This gives you insight into which explicit track characteristics are associated with user engagement in the MPD dataset.')\n","                            print('- Compare these scores to the implicit features learned by NCF and how they relate to the embedding space.')\n","\n","\n","        except Exception as e:\n","            print(f'An error occurred during explicit feature importance analysis: {e}')\n","            print('Please ensure all necessary libraries are installed and the code structure is correct.')\n","\n","\n","    \n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2570056,"sourceId":4372070,"sourceType":"datasetVersion"},{"datasetId":6810958,"sourceId":10949715,"sourceType":"datasetVersion"},{"datasetId":7430955,"sourceId":11828786,"sourceType":"datasetVersion"}],"dockerImageVersionId":31041,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
