{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-05-15T10:18:36.975932Z","iopub.status.busy":"2025-05-15T10:18:36.975780Z","iopub.status.idle":"2025-05-15T10:18:40.900738Z","shell.execute_reply":"2025-05-15T10:18:40.899722Z","shell.execute_reply.started":"2025-05-15T10:18:36.975918Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/-spotify-tracks-dataset/dataset.csv\n","/kaggle/input/spotify-challenge/md5sums\n","/kaggle/input/spotify-challenge/README.md\n","/kaggle/input/spotify-challenge/license.txt\n","/kaggle/input/spotify-challenge/stats.txt\n","/kaggle/input/spotify-challenge/src/check.py\n","/kaggle/input/spotify-challenge/src/descriptions.py\n","/kaggle/input/spotify-challenge/src/stats.py\n","/kaggle/input/spotify-challenge/src/show.py\n","/kaggle/input/spotify-challenge/src/deeper_stats.py\n","/kaggle/input/spotify-challenge/src/print.py\n","/kaggle/input/spotify-challenge/data/mpd.slice.35000-35999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.98000-98999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.405000-405999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.601000-601999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.567000-567999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.421000-421999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.983000-983999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.434000-434999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.315000-315999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.797000-797999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.194000-194999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.981000-981999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.548000-548999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.618000-618999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.943000-943999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.907000-907999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.805000-805999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.481000-481999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.208000-208999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.574000-574999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.352000-352999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.850000-850999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.716000-716999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.667000-667999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.691000-691999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.706000-706999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.811000-811999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.484000-484999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.901000-901999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.899000-899999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.628000-628999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.76000-76999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.178000-178999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.401000-401999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.843000-843999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.426000-426999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.919000-919999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.156000-156999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.733000-733999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.550000-550999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.737000-737999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.323000-323999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.344000-344999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.928000-928999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.917000-917999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.579000-579999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.662000-662999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.788000-788999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.837000-837999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.244000-244999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.228000-228999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.66000-66999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.814000-814999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.476000-476999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.975000-975999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.866000-866999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.12000-12999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.389000-389999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.522000-522999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.276000-276999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.894000-894999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.286000-286999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.589000-589999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.397000-397999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.684000-684999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.590000-590999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.530000-530999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.415000-415999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.65000-65999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.185000-185999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.255000-255999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.715000-715999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.16000-16999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.468000-468999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.408000-408999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.61000-61999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.923000-923999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.930000-930999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.433000-433999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.927000-927999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.639000-639999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.841000-841999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.263000-263999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.501000-501999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.772000-772999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.39000-39999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.206000-206999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.467000-467999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.746000-746999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.36000-36999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.182000-182999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.536000-536999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.248000-248999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.158000-158999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.830000-830999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.474000-474999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.625000-625999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.30000-30999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.488000-488999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.168000-168999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.766000-766999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.931000-931999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.480000-480999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.6000-6999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.880000-880999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.317000-317999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.151000-151999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.821000-821999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.606000-606999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.465000-465999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.319000-319999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.149000-149999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.277000-277999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.493000-493999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.51000-51999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.782000-782999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.45000-45999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.575000-575999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.701000-701999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.130000-130999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.387000-387999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.534000-534999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.90000-90999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.661000-661999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.867000-867999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.815000-815999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.925000-925999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.101000-101999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.489000-489999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.325000-325999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.190000-190999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.174000-174999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.940000-940999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.68000-68999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.833000-833999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.611000-611999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.21000-21999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.472000-472999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.403000-403999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.637000-637999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.392000-392999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.793000-793999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.115000-115999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.338000-338999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.494000-494999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.912000-912999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.463000-463999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.761000-761999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.300000-300999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.18000-18999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.70000-70999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.592000-592999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.95000-95999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.41000-41999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.496000-496999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.170000-170999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.596000-596999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.138000-138999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.394000-394999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.165000-165999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.942000-942999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.395000-395999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.273000-273999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.11000-11999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.124000-124999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.343000-343999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.752000-752999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.703000-703999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.355000-355999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.961000-961999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.603000-603999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.279000-279999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.322000-322999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.60000-60999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.721000-721999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.23000-23999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.898000-898999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.47000-47999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.308000-308999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.908000-908999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.340000-340999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.346000-346999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.264000-264999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.498000-498999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.462000-462999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.213000-213999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.617000-617999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.295000-295999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.386000-386999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.431000-431999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.172000-172999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.836000-836999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.456000-456999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.585000-585999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.845000-845999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.597000-597999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.763000-763999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.102000-102999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.204000-204999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.676000-676999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.240000-240999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.541000-541999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.741000-741999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.309000-309999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.525000-525999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.687000-687999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.513000-513999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.681000-681999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.835000-835999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.486000-486999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.85000-85999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.742000-742999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.116000-116999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.977000-977999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.54000-54999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.207000-207999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.283000-283999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.396000-396999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.407000-407999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.285000-285999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.479000-479999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.164000-164999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.477000-477999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.932000-932999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.473000-473999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.379000-379999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.311000-311999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.851000-851999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.274000-274999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.900000-900999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.97000-97999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.307000-307999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.239000-239999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.495000-495999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.34000-34999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.233000-233999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.935000-935999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.840000-840999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.441000-441999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.289000-289999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.200000-200999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.188000-188999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.863000-863999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.695000-695999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.2000-2999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.374000-374999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.388000-388999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.936000-936999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.816000-816999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.973000-973999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.784000-784999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.946000-946999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.380000-380999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.951000-951999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.682000-682999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.189000-189999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.658000-658999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.826000-826999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.129000-129999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.152000-152999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.173000-173999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.375000-375999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.985000-985999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.870000-870999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.139000-139999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.67000-67999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.160000-160999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.105000-105999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.15000-15999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.879000-879999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.680000-680999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.938000-938999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.336000-336999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.48000-48999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.780000-780999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.600000-600999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.719000-719999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.393000-393999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.112000-112999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.728000-728999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.711000-711999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.783000-783999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.664000-664999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.586000-586999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.881000-881999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.669000-669999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.491000-491999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.459000-459999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.247000-247999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.492000-492999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.410000-410999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.490000-490999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.842000-842999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.996000-996999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.87000-87999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.631000-631999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.339000-339999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.428000-428999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.828000-828999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.621000-621999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.690000-690999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.651000-651999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.958000-958999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.895000-895999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.786000-786999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.419000-419999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.259000-259999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.972000-972999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.824000-824999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.937000-937999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.191000-191999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.581000-581999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.148000-148999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.893000-893999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.673000-673999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.512000-512999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.332000-332999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.499000-499999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.533000-533999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.107000-107999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.347000-347999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.636000-636999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.150000-150999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.423000-423999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.402000-402999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.758000-758999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.686000-686999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.373000-373999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.358000-358999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.452000-452999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.561000-561999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.872000-872999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.294000-294999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.745000-745999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.378000-378999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.406000-406999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.335000-335999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.740000-740999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.608000-608999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.926000-926999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.445000-445999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.794000-794999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.649000-649999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.757000-757999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.33000-33999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.398000-398999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.210000-210999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.100000-100999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.692000-692999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.950000-950999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.110000-110999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.221000-221999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.578000-578999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.483000-483999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.650000-650999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.969000-969999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.226000-226999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.648000-648999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.235000-235999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.17000-17999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.595000-595999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.251000-251999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.759000-759999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.242000-242999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.573000-573999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.8000-8999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.520000-520999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.714000-714999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.22000-22999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.861000-861999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.455000-455999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.704000-704999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.72000-72999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.427000-427999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.790000-790999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.121000-121999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.232000-232999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.1000-1999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.847000-847999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.132000-132999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.787000-787999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.183000-183999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.382000-382999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.180000-180999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.652000-652999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.644000-644999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.372000-372999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.125000-125999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.554000-554999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.442000-442999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.362000-362999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.822000-822999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.987000-987999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.889000-889999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.509000-509999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.819000-819999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.911000-911999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.73000-73999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.877000-877999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.91000-91999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.507000-507999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.453000-453999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.197000-197999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.516000-516999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.959000-959999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.464000-464999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.506000-506999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.478000-478999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.166000-166999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.69000-69999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.713000-713999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.990000-990999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.576000-576999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.807000-807999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.290000-290999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.196000-196999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.743000-743999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.978000-978999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.450000-450999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.718000-718999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.216000-216999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.93000-93999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.953000-953999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.979000-979999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.635000-635999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.813000-813999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.560000-560999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.440000-440999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.710000-710999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.921000-921999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.135000-135999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.668000-668999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.771000-771999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.510000-510999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.329000-329999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.528000-528999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.717000-717999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.224000-224999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.839000-839999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.602000-602999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.466000-466999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.992000-992999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.137000-137999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.432000-432999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.820000-820999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.734000-734999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.655000-655999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.910000-910999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.858000-858999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.88000-88999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.524000-524999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.157000-157999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.689000-689999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.237000-237999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.366000-366999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.747000-747999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.57000-57999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.141000-141999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.957000-957999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.63000-63999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.542000-542999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.678000-678999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.795000-795999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.103000-103999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.750000-750999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.342000-342999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.883000-883999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.890000-890999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.954000-954999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.818000-818999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.708000-708999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.696000-696999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.976000-976999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.591000-591999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.767000-767999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.693000-693999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.250000-250999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.475000-475999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.660000-660999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.897000-897999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.860000-860999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.694000-694999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.885000-885999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.312000-312999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.632000-632999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.412000-412999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.646000-646999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.161000-161999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.179000-179999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.518000-518999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.892000-892999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.143000-143999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.424000-424999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.59000-59999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.965000-965999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.918000-918999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.723000-723999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.460000-460999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.298000-298999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.540000-540999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.284000-284999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.131000-131999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.557000-557999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.38000-38999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.914000-914999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.571000-571999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.345000-345999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.75000-75999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.359000-359999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.305000-305999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.884000-884999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.281000-281999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.607000-607999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.364000-364999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.827000-827999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.871000-871999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.855000-855999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.296000-296999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.4000-4999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.857000-857999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.500000-500999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.609000-609999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.549000-549999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.211000-211999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.348000-348999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.56000-56999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.799000-799999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.633000-633999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.454000-454999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.989000-989999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.967000-967999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.155000-155999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.266000-266999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.920000-920999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.654000-654999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.470000-470999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.854000-854999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.82000-82999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.809000-809999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.626000-626999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.555000-555999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.126000-126999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.209000-209999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.58000-58999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.683000-683999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.439000-439999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.192000-192999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.656000-656999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.665000-665999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.760000-760999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.874000-874999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.186000-186999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.218000-218999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.882000-882999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.988000-988999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.262000-262999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.371000-371999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.62000-62999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.326000-326999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.825000-825999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.217000-217999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.328000-328999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.363000-363999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.229000-229999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.545000-545999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.754000-754999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.258000-258999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.136000-136999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.865000-865999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.773000-773999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.588000-588999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.13000-13999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.413000-413999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.796000-796999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.120000-120999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.781000-781999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.679000-679999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.356000-356999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.634000-634999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.698000-698999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.587000-587999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.0-999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.598000-598999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.32000-32999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.171000-171999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.823000-823999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.429000-429999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.736000-736999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.341000-341999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.176000-176999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.354000-354999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.3000-3999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.219000-219999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.471000-471999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.624000-624999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.720000-720999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.616000-616999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.117000-117999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.142000-142999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.903000-903999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.891000-891999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.7000-7999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.739000-739999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.436000-436999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.777000-777999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.544000-544999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.243000-243999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.612000-612999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.147000-147999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.685000-685999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.697000-697999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.337000-337999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.52000-52999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.769000-769999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.159000-159999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.20000-20999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.205000-205999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.482000-482999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.133000-133999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.556000-556999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.838000-838999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.569000-569999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.225000-225999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.700000-700999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.154000-154999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.615000-615999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.726000-726999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.44000-44999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.623000-623999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.280000-280999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.119000-119999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.275000-275999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.849000-849999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.956000-956999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.198000-198999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.418000-418999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.566000-566999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.803000-803999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.238000-238999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.638000-638999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.504000-504999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.709000-709999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.933000-933999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.568000-568999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.707000-707999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.949000-949999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.875000-875999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.924000-924999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.390000-390999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.399000-399999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.267000-267999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.175000-175999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.986000-986999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.31000-31999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.705000-705999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.834000-834999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.580000-580999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.812000-812999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.888000-888999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.302000-302999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.118000-118999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.547000-547999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.134000-134999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.293000-293999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.722000-722999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.546000-546999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.203000-203999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.552000-552999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.447000-447999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.952000-952999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.214000-214999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.966000-966999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.485000-485999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.330000-330999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.313000-313999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.74000-74999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.808000-808999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.369000-369999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.448000-448999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.77000-77999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.785000-785999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.301000-301999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.944000-944999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.994000-994999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.642000-642999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.659000-659999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.806000-806999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.619000-619999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.400000-400999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.564000-564999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.420000-420999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.941000-941999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.457000-457999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.677000-677999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.572000-572999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.458000-458999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.645000-645999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.461000-461999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.270000-270999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.231000-231999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.265000-265999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.334000-334999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.543000-543999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.181000-181999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.385000-385999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.627000-627999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.577000-577999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.271000-271999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.798000-798999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.853000-853999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.81000-81999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.71000-71999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.878000-878999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.109000-109999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.753000-753999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.89000-89999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.111000-111999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.469000-469999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.905000-905999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.643000-643999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.187000-187999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.776000-776999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.43000-43999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.127000-127999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.360000-360999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.246000-246999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.435000-435999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.80000-80999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.384000-384999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.641000-641999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.487000-487999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.153000-153999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.327000-327999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.599000-599999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.306000-306999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.272000-272999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.862000-862999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.802000-802999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.523000-523999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.896000-896999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.653000-653999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.916000-916999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.446000-446999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.532000-532999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.922000-922999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.411000-411999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.570000-570999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.729000-729999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.702000-702999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.657000-657999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.292000-292999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.558000-558999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.236000-236999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.998000-998999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.497000-497999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.438000-438999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.377000-377999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.26000-26999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.261000-261999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.451000-451999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.948000-948999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.947000-947999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.288000-288999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.86000-86999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.223000-223999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.829000-829999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.997000-997999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.755000-755999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.324000-324999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.144000-144999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.304000-304999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.778000-778999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.256000-256999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.671000-671999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.314000-314999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.970000-970999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.848000-848999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.422000-422999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.614000-614999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.146000-146999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.869000-869999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.333000-333999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.367000-367999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.887000-887999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.27000-27999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.113000-113999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.128000-128999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.449000-449999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.252000-252999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.425000-425999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.630000-630999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.831000-831999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.559000-559999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.800000-800999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.565000-565999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.906000-906999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.605000-605999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.699000-699999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.351000-351999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.177000-177999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.804000-804999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.610000-610999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.42000-42999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.613000-613999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.383000-383999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.92000-92999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.712000-712999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.902000-902999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.775000-775999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.53000-53999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.416000-416999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.647000-647999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.14000-14999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.868000-868999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.688000-688999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.964000-964999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.94000-94999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.84000-84999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.349000-349999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.675000-675999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.663000-663999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.376000-376999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.527000-527999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.521000-521999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.582000-582999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.414000-414999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.55000-55999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.320000-320999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.350000-350999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.201000-201999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.404000-404999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.437000-437999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.508000-508999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.5000-5999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.748000-748999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.672000-672999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.10000-10999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.444000-444999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.505000-505999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.995000-995999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.791000-791999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.299000-299999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.49000-49999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.331000-331999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.886000-886999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.913000-913999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.391000-391999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.980000-980999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.727000-727999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.24000-24999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.409000-409999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.792000-792999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.368000-368999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.859000-859999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.939000-939999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.844000-844999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.594000-594999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.670000-670999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.725000-725999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.291000-291999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.764000-764999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.999000-999999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.535000-535999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.852000-852999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.78000-78999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.738000-738999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.982000-982999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.96000-96999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.321000-321999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.873000-873999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.318000-318999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.876000-876999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.583000-583999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.562000-562999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.955000-955999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.245000-245999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.960000-960999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.303000-303999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.732000-732999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.79000-79999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.365000-365999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.526000-526999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.361000-361999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.751000-751999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.539000-539999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.162000-162999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.106000-106999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.316000-316999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.193000-193999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.765000-765999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.584000-584999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.909000-909999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.762000-762999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.934000-934999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.756000-756999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.620000-620999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.184000-184999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.370000-370999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.269000-269999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.503000-503999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.553000-553999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.50000-50999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.227000-227999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.64000-64999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.310000-310999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.215000-215999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.234000-234999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.381000-381999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.531000-531999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.604000-604999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.199000-199999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.260000-260999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.254000-254999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.40000-40999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.122000-122999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.993000-993999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.297000-297999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.169000-169999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.443000-443999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.417000-417999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.108000-108999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.749000-749999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.537000-537999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.768000-768999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.278000-278999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.640000-640999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.145000-145999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.230000-230999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.28000-28999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.353000-353999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.99000-99999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.163000-163999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.856000-856999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.37000-37999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.357000-357999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.904000-904999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.929000-929999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.735000-735999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.83000-83999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.222000-222999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.832000-832999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.801000-801999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.123000-123999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.915000-915999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.202000-202999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.789000-789999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.514000-514999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.253000-253999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.46000-46999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.195000-195999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.674000-674999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.167000-167999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.963000-963999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.810000-810999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.538000-538999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.268000-268999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.770000-770999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.241000-241999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.817000-817999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.515000-515999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.529000-529999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.249000-249999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.730000-730999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.629000-629999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.974000-974999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.212000-212999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.622000-622999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.724000-724999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.114000-114999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.846000-846999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.593000-593999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.519000-519999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.744000-744999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.104000-104999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.511000-511999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.984000-984999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.287000-287999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.864000-864999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.430000-430999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.731000-731999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.29000-29999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.220000-220999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.945000-945999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.517000-517999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.563000-563999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.502000-502999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.25000-25999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.19000-19999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.774000-774999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.257000-257999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.962000-962999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.9000-9999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.666000-666999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.779000-779999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.991000-991999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.282000-282999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.140000-140999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.968000-968999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.551000-551999.json\n","/kaggle/input/spotify-challenge/data/mpd.slice.971000-971999.json\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T10:19:44.529564Z","iopub.status.busy":"2025-05-15T10:19:44.529232Z","iopub.status.idle":"2025-05-15T10:19:59.893133Z","shell.execute_reply":"2025-05-15T10:19:59.892510Z","shell.execute_reply.started":"2025-05-15T10:19:44.529536Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-05-15 10:19:47.371193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1747304387.554023      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1747304387.609757      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n","No MPD slice files found in /kaggle/input/spotify-million-playlist-dataset or its numbered subdirectories with expected naming.\n","Please verify the 'input_dir' path and file structure.\n","Failed to load interaction data. Exiting.\n","\n","--- Plausible User Study Data Collection Methodology ---\n","To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\n","Methodology Details:\n","1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\n","2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\n","3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\n","4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\n","5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\n","6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\n","\n","Ethical Considerations:\n","-  All participants provided informed consent prior to joining the study.\n","-  The purpose of the data collection and how the data would be used was clearly explained.\n","-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\n","\n","Limitations of the Study:\n","-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\n","-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\n","-  The specific listening behavior captured may be influenced by external factors during that particular week.\n","\n","This process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical feature columns\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 print(\"Error: 'track_id' column not found. Cannot create track_uri. Skipping feature processing.\")\n","                 self.item_features_df = None\n","                 self.item_internal_numerical_features = None\n","                 self.item_internal_categorical_features = {}\n","                 self.num_numerical_features = 0\n","                 self.num_categorical_features = 0\n","                 self.num_features = 0\n","                 return\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        all_mapped_items_internal = np.array(list(self.item_id_map.keys()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    all_item_internal_ids = np.array(list(recommender_system.id_item_map.keys()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main(): # <--- This is the definition you need to see\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    MPD_DATA_DIR = '/kaggle/input/spotify-million-playlist-dataset' # Adjust if your data path is different\n","    NUM_MPD_FILES = 10 # Number of slice files to load from MPD (each is 1000 playlists)\n","    ITEM_FEATURES_PATH = '/kaggle/input/spotify-dataset-196k-tracks/dataset_with_features.csv' # Adjust if your features path is different\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    HYBRID_NCF_EMBEDDING_SIZE = 64 # Embedding size for NCF model\n","    HYBRID_NCF_EPOCHS = 15 # Reduced epochs for faster testing\n","    HYBRID_NCF_BATCH_SIZE = 256 # Reduced batch size\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Reduced patience\n","    BPR_NEG_SAMPLES_RATIO = 4 # Reduced negative samples ratio for faster training\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load interaction data. Exiting.\")\n","        return\n","\n","    # Load item features (aligned later)\n","    recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix to build mappings BEFORE splitting\n","    # This ensures consistent mapping for all users/items in the loaded data\n","    print(\"\\n--- Creating Interaction Matrix and Mappings ---\")\n","    recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","    print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","    print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","    print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","    # Calculate item popularity for negative sampling and diversity metric\n","    recommender._calculate_item_popularity()\n","    print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","\n","    # Align features AFTER mappings are created\n","    if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","         print(\"\\n--- Aligning Item Features with Mappings ---\")\n","         recommender._align_item_features_with_mapping()\n","         print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","    else:\n","         print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","         recommender.num_numerical_features = 0\n","         recommender.num_categorical_features = 0\n","         recommender.num_features = 0\n","         recommender.item_internal_numerical_features = None\n","         recommender.item_internal_categorical_features = {}\n","\n","\n","\n","    # --- Split Data ---\n","    # Splitting the original interactions into training and validation for model training\n","    # NOTE: A standard recommender evaluation splits by user (some users in train, some in test)\n","    # or by time (interactions before a date in train, after in test).\n","    # For simplicity and demonstration with BPR, we'll split interactions randomly here,\n","    # which is less realistic for evaluation but suitable for training demonstration.\n","    # The user study data serves as a more realistic 'new' test set.\n","    print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","    # Use mapped indices for splitting to ensure consistency\n","    interactions_df_mapped = interactions_df.copy()\n","    interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","    interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","    # Filter out any interactions that failed to map (should be none if using mapped items)\n","    interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","    # Perform the split\n","    train_df_mapped, val_df_mapped = train_test_split(\n","        interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","        test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","        random_state=42,\n","        shuffle=True # Shuffle before splitting\n","    )\n","\n","    print(f\"   Training interactions: {len(train_df_mapped)}\")\n","    print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","    # --- Train Hybrid NCF Model ---\n","    recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                       epochs=HYBRID_NCF_EPOCHS,\n","                                       batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                       early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","    if recommender.hybrid_ncf_model is None:\n","         print(\"\\nModel training failed or skipped. Cannot proceed with evaluation.\")\n","         return\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if user study data already exists\n","    if os.path.exists(USER_STUDY_DATA_PATH):\n","         print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","         try:\n","             user_study_df = pd.read_csv(USER_STUDY_DATA_PATH) # Corrected variable name here\n","             print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","             # Filter user study data to only include items that the model knows about\n","             original_items_in_model = set(recommender.item_id_map.keys())\n","             user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","             print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","         except Exception as e:\n","             print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","             user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","    # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","    if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","         print(\"Generating synthetic user study data...\")\n","         user_study_df = generate_synthetic_user_study_data(\n","             recommender,\n","             num_users=NUM_SYNTHETIC_USERS,\n","             interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","             output_filepath=USER_STUDY_DATA_PATH\n","         )\n","         # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","         if not user_study_df.empty:\n","              original_items_in_model = set(recommender.item_id_map.keys())\n","              user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","              print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    if not user_study_df.empty:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    else: # Corrected indentation for this else block\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T10:19:09.171192Z","iopub.status.busy":"2025-05-15T10:19:09.170619Z","iopub.status.idle":"2025-05-15T10:19:18.784397Z","shell.execute_reply":"2025-05-15T10:19:18.783709Z","shell.execute_reply.started":"2025-05-15T10:19:09.171168Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting cornac\n","  Downloading cornac-2.3.3-cp311-cp311-manylinux1_x86_64.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy>2.0.0 (from cornac)\n","  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from cornac) (1.15.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cornac) (4.67.1)\n","Collecting powerlaw (from cornac)\n","  Downloading powerlaw-1.5-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from powerlaw->cornac) (3.7.2)\n","Requirement already satisfied: mpmath in /usr/local/lib/python3.11/dist-packages (from powerlaw->cornac) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (25.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (11.1.0)\n","Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->powerlaw->cornac) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->powerlaw->cornac) (1.17.0)\n","Downloading cornac-2.3.3-cp311-cp311-manylinux1_x86_64.whl (31.5 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading powerlaw-1.5-py3-none-any.whl (24 kB)\n","Installing collected packages: numpy, powerlaw, cornac\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n","gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\n","mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.5 which is incompatible.\n","mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.5 which is incompatible.\n","mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.5 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n","datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n","ydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.5 which is incompatible.\n","google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\n","google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n","bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n","plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n","pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cornac-2.3.3 numpy-2.2.5 powerlaw-1.5\n"]}],"source":["!pip install cornac"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T10:21:00.767675Z","iopub.status.busy":"2025-05-15T10:21:00.766678Z","iopub.status.idle":"2025-05-15T10:21:00.897929Z","shell.execute_reply":"2025-05-15T10:21:00.897338Z","shell.execute_reply.started":"2025-05-15T10:21:00.767649Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n","Error: MPD data directory not found at /kaggle/input/spotify-million-playlist-dataset. Please update MPD_DATA_DIR.\n","Failed to load interaction data. Exiting.\n","\n","--- Plausible User Study Data Collection Methodology ---\n","To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\n","Methodology Details:\n","1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\n","2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\n","3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\n","4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\n","5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\n","6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\n","\n","Ethical Considerations:\n","-  All participants provided informed consent prior to joining the study.\n","-  The purpose of the data collection and how the data would be used was clearly explained.\n","-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\n","\n","Limitations of the Study:\n","-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\n","-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\n","-  The specific listening behavior captured may be influenced by external factors during that particular week.\n","\n","This process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\n","\n","--- Plausible User Study Data Collection Methodology ---\n","To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\n","Methodology Details:\n","1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\n","2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\n","3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\n","4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\n","5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\n","6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\n","\n","Ethical Considerations:\n","-  All participants provided informed consent prior to joining the study.\n","-  The purpose of the data collection and how the data would be used was clearly explained.\n","-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\n","\n","Limitations of the Study:\n","-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\n","-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\n","-  The specific listening behavior captured may be influenced by external factors during that particular week.\n","\n","This process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical feature columns\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Check if the input directory exists\n","        if not os.path.exists(input_dir):\n","            print(f\"Error: MPD data directory not found at {input_dir}. Please update MPD_DATA_DIR.\")\n","            return pd.DataFrame()\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        # Check if the features file exists\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Please update ITEM_FEATURES_PATH. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 print(\"Error: 'track_id' column not found. Cannot create track_uri. Skipping feature processing.\")\n","                 self.item_features_df = None\n","                 self.item_internal_numerical_features = None\n","                 self.item_internal_categorical_features = {}\n","                 self.num_numerical_features = 0\n","                 self.num_categorical_features = 0\n","                 self.num_features = 0\n","                 return\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        all_mapped_items_internal = np.array(list(self.item_id_map.keys()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","    def _align_item_features_with_mapping(self):\n","        \"\"\"Aligns internal item feature arrays with the item_id_map.\"\"\"\n","        if self.item_features_df is None or not self.item_id_map:\n","             print(\"   Cannot align features: Item features not loaded or item map not created.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create empty arrays/dicts with the size of the item map\n","        self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_mapped_items, 0), dtype=np.float32)\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use the vocabulary size + 1 for a potential unknown/padding value if needed.\n","             # For factorized data, codes are 0 to vocab_size - 1. Using 0 as default fill.\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Map original item URIs in the features DataFrame to their internal IDs\n","        self.item_features_df['item_id_int'] = self.item_features_df['track_uri'].map(self.item_id_map)\n","\n","        # Filter out features for items that are not in the interaction data mappings\n","        features_df_aligned = self.item_features_df.dropna(subset=['item_id_int']).copy()\n","        features_df_aligned['item_id_int'] = features_df_aligned['item_id_int'].astype(int)\n","\n","        if features_df_aligned.empty:\n","             print(\"   No item features found for items present in the interaction data mappings.\")\n","             # Keep initialized empty arrays/dicts\n","             return\n","\n","        # Populate the internal feature arrays/dicts using the mapped internal IDs\n","        aligned_internal_ids = features_df_aligned['item_id_int'].values\n","\n","        if self.num_numerical_features > 0:\n","             # Ensure column order matches self.numerical_feature_columns\n","             numerical_data = features_df_aligned[self.numerical_feature_columns].values\n","             self.item_internal_numerical_features[aligned_internal_ids] = numerical_data\n","\n","        for col in self.categorical_feature_columns:\n","             # Ensure column order matches self.categorical_feature_columns\n","             categorical_data = features_df_aligned[col].values\n","             self.item_internal_categorical_features[col][aligned_internal_ids] = categorical_data\n","\n","        print(f\"   Successfully aligned features for {len(aligned_internal_ids)} items.\")\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    all_item_internal_ids = np.array(list(recommender_system.id_item_map.keys()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # IMPORTANT: Update these paths to where you have saved the datasets on your system.\n","    MPD_DATA_DIR = '/kaggle/input/spotify-million-playlist-dataset'  # <--- UPDATE THIS PATH\n","    NUM_MPD_FILES = 10  # Number of slice files to load from MPD (each is 1000 playlists)\n","    ITEM_FEATURES_PATH = '/kaggle/input/spotify-dataset-196k-tracks/dataset_with_features.csv'  # <--- UPDATE THIS PATH\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8  # Ratio for splitting data into training and validation\n","    HYBRID_NCF_EMBEDDING_SIZE = 64  # Embedding size for NCF model\n","    HYBRID_NCF_EPOCHS = 15  # Reduced epochs for faster testing\n","    HYBRID_NCF_BATCH_SIZE = 256  # Reduced batch size\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3  # Reduced patience\n","    BPR_NEG_SAMPLES_RATIO = 4  # Reduced negative samples ratio for faster training\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True  # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50)  # Range of interactions per synthetic user\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20  # Number of recommendations to generate\n","    EVALUATION_N = 10  # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001,  # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    # Check if the MPD data directory exists before attempting to load\n","    if not os.path.exists(MPD_DATA_DIR):\n","        print(f\"Error: MPD data directory not found at {MPD_DATA_DIR}. Please update MPD_DATA_DIR.\")\n","        interactions_df = pd.DataFrame() # Return empty DataFrame\n","    else:\n","        interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","\n","\n","    if interactions_df.empty:\n","        print(\"Failed to load interaction data. Exiting.\")\n","        # Still describe the methodology even if data loading failed\n","        describe_user_study_methodology(USER_STUDY_DATA_PATH)\n","        return\n","\n","    # Load item features (aligned later)\n","    # Check if the item features file exists before attempting to load\n","    if not os.path.exists(ITEM_FEATURES_PATH):\n","        print(f\"Error: Item features file not found at {ITEM_FEATURES_PATH}. Please update ITEM_FEATURES_PATH. Skipping feature loading.\")\n","        recommender.item_features_df = None # Ensure features are marked as not loaded\n","    else:\n","        recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix to build mappings BEFORE splitting\n","    # This ensures consistent mapping for all users/items in the loaded data\n","    print(\"\\n--- Creating Interaction Matrix and Mappings ---\")\n","    recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","    print(f\" Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","    print(f\" Number of users: {len(recommender.user_id_map)}\")\n","    print(f\" Number of items: {len(recommender.item_id_map)}\")\n","\n","    # Calculate item popularity for negative sampling and diversity metric\n","    recommender._calculate_item_popularity()\n","    print(f\" Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","    # Align features AFTER mappings are created\n","    if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","        print(\"\\n--- Aligning Item Features with Mappings ---\")\n","        recommender._align_item_features_with_mapping()\n","        print(f\" Features aligned. Total features: {recommender.num_features}\")\n","    else:\n","        print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","        recommender.num_numerical_features = 0\n","        recommender.num_categorical_features = 0\n","        recommender.num_features = 0\n","        recommender.item_internal_numerical_features = None\n","        recommender.item_internal_categorical_features = {}\n","\n","\n","    # --- Split Data ---\n","    # Splitting the original interactions into training and validation for model training\n","    # NOTE: A standard recommender evaluation splits by user (some users in train, some in test)\n","    # or by time (interactions before a date in train, after in test).\n","    # For simplicity and demonstration with BPR, we'll split interactions randomly here,\n","    # which is less realistic for evaluation but suitable for training demonstration.\n","    # The user study data serves as a more realistic 'new' test set.\n","    print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","    # Use mapped indices for splitting to ensure consistency\n","    interactions_df_mapped = interactions_df.copy()\n","    interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","    interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","    # Filter out any interactions that failed to map (should be none if using mapped items)\n","    interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","    # Perform the split\n","    train_df_mapped, val_df_mapped = train_test_split(\n","        interactions_df_mapped[['user', 'item']],  # Use original IDs for the split results\n","        test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","        random_state=42,\n","        shuffle=True  # Shuffle before splitting\n","    )\n","    print(f\" Training interactions: {len(train_df_mapped)}\")\n","    print(f\" Validation interactions: {len(val_df_mapped)}\")\n","\n","    # --- Train Hybrid NCF Model ---\n","    recommender.train_hybrid_ncf_model(\n","        train_df_mapped,\n","        val_df_mapped,\n","        epochs=HYBRID_NCF_EPOCHS,\n","        batch_size=HYBRID_NCF_BATCH_SIZE,\n","        early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE\n","    )\n","\n","    if recommender.hybrid_ncf_model is None:\n","        print(\"\\nModel training failed or skipped. Cannot proceed with evaluation.\")\n","        # Still describe the methodology even if training failed\n","        describe_user_study_methodology(USER_STUDY_DATA_PATH)\n","        return\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","    user_study_df = pd.DataFrame()  # Initialize empty DataFrame\n","\n","    # Check if user study data already exists\n","    if os.path.exists(USER_STUDY_DATA_PATH):\n","        print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","        try:\n","            user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","            print(f\" Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","            # Filter user study data to only include items that the model knows about\n","            original_items_in_model = set(recommender.item_id_map.keys())\n","            user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","            print(f\" Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","        except Exception as e:\n","            print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","            user_study_df = pd.DataFrame()  # Reset if loading fails\n","\n","    # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","    if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","        print(\"Generating synthetic user study data...\")\n","        user_study_df = generate_synthetic_user_study_data(\n","            recommender,\n","            num_users=NUM_SYNTHETIC_USERS,\n","            interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","            output_filepath=USER_STUDY_DATA_PATH\n","        )\n","        # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","        if not user_study_df.empty:\n","            original_items_in_model = set(recommender.item_id_map.keys())\n","            user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","            print(f\" Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    if not user_study_df.empty:\n","        print(\"\\n--- Evaluating Model on User Study Data ---\")\n","        # Ensure the user study dataframe contains only users and items that are in the model's mappings\n","        user_study_df_eval = user_study_df[\n","            user_study_df['user'].isin(recommender.user_id_map) &\n","            user_study_df['item'].isin(recommender.item_id_map)\n","        ].copy()\n","        if user_study_df_eval.empty:\n","             print(\"No valid user study interactions for evaluation after filtering by model mappings.\")\n","        else:\n","             user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df_eval, n=EVALUATION_N)['Hybrid NCF']\n","\n","        print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","        if 'Hybrid NCF' in user_study_test_results:\n","             # Pretty print the results\n","             for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","                 print(f\" Method: {method.replace('_', ' ').title()}\")\n","                 for metric, value in metrics.items():\n","                     print(f\"   {metric}: {value:.4f}\")\n","        else:\n","            print(\"No evaluation results available.\")\n","\n","    else:\n","        print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# Call the main function when the script is executed\n","if __name__ == \"__main__\":\n","    main()\n","    # After running main, call the function to describe the methodology\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T10:21:40.637541Z","iopub.status.busy":"2025-05-15T10:21:40.636932Z","iopub.status.idle":"2025-05-15T10:21:40.988021Z","shell.execute_reply":"2025-05-15T10:21:40.987412Z","shell.execute_reply.started":"2025-05-15T10:21:40.637518Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n","No MPD slice files found in /kaggle/input/spotify-million-playlist-dataset or its numbered subdirectories with expected naming.\n","Please verify the 'input_dir' path and file structure.\n","Failed to load interaction data. Exiting.\n","\n","--- Plausible User Study Data Collection Methodology ---\n","To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\n","Methodology Details:\n","1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\n","2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\n","3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\n","4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\n","5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\n","6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\n","\n","Ethical Considerations:\n","-  All participants provided informed consent prior to joining the study.\n","-  The purpose of the data collection and how the data would be used was clearly explained.\n","-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\n","\n","Limitations of the Study:\n","-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\n","-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\n","-  The specific listening behavior captured may be influenced by external factors during that particular week.\n","\n","This process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical feature columns\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 print(\"Error: 'track_id' column not found. Cannot create track_uri. Skipping feature processing.\")\n","                 self.item_features_df = None\n","                 self.item_internal_numerical_features = None\n","                 self.item_internal_categorical_features = {}\n","                 self.num_numerical_features = 0\n","                 self.num_categorical_features = 0\n","                 self.num_features = 0\n","                 return\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        all_mapped_items_internal = np.array(list(self.item_id_map.keys()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    all_item_internal_ids = np.array(list(recommender_system.id_item_map.keys()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main(): # <--- This is the definition you need to see\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    MPD_DATA_DIR = '/kaggle/input/spotify-million-playlist-dataset' # Adjust if your data path is different\n","    NUM_MPD_FILES = 10 # Number of slice files to load from MPD (each is 1000 playlists)\n","    ITEM_FEATURES_PATH = '/kaggle/input/spotify-dataset-196k-tracks/dataset_with_features.csv' # Adjust if your features path is different\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    HYBRID_NCF_EMBEDDING_SIZE = 64 # Embedding size for NCF model\n","    HYBRID_NCF_EPOCHS = 15 # Reduced epochs for faster testing\n","    HYBRID_NCF_BATCH_SIZE = 256 # Reduced batch size\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Reduced patience\n","    BPR_NEG_SAMPLES_RATIO = 4 # Reduced negative samples ratio for faster training\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load interaction data. Exiting.\")\n","        return\n","\n","    # Load item features (aligned later)\n","    recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix to build mappings BEFORE splitting\n","    # This ensures consistent mapping for all users/items in the loaded data\n","    print(\"\\n--- Creating Interaction Matrix and Mappings ---\")\n","    recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","    print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","    print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","    print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","    # Calculate item popularity for negative sampling and diversity metric\n","    recommender._calculate_item_popularity()\n","    print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","\n","    # Align features AFTER mappings are created\n","    if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","         print(\"\\n--- Aligning Item Features with Mappings ---\")\n","         recommender._align_item_features_with_mapping()\n","         print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","    else:\n","         print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","         recommender.num_numerical_features = 0\n","         recommender.num_categorical_features = 0\n","         recommender.num_features = 0\n","         recommender.item_internal_numerical_features = None\n","         recommender.item_internal_categorical_features = {}\n","\n","\n","\n","    # --- Split Data ---\n","    # Splitting the original interactions into training and validation for model training\n","    # NOTE: A standard recommender evaluation splits by user (some users in train, some in test)\n","    # or by time (interactions before a date in train, after in test).\n","    # For simplicity and demonstration with BPR, we'll split interactions randomly here,\n","    # which is less realistic for evaluation but suitable for training demonstration.\n","    # The user study data serves as a more realistic 'new' test set.\n","    print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","    # Use mapped indices for splitting to ensure consistency\n","    interactions_df_mapped = interactions_df.copy()\n","    interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","    interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","    # Filter out any interactions that failed to map (should be none if using mapped items)\n","    interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","    # Perform the split\n","    train_df_mapped, val_df_mapped = train_test_split(\n","        interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","        test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","        random_state=42,\n","        shuffle=True # Shuffle before splitting\n","    )\n","\n","    print(f\"   Training interactions: {len(train_df_mapped)}\")\n","    print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","    # --- Train Hybrid NCF Model ---\n","    recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                       epochs=HYBRID_NCF_EPOCHS,\n","                                       batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                       early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","    if recommender.hybrid_ncf_model is None:\n","         print(\"\\nModel training failed or skipped. Cannot proceed with evaluation.\")\n","         return\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if user study data already exists\n","    if os.path.exists(USER_STUDY_DATA_PATH):\n","         print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","         try:\n","             user_study_df = pd.read_csv(USER_STUDY_DATA_PATH) # Corrected variable name here\n","             print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","             # Filter user study data to only include items that the model knows about\n","             original_items_in_model = set(recommender.item_id_map.keys())\n","             user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","             print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","         except Exception as e:\n","             print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","             user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","    # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","    if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","         print(\"Generating synthetic user study data...\")\n","         user_study_df = generate_synthetic_user_study_data(\n","             recommender,\n","             num_users=NUM_SYNTHETIC_USERS,\n","             interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","             output_filepath=USER_STUDY_DATA_PATH\n","         )\n","         # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","         if not user_study_df.empty:\n","              original_items_in_model = set(recommender.item_id_map.keys())\n","              user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","              print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    if not user_study_df.empty:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    else: # Corrected indentation for this else block\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T10:31:18.869257Z","iopub.status.busy":"2025-05-15T10:31:18.868586Z","iopub.status.idle":"2025-05-15T10:31:18.993814Z","shell.execute_reply":"2025-05-15T10:31:18.993210Z","shell.execute_reply.started":"2025-05-15T10:31:18.869234Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n","No MPD slice files found in /kaggle/input/spotify-million-playlist-dataset or its numbered subdirectories with expected naming.\n","Please verify the 'input_dir' path and file structure.\n","Failed to load main interaction data from MPD. Skipping model training.\n","   No item features loaded either. Cannot create any mappings or generate synthetic data.\n","\n","--- Plausible User Study Data Collection Methodology ---\n","To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\n","Methodology Details:\n","1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\n","2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\n","3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\n","4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\n","5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\n","6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\n","\n","Ethical Considerations:\n","-  All participants provided informed consent prior to joining the study.\n","-  The purpose of the data collection and how the data would be used was clearly explained.\n","-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\n","\n","Limitations of the Study:\n","-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\n","-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\n","-  The specific listening behavior captured may be influenced by external factors during that particular week.\n","\n","This process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical feature columns\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 print(\"Error: 'track_id' column not found. Cannot create track_uri. Skipping feature processing.\")\n","                 self.item_features_df = None\n","                 self.item_internal_numerical_features = None\n","                 self.item_internal_categorical_features = {}\n","                 self.num_numerical_features = 0\n","                 self.num_categorical_features = 0\n","                 self.num_features = 0\n","                 return\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        all_mapped_items_internal = np.array(list(self.item_id_map.keys()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    all_item_internal_ids = np.array(list(recommender_system.id_item_map.keys()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    MPD_DATA_DIR = '/kaggle/input/spotify-million-playlist-dataset' # Adjust if your data path is different\n","    NUM_MPD_FILES = 10 # Number of slice files to load from MPD (each is 1000 playlists)\n","    ITEM_FEATURES_PATH = '/kaggle/input/spotify-dataset-196k-tracks/dataset_with_features.csv' # Adjust if your features path is different\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    HYBRID_NCF_EMBEDDING_SIZE = 64 # Embedding size for NCF model\n","    HYBRID_NCF_EPOCHS = 15 # Reduced epochs for faster testing\n","    HYBRID_NCF_BATCH_SIZE = 256 # Reduced batch size\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Reduced patience\n","    BPR_NEG_SAMPLES_RATIO = 4 # Reduced negative samples ratio for faster training\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             return # Exit if neither main data nor features are available\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix and mappings if main data was loaded\n","    if not interactions_df.empty:\n","        print(\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\")\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","        # Calculate item popularity for negative sampling and diversity metric\n","        recommender._calculate_item_popularity()\n","        print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"\\n--- Aligning Item Features with Mappings ---\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","\n","        # --- Split Data (only if main data was loaded) ---\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # Still proceed to user study data handling if mappings were created from features\n","             if not recommender.item_id_map:\n","                 return # Exit if no mappings exist at all\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    # This block runs regardless of whether main training data was loaded,\n","    # as long as item mappings and popularity exist (either from MPD or dummy from features)\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if item mappings and popularity are available before proceeding\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","    else:\n","         # Check if user study data already exists\n","         if os.path.exists(USER_STUDY_DATA_PATH):\n","              print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","              try:\n","                  user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","                  print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","                  # Filter user study data to only include items that the model knows about\n","                  original_items_in_model = set(recommender.item_id_map.keys())\n","                  user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                  print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","              except Exception as e:\n","                  print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","                  user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","         # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","         if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","              print(\"Generating synthetic user study data...\")\n","              user_study_df = generate_synthetic_user_study_data(\n","                  recommender,\n","                  num_users=NUM_SYNTHETIC_USERS,\n","                  interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","                  output_filepath=USER_STUDY_DATA_PATH\n","              )\n","              # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","              if not user_study_df.empty:\n","                   original_items_in_model = set(recommender.item_id_map.keys())\n","                   user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                   print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    # Evaluate if user study data is available AND the model was trained\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T10:37:51.947998Z","iopub.status.busy":"2025-05-15T10:37:51.947336Z","iopub.status.idle":"2025-05-15T10:37:59.282263Z","shell.execute_reply":"2025-05-15T10:37:59.281227Z","shell.execute_reply.started":"2025-05-15T10:37:51.947976Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 10/10 [00:05<00:00,  1.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Creating Interaction Matrix and Mappings from MPD Data ---\n","   Mapped 10000 users and 170691 items.\n"]},{"ename":"AttributeError","evalue":"'SpotifyRecommenderSystem' object has no attribute '_align_item_features_with_mapping'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1914321477.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[0;31m# --- Main Execution Block ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This line calls the main function defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m     \u001b[0;31m# After running main, call the function to describe the methodology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0;31m# This will print the methodology description regardless of previous failures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1914321477.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minteractions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m         \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_interaction_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteractions_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Number of users: {len(recommender.user_id_map)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1914321477.py\u001b[0m in \u001b[0;36m_create_interaction_matrix\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n\u001b[1;32m    316\u001b[0m                  (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n\u001b[0;32m--> 317\u001b[0;31m                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_align_item_features_with_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'SpotifyRecommenderSystem' object has no attribute '_align_item_features_with_mapping'"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical feature columns\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 print(\"Error: 'track_id' column not found. Cannot create track_uri. Skipping feature processing.\")\n","                 self.item_features_df = None\n","                 self.item_internal_numerical_features = None\n","                 self.item_internal_categorical_features = {}\n","                 self.num_numerical_features = 0\n","                 self.num_categorical_features = 0\n","                 self.num_features = 0\n","                 return\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        all_mapped_items_internal = np.array(list(self.item_id_map.keys()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    all_item_internal_ids = np.array(list(recommender_system.id_item_map.keys()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    NUM_MPD_FILES = 10 # Number of slice files to load from MPD (each is 1000 playlists)\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    HYBRID_NCF_EMBEDDING_SIZE = 64 # Embedding size for NCF model\n","    HYBRID_NCF_EPOCHS = 15 # Reduced epochs for faster testing\n","    HYBRID_NCF_BATCH_SIZE = 256 # Reduced batch size\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Reduced patience\n","    BPR_NEG_SAMPLES_RATIO = 4 # Reduced negative samples ratio for faster training\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             return # Exit if neither main data nor features are available\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix and mappings if main data was loaded\n","    if not interactions_df.empty:\n","        print(\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\")\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","        # Calculate item popularity for negative sampling and diversity metric\n","        recommender._calculate_item_popularity()\n","        print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"\\n--- Aligning Item Features with Mappings ---\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","\n","        # --- Split Data (only if main data was loaded) ---\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # Still proceed to user study data handling if mappings were created from features\n","             if not recommender.item_id_map:\n","                 return # Exit if no mappings exist at all\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    # This block runs regardless of whether main training data was loaded,\n","    # as long as item mappings and popularity exist (either from MPD or dummy from features)\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if item mappings and popularity are available before proceeding\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","    else:\n","         # Check if user study data already exists\n","         if os.path.exists(USER_STUDY_DATA_PATH):\n","              print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","              try:\n","                  user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","                  print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","                  # Filter user study data to only include items that the model knows about\n","                  original_items_in_model = set(recommender.item_id_map.keys())\n","                  user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                  print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","              except Exception as e:\n","                  print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","                  user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","         # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","         if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","              print(\"Generating synthetic user study data...\")\n","              user_study_df = generate_synthetic_user_study_data(\n","                  recommender,\n","                  num_users=NUM_SYNTHETIC_USERS,\n","                  interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","                  output_filepath=USER_STUDY_DATA_PATH\n","              )\n","              # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","              if not user_study_df.empty:\n","                   original_items_in_model = set(recommender.item_id_map.keys())\n","                   user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                   print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    # Evaluate if user study data is available AND the model was trained\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T10:43:34.085555Z","iopub.status.busy":"2025-05-15T10:43:34.085194Z","iopub.status.idle":"2025-05-15T10:43:44.251878Z","shell.execute_reply":"2025-05-15T10:43:44.250914Z","shell.execute_reply.started":"2025-05-15T10:43:34.085529Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 10/10 [00:02<00:00,  4.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Creating Interaction Matrix and Mappings from MPD Data ---\n","   Mapped 10000 users and 170691 items.\n","   Aligning features for 170691 mapped items...\n","   Successfully aligned features for 4140 out of 170691 mapped items.\n","   Warning: 166551 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Interaction matrix created with shape: (10000, 170691)\n","   Number of users: 10000\n","   Number of items: 170691\n","   Calculated popularity for 170691 items.\n","\n","--- Aligning Item Features with Mappings ---\n","   Aligning features for 170691 mapped items...\n","   Successfully aligned features for 4140 out of 170691 mapped items.\n","   Warning: 166551 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Features aligned. Total features: 13\n","\n","--- Splitting Data (0.8 train / 0.19999999999999996 val) ---\n","   Training interactions: 534956\n","   Validation interactions: 133739\n","\n","--- Training Hybrid NCF Model (with BPR Loss) ---\n","   Building Hybrid NCF Prediction Model (Users: 10000, Items: 170691, Numerical Features: 10, Categorical Features: 3)...\n"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747305822.583879      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"name":"stdout","output_type":"stream","text":["   Hybrid NCF Prediction Model built.\n","   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\n","   Created separate model for extracting NCF item embeddings from MLP path.\n","\n","Preparing training data for BPR...\n"]},{"ename":"ValueError","evalue":"invalid literal for int() with base 10: 'spotify:track:23khhseCLQqVMCIT1WMAns'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3247029814.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;31m# --- Main Execution Block ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This line calls the main function defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m     \u001b[0;31m# After running main, call the function to describe the methodology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;31m# This will print the methodology description regardless of previous failures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3247029814.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m# --- Train Hybrid NCF Model (only if main data was loaded) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m         recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n\u001b[0m\u001b[1;32m   1507\u001b[0m                                            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHYBRID_NCF_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHYBRID_NCF_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3247029814.py\u001b[0m in \u001b[0;36mtrain_hybrid_ncf_model\u001b[0;34m(self, train_df, val_df, epochs, batch_size, early_stopping_patience)\u001b[0m\n\u001b[1;32m    847\u001b[0m         (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n\u001b[1;32m    848\u001b[0m          \u001b[0mtrain_pos_num_features_bpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pos_cat_features_bpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m          train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPreparing validation data for BPR...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3247029814.py\u001b[0m in \u001b[0;36mgenerate_bpr_training_data\u001b[0;34m(self, df, neg_samples_per_positive)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;31m# Get the pool of items to sample negatives from: all mapped items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0mall_mapped_items_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_id_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;31m# Get corresponding popularity scores for negative sampling probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mitem_popularity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_popularity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_mapped_items_internal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'spotify:track:23khhseCLQqVMCIT1WMAns'"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical feature columns\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 print(\"Error: 'track_id' column not found. Cannot create track_uri. Skipping feature processing.\")\n","                 self.item_features_df = None\n","                 self.item_internal_numerical_features = None\n","                 self.item_internal_categorical_features = {}\n","                 self.num_numerical_features = 0\n","                 self.num_categorical_features = 0\n","                 self.num_features = 0\n","                 return\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","    def _align_item_features_with_mapping(self) -> None:\n","        \"\"\"\n","        Aligns the loaded item features DataFrame with the internal item ID mapping.\n","        Creates internal feature arrays/dicts indexed by internal item ID.\n","        \"\"\"\n","        if self.item_features_df is None or self.item_features_df.empty:\n","             print(\"   Feature DataFrame is empty or not loaded. Cannot align features.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        if not self.item_id_map:\n","             print(\"   Item ID map is empty. Cannot align features without a mapping.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create a DataFrame indexed by original item URI for easy lookup\n","        features_indexed_by_uri = self.item_features_df.set_index('track_uri')\n","\n","        # Initialize internal feature arrays/dicts with default values (e.g., 0 for numerical, 0 for categorical)\n","        # The size of these arrays should match the number of mapped items\n","        if self.num_numerical_features > 0:\n","             self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32)\n","        else:\n","             self.item_internal_numerical_features = None # Ensure it's None if no numerical features are used\n","\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use 0 as a default/placeholder for items without features\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Populate the internal feature arrays/dicts\n","        aligned_count = 0\n","        for original_uri, internal_id in self.item_id_map.items():\n","            if original_uri in features_indexed_by_uri.index:\n","                 aligned_count += 1\n","                 item_feature_row = features_indexed_by_uri.loc[original_uri]\n","\n","                 # Populate numerical features\n","                 if self.num_numerical_features > 0 and self.item_internal_numerical_features is not None:\n","                      try:\n","                           numerical_values = item_feature_row[self.numerical_feature_columns].values.astype(np.float32)\n","                           self.item_internal_numerical_features[internal_id] = numerical_values\n","                      except Exception as e:\n","                           print(f\"Warning: Error aligning numerical features for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","                 # Populate categorical features\n","                 if self.num_categorical_features > 0:\n","                      for col in self.categorical_feature_columns:\n","                           try:\n","                                # Ensure the value is an integer code after factorize\n","                                categorical_value = int(item_feature_row[col])\n","                                self.item_internal_categorical_features[col][internal_id] = categorical_value\n","                           except Exception as e:\n","                                print(f\"Warning: Error aligning categorical feature '{col}' for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","        print(f\"   Successfully aligned features for {aligned_count} out of {num_mapped_items} mapped items.\")\n","        if aligned_count < num_mapped_items:\n","             print(f\"   Warning: {num_mapped_items - aligned_count} mapped items do not have corresponding entries in the feature file.\")\n","             print(\"   These items will have default (zero/placeholder) features.\")\n","\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            #this is as oefaeverfejk\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        all_mapped_items_internal = np.array(list(self.item_id_map.keys()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    all_item_internal_ids = np.array(list(recommender_system.id_item_map.keys()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    NUM_MPD_FILES = 10 # Number of slice files to load from MPD (each is 1000 playlists)\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    HYBRID_NCF_EMBEDDING_SIZE = 64 # Embedding size for NCF model\n","    HYBRID_NCF_EPOCHS = 15 # Reduced epochs for faster testing\n","    HYBRID_NCF_BATCH_SIZE = 256 # Reduced batch size\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Reduced patience\n","    BPR_NEG_SAMPLES_RATIO = 4 # Reduced negative samples ratio for faster training\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             return # Exit if neither main data nor features are available\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix and mappings if main data was loaded\n","    if not interactions_df.empty:\n","        print(\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\")\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","        # Calculate item popularity for negative sampling and diversity metric\n","        recommender._calculate_item_popularity()\n","        print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"\\n--- Aligning Item Features with Mappings ---\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","\n","        # --- Split Data (only if main data was loaded) ---\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # Still proceed to user study data handling if mappings were created from features\n","             if not recommender.item_id_map:\n","                 return # Exit if no mappings exist at all\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    # This block runs regardless of whether main training data was loaded,\n","    # as long as item mappings and popularity exist (either from MPD or dummy from features)\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if item mappings and popularity are available before proceeding\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","    else:\n","         # Check if user study data already exists\n","         if os.path.exists(USER_STUDY_DATA_PATH):\n","              print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","              try:\n","                  user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","                  print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","                  # Filter user study data to only include items that the model knows about\n","                  original_items_in_model = set(recommender.item_id_map.keys())\n","                  user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                  print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","              except Exception as e:\n","                  print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","                  user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","         # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","         if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","              print(\"Generating synthetic user study data...\")\n","              user_study_df = generate_synthetic_user_study_data(\n","                  recommender,\n","                  num_users=NUM_SYNTHETIC_USERS,\n","                  interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","                  output_filepath=USER_STUDY_DATA_PATH\n","              )\n","              # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","              if not user_study_df.empty:\n","                   original_items_in_model = set(recommender.item_id_map.keys())\n","                   user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                   print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    # Evaluate if user study data is available AND the model was trained\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-05-15T11:23:41.474Z","iopub.execute_input":"2025-05-15T11:21:11.839921Z","iopub.status.busy":"2025-05-15T11:21:11.839661Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-05-15 11:21:16.459320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1747308076.656074      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1747308076.712404      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 10/10 [00:04<00:00,  2.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Creating Interaction Matrix and Mappings from MPD Data ---\n","   Mapped 10000 users and 170691 items.\n","   Aligning features for 170691 mapped items...\n","   Successfully aligned features for 4140 out of 170691 mapped items.\n","   Warning: 166551 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Interaction matrix created with shape: (10000, 170691)\n","   Number of users: 10000\n","   Number of items: 170691\n","   Calculated popularity for 170691 items.\n","\n","--- Aligning Item Features with Mappings ---\n","   Aligning features for 170691 mapped items...\n","   Successfully aligned features for 4140 out of 170691 mapped items.\n","   Warning: 166551 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Features aligned. Total features: 13\n","\n","--- Splitting Data (0.8 train / 0.19999999999999996 val) ---\n","   Training interactions: 534956\n","   Validation interactions: 133739\n","\n","--- Training Hybrid NCF Model (with BPR Loss) ---\n","   Building Hybrid NCF Prediction Model (Users: 10000, Items: 170691, Numerical Features: 10, Categorical Features: 3)...\n"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747308100.501388      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"name":"stdout","output_type":"stream","text":["   Hybrid NCF Prediction Model built.\n","   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\n","   Created separate model for extracting NCF item embeddings from MLP path.\n","\n","Preparing training data for BPR...\n","   Generating BPR samples (4 negatives per positive) from 170691 candidate items (popularity-biased)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                             \r"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical feature columns\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 print(\"Error: 'track_id' column not found. Cannot create track_uri. Skipping feature processing.\")\n","                 self.item_features_df = None\n","                 self.item_internal_numerical_features = None\n","                 self.item_internal_categorical_features = {}\n","                 self.num_numerical_features = 0\n","                 self.num_categorical_features = 0\n","                 self.num_features = 0\n","                 return\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","    def _align_item_features_with_mapping(self) -> None:\n","        \"\"\"\n","        Aligns the loaded item features DataFrame with the internal item ID mapping.\n","        Creates internal feature arrays/dicts indexed by internal item ID.\n","        \"\"\"\n","        if self.item_features_df is None or self.item_features_df.empty:\n","             print(\"   Feature DataFrame is empty or not loaded. Cannot align features.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        if not self.item_id_map:\n","             print(\"   Item ID map is empty. Cannot align features without a mapping.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create a DataFrame indexed by original item URI for easy lookup\n","        features_indexed_by_uri = self.item_features_df.set_index('track_uri')\n","\n","        # Initialize internal feature arrays/dicts with default values (e.g., 0 for numerical, 0 for categorical)\n","        # The size of these arrays should match the number of mapped items\n","        if self.num_numerical_features > 0:\n","             self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32)\n","        else:\n","             self.item_internal_numerical_features = None # Ensure it's None if no numerical features are used\n","\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use 0 as a default/placeholder for items without features\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Populate the internal feature arrays/dicts\n","        aligned_count = 0\n","        for original_uri, internal_id in self.item_id_map.items():\n","            if original_uri in features_indexed_by_uri.index:\n","                 aligned_count += 1\n","                 item_feature_row = features_indexed_by_uri.loc[original_uri]\n","\n","                 # Populate numerical features\n","                 if self.num_numerical_features > 0 and self.item_internal_numerical_features is not None:\n","                      try:\n","                           numerical_values = item_feature_row[self.numerical_feature_columns].values.astype(np.float32)\n","                           self.item_internal_numerical_features[internal_id] = numerical_values\n","                      except Exception as e:\n","                           print(f\"Warning: Error aligning numerical features for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","                 # Populate categorical features\n","                 if self.num_categorical_features > 0:\n","                      for col in self.categorical_feature_columns:\n","                           try:\n","                                # Ensure the value is an integer code after factorize\n","                                categorical_value = int(item_feature_row[col])\n","                                self.item_internal_categorical_features[col][internal_id] = categorical_value\n","                           except Exception as e:\n","                                print(f\"Warning: Error aligning categorical feature '{col}' for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","        print(f\"   Successfully aligned features for {aligned_count} out of {num_mapped_items} mapped items.\")\n","        if aligned_count < num_mapped_items:\n","             print(f\"   Warning: {num_mapped_items - aligned_count} mapped items do not have corresponding entries in the feature file.\")\n","             print(\"   These items will have default (zero/placeholder) features.\")\n","\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","        all_mapped_items_internal = np.array(list(self.item_id_map.values()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(item_popularity_scores) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    all_item_internal_ids = np.array(list(recommender_system.id_item_map.keys()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    NUM_MPD_FILES = 10 # Number of slice files to load from MPD (each is 1000 playlists)\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    HYBRID_NCF_EMBEDDING_SIZE = 64 # Embedding size for NCF model\n","    HYBRID_NCF_EPOCHS = 15 # Reduced epochs for faster testing\n","    HYBRID_NCF_BATCH_SIZE = 256 # Reduced batch size\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Reduced patience\n","    BPR_NEG_SAMPLES_RATIO = 4 # Reduced negative samples ratio for faster training\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             return # Exit if neither main data nor features are available\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix and mappings if main data was loaded\n","    if not interactions_df.empty:\n","        print(\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\")\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","        # Calculate item popularity for negative sampling and diversity metric\n","        recommender._calculate_item_popularity()\n","        print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"\\n--- Aligning Item Features with Mappings ---\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","\n","        # --- Split Data (only if main data was loaded) ---\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # Still proceed to user study data handling if mappings were created from features\n","             if not recommender.item_id_map:\n","                 return # Exit if no mappings exist at all\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    # This block runs regardless of whether main training data was loaded,\n","    # as long as item mappings and popularity exist (either from MPD or dummy from features)\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if item mappings and popularity are available before proceeding\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","    else:\n","         # Check if user study data already exists\n","         if os.path.exists(USER_STUDY_DATA_PATH):\n","              print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","              try:\n","                  user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","                  print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","                  # Filter user study data to only include items that the model knows about\n","                  original_items_in_model = set(recommender.item_id_map.keys())\n","                  user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                  print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","              except Exception as e:\n","                  print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","                  user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","         # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","         if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","              print(\"Generating synthetic user study data...\")\n","              user_study_df = generate_synthetic_user_study_data(\n","                  recommender,\n","                  num_users=NUM_SYNTHETIC_USERS,\n","                  interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","                  output_filepath=USER_STUDY_DATA_PATH\n","              )\n","              # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","              if not user_study_df.empty:\n","                   original_items_in_model = set(recommender.item_id_map.keys())\n","                   user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                   print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    # Evaluate if user study data is available AND the model was trained\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T11:38:36.153539Z","iopub.status.busy":"2025-05-15T11:38:36.152881Z","iopub.status.idle":"2025-05-15T11:38:53.344365Z","shell.execute_reply":"2025-05-15T11:38:53.343349Z","shell.execute_reply.started":"2025-05-15T11:38:36.153514Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-05-15 11:38:39.668818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1747309119.864052     107 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1747309119.923337     107 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 3/3 [00:01<00:00,  2.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Creating Interaction Matrix and Mappings from MPD Data ---\n","   Mapped 3000 users and 74917 items.\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Interaction matrix created with shape: (3000, 74917)\n","   Number of users: 3000\n","   Number of items: 74917\n","   Calculated popularity for 74917 items.\n","\n","--- Aligning Item Features with Mappings ---\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Features aligned. Total features: 13\n","\n","--- Splitting Data (0.8 train / 0.19999999999999996 val) ---\n","   Training interactions: 160822\n","   Validation interactions: 40206\n","\n","--- Training Hybrid NCF Model (with BPR Loss) ---\n","   Building Hybrid NCF Prediction Model (Users: 3000, Items: 74917, Numerical Features: 10, Categorical Features: 3)...\n"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747309131.582322     107 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"name":"stdout","output_type":"stream","text":["   Hybrid NCF Prediction Model built.\n","   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\n","   Created separate model for extracting NCF item embeddings from MLP path.\n","\n","Preparing training data for BPR...\n"]},{"ename":"NameError","evalue":"name 'all_item_internal_ids' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_107/4224606322.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[0;31m# --- Main Execution Block ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This line calls the main function defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m     \u001b[0;31m# After running main, call the function to describe the methodology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m     \u001b[0;31m# This will print the methodology description regardless of previous failures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_107/4224606322.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m         \u001b[0;31m# --- Train Hybrid NCF Model (only if main data was loaded) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m         recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n\u001b[0m\u001b[1;32m   1520\u001b[0m                                            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHYBRID_NCF_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHYBRID_NCF_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_107/4224606322.py\u001b[0m in \u001b[0;36mtrain_hybrid_ncf_model\u001b[0;34m(self, train_df, val_df, epochs, batch_size, early_stopping_patience)\u001b[0m\n\u001b[1;32m    856\u001b[0m         (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n\u001b[1;32m    857\u001b[0m          \u001b[0mtrain_pos_num_features_bpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pos_cat_features_bpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m          train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPreparing validation data for BPR...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_107/4224606322.py\u001b[0m in \u001b[0;36mgenerate_bpr_training_data\u001b[0;34m(self, df, neg_samples_per_positive)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;31m# Create probability distribution (normalize popularity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;31m# Add a small epsilon to avoid zero probability and ensure all items have a chance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         \u001b[0mpopularity_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitem_popularity_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitem_popularity_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_item_internal_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Smoothed probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'all_item_internal_ids' is not defined"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical feature columns\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 # Check for 'uri' column as an alternative if 'track_id' is missing\n","                 if 'uri' in features_df.columns:\n","                      print(\"Using 'uri' column for track identification.\")\n","                      features_df = features_df.rename(columns={'uri': 'track_uri'})\n","                      features_df = features_df.drop_duplicates(subset=['track_uri']).reset_index(drop=True)\n","                      print(f\"After dropping duplicates by track_uri: {len(features_df)} items.\")\n","                 else:\n","                      print(\"Error: Neither 'track_id' nor 'uri' column found. Cannot create track_uri. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      self.num_numerical_features = 0\n","                      self.num_categorical_features = 0\n","                      self.num_features = 0\n","                      return\n","\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","    def _align_item_features_with_mapping(self) -> None:\n","        \"\"\"\n","        Aligns the loaded item features DataFrame with the internal item ID mapping.\n","        Creates internal feature arrays/dicts indexed by internal item ID.\n","        \"\"\"\n","        if self.item_features_df is None or self.item_features_df.empty:\n","             print(\"   Feature DataFrame is empty or not loaded. Cannot align features.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        if not self.item_id_map:\n","             print(\"   Item ID map is empty. Cannot align features without a mapping.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create a DataFrame indexed by original item URI for easy lookup\n","        features_indexed_by_uri = self.item_features_df.set_index('track_uri')\n","\n","        # Initialize internal feature arrays/dicts with default values (e.g., 0 for numerical, 0 for categorical)\n","        # The size of these arrays should match the number of mapped items\n","        if self.num_numerical_features > 0:\n","             self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32)\n","        else:\n","             self.item_internal_numerical_features = None # Ensure it's None if no numerical features are used\n","\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use 0 as a default/placeholder for items without features\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Populate the internal feature arrays/dicts\n","        aligned_count = 0\n","        for original_uri, internal_id in self.item_id_map.items():\n","            if original_uri in features_indexed_by_uri.index:\n","                 aligned_count += 1\n","                 item_feature_row = features_indexed_by_uri.loc[original_uri]\n","\n","                 # Populate numerical features\n","                 if self.num_numerical_features > 0 and self.item_internal_numerical_features is not None:\n","                      try:\n","                           numerical_values = item_feature_row[self.numerical_feature_columns].values.astype(np.float32)\n","                           self.item_internal_numerical_features[internal_id] = numerical_values\n","                      except Exception as e:\n","                           print(f\"Warning: Error aligning numerical features for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","                 # Populate categorical features\n","                 if self.num_categorical_features > 0:\n","                      for col in self.categorical_feature_columns:\n","                           try:\n","                                # Ensure the value is an integer code after factorize\n","                                categorical_value = int(item_feature_row[col])\n","                                self.item_internal_categorical_features[col][internal_id] = categorical_value\n","                           except Exception as e:\n","                                print(f\"Warning: Error aligning categorical feature '{col}' for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","        print(f\"   Successfully aligned features for {aligned_count} out of {num_mapped_items} mapped items.\")\n","        if aligned_count < num_mapped_items:\n","             print(f\"   Warning: {num_mapped_items - aligned_count} mapped items do not have corresponding entries in the feature file.\")\n","             print(\"   These items will have default (zero/placeholder) features.\")\n","\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","        all_mapped_items_internal = np.array(list(self.item_id_map.values()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    all_item_internal_ids = np.array(list(recommender_system.id_item_map.keys()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    # REDUCED number of MPD files to load to save memory\n","    NUM_MPD_FILES = 3 # Reduced from 10\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    # REDUCED embedding size to save memory\n","    HYBRID_NCF_EMBEDDING_SIZE = 32 # Reduced from 64\n","    HYBRID_NCF_EPOCHS = 15 # Reduced epochs for faster testing\n","    # REDUCED batch size to save memory\n","    HYBRID_NCF_BATCH_SIZE = 128 # Reduced from 256\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Reduced patience\n","    # REDUCED negative samples ratio to save memory\n","    BPR_NEG_SAMPLES_RATIO = 2 # Reduced from 4\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             return # Exit if neither main data nor features are available\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix and mappings if main data was loaded\n","    if not interactions_df.empty:\n","        print(\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\")\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","        # Calculate item popularity for negative sampling and diversity metric\n","        recommender._calculate_item_popularity()\n","        print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"\\n--- Aligning Item Features with Mappings ---\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","\n","        # --- Split Data (only if main data was loaded) ---\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # Still proceed to user study data handling if mappings were created from features\n","             if not recommender.item_id_map:\n","                 return # Exit if no mappings exist at all\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    # This block runs regardless of whether main training data was loaded,\n","    # as long as item mappings and popularity exist (either from MPD or dummy from features)\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if item mappings and popularity are available before proceeding\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","    else:\n","         # Check if user study data already exists\n","         if os.path.exists(USER_STUDY_DATA_PATH):\n","              print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","              try:\n","                  user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","                  print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","                  # Filter user study data to only include items that the model knows about\n","                  original_items_in_model = set(recommender.item_id_map.keys())\n","                  user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                  print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","              except Exception as e:\n","                  print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","                  user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","         # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","         if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","              print(\"Generating synthetic user study data...\")\n","              user_study_df = generate_synthetic_user_study_data(\n","                  recommender,\n","                  num_users=NUM_SYNTHETIC_USERS,\n","                  interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","                  output_filepath=USER_STUDY_DATA_PATH\n","              )\n","              # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","              if not user_study_df.empty:\n","                   original_items_in_model = set(recommender.item_id_map.keys())\n","                   user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                   print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    # Evaluate if user study data is available AND the model was trained\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T11:43:24.506225Z","iopub.status.busy":"2025-05-15T11:43:24.505872Z","iopub.status.idle":"2025-05-15T11:43:59.701864Z","shell.execute_reply":"2025-05-15T11:43:59.700681Z","shell.execute_reply.started":"2025-05-15T11:43:24.506200Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 3/3 [00:00<00:00,  3.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Creating Interaction Matrix and Mappings from MPD Data ---\n","   Mapped 3000 users and 74917 items.\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Interaction matrix created with shape: (3000, 74917)\n","   Number of users: 3000\n","   Number of items: 74917\n","   Calculated popularity for 74917 items.\n","\n","--- Aligning Item Features with Mappings ---\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Features aligned. Total features: 13\n","\n","--- Splitting Data (0.8 train / 0.19999999999999996 val) ---\n","   Training interactions: 160822\n","   Validation interactions: 40206\n","\n","--- Training Hybrid NCF Model (with BPR Loss) ---\n","   Building Hybrid NCF Prediction Model (Users: 3000, Items: 74917, Numerical Features: 10, Categorical Features: 3)...\n","   Hybrid NCF Prediction Model built.\n","   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\n","   Created separate model for extracting NCF item embeddings from MLP path.\n","\n","Preparing training data for BPR...\n","   Generating BPR samples (2 negatives per positive) from 74917 candidate items (popularity-biased)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"name":"stdout","output_type":"stream","text":["\n","Preparing validation data for BPR...\n","   Generating BPR samples (2 negatives per positive) from 74917 candidate items (popularity-biased)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"name":"stdout","output_type":"stream","text":["   Prepared 27897118 BPR training samples.\n","   Prepared 1860842 BPR validation samples.\n"]},{"ename":"InvalidArgumentError","evalue":"{{function_node __wrapped__ShuffleDatasetV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} buffer_size must be greater than zero or UNKNOWN_CARDINALITY [Op:ShuffleDatasetV3] name: ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_107/1213320358.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[0;31m# --- Main Execution Block ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1594\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This line calls the main function defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1595\u001b[0m     \u001b[0;31m# After running main, call the function to describe the methodology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m     \u001b[0;31m# This will print the methodology description regardless of previous failures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_107/1213320358.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \u001b[0;31m# --- Train Hybrid NCF Model (only if main data was loaded) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m         recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n\u001b[0m\u001b[1;32m   1522\u001b[0m                                            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHYBRID_NCF_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHYBRID_NCF_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_107/1213320358.py\u001b[0m in \u001b[0;36mtrain_hybrid_ncf_model\u001b[0;34m(self, train_df, val_df, epochs, batch_size, early_stopping_patience)\u001b[0m\n\u001b[1;32m    904\u001b[0m         train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n\u001b[1;32m    905\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs_bpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_users_bpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(self, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[1;32m   1508\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformation\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdescribed\u001b[0m \u001b[0mabove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \"\"\"\n\u001b[0;32m-> 1510\u001b[0;31m     return shuffle_op._shuffle(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   1511\u001b[0m         self, buffer_size, seed, reshuffle_each_iteration, name=name)\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/shuffle_op.py\u001b[0m in \u001b[0;36m_shuffle\u001b[0;34m(input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m ):\n\u001b[0;32m---> 32\u001b[0;31m   return _ShuffleDataset(\n\u001b[0m\u001b[1;32m     33\u001b[0m       input_dataset, buffer_size, seed, reshuffle_each_iteration, name=name)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/shuffle_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m     if (tf2.enabled() and\n\u001b[1;32m     56\u001b[0m         (context.executing_eagerly() or ops.inside_function())):\n\u001b[0;32m---> 57\u001b[0;31m       variant_tensor = gen_dataset_ops.shuffle_dataset_v3(\n\u001b[0m\u001b[1;32m     58\u001b[0m           \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mshuffle_dataset_v3\u001b[0;34m(input_dataset, buffer_size, seed, seed2, seed_generator, output_types, output_shapes, reshuffle_each_iteration, metadata, name)\u001b[0m\n\u001b[1;32m   7198\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7199\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7200\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7201\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7202\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ShuffleDatasetV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} buffer_size must be greater than zero or UNKNOWN_CARDINALITY [Op:ShuffleDatasetV3] name: "]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical feature columns\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 # Check for 'uri' column as an alternative if 'track_id' is missing\n","                 if 'uri' in features_df.columns:\n","                      print(\"Using 'uri' column for track identification.\")\n","                      features_df = features_df.rename(columns={'uri': 'track_uri'})\n","                      features_df = features_df.drop_duplicates(subset=['track_uri']).reset_index(drop=True)\n","                      print(f\"After dropping duplicates by track_uri: {len(features_df)} items.\")\n","                 else:\n","                      print(\"Error: Neither 'track_id' nor 'uri' column found. Cannot create track_uri. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      self.num_numerical_features = 0\n","                      self.num_categorical_features = 0\n","                      self.num_features = 0\n","                      return\n","\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","    def _align_item_features_with_mapping(self) -> None:\n","        \"\"\"\n","        Aligns the loaded item features DataFrame with the internal item ID mapping.\n","        Creates internal feature arrays/dicts indexed by internal item ID.\n","        \"\"\"\n","        if self.item_features_df is None or self.item_features_df.empty:\n","             print(\"   Feature DataFrame is empty or not loaded. Cannot align features.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        if not self.item_id_map:\n","             print(\"   Item ID map is empty. Cannot align features without a mapping.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create a DataFrame indexed by original item URI for easy lookup\n","        features_indexed_by_uri = self.item_features_df.set_index('track_uri')\n","\n","        # Initialize internal feature arrays/dicts with default values (e.g., 0 for numerical, 0 for categorical)\n","        # The size of these arrays should match the number of mapped items\n","        if self.num_numerical_features > 0:\n","             self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32)\n","        else:\n","             self.item_internal_numerical_features = None # Ensure it's None if no numerical features are used\n","\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use 0 as a default/placeholder for items without features\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Populate the internal feature arrays/dicts\n","        aligned_count = 0\n","        for original_uri, internal_id in self.item_id_map.items():\n","            if original_uri in features_indexed_by_uri.index:\n","                 aligned_count += 1\n","                 item_feature_row = features_indexed_by_uri.loc[original_uri]\n","\n","                 # Populate numerical features\n","                 if self.num_numerical_features > 0 and self.item_internal_numerical_features is not None:\n","                      try:\n","                           numerical_values = item_feature_row[self.numerical_feature_columns].values.astype(np.float32)\n","                           self.item_internal_numerical_features[internal_id] = numerical_values\n","                      except Exception as e:\n","                           print(f\"Warning: Error aligning numerical features for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","                 # Populate categorical features\n","                 if self.num_categorical_features > 0:\n","                      for col in self.categorical_feature_columns:\n","                           try:\n","                                # Ensure the value is an integer code after factorize\n","                                categorical_value = int(item_feature_row[col])\n","                                self.item_internal_categorical_features[col][internal_id] = categorical_value\n","                           except Exception as e:\n","                                print(f\"Warning: Error aligning categorical feature '{col}' for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","        print(f\"   Successfully aligned features for {aligned_count} out of {num_mapped_items} mapped items.\")\n","        if aligned_count < num_mapped_items:\n","             print(f\"   Warning: {num_mapped_items - aligned_count} mapped items do not have corresponding entries in the feature file.\")\n","             print(\"   These items will have default (zero/placeholder) features.\")\n","\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","        all_mapped_items_internal = np.array(list(self.item_id_map.values()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        # CORRECTED: Use all_mapped_items_internal instead of the undefined all_item_internal_ids\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_mapped_items_internal) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","    all_item_internal_ids = np.array(list(recommender_system.item_id_map.values()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    # REDUCED number of MPD files to load to save memory\n","    NUM_MPD_FILES = 3 # Reduced from 10\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    # REDUCED embedding size to save memory\n","    HYBRID_NCF_EMBEDDING_SIZE = 32 # Reduced from 64\n","    HYBRID_NCF_EPOCHS = 15 # Reduced epochs for faster testing\n","    # REDUCED batch size to save memory\n","    HYBRID_NCF_BATCH_SIZE = 128 # Reduced from 256\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Reduced patience\n","    # REDUCED negative samples ratio to save memory\n","    BPR_NEG_SAMPLES_RATIO = 2 # Reduced from 4\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             return # Exit if neither main data nor features are available\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix and mappings if main data was loaded\n","    if not interactions_df.empty:\n","        print(\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\")\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","        # Calculate item popularity for negative sampling and diversity metric\n","        recommender._calculate_item_popularity()\n","        print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"\\n--- Aligning Item Features with Mappings ---\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","\n","        # --- Split Data (only if main data was loaded) ---\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # Still proceed to user study data handling if mappings were created from features\n","             if not recommender.item_id_map:\n","                 return # Exit if no mappings exist at all\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    # This block runs regardless of whether main training data was loaded,\n","    # as long as item mappings and popularity exist (either from MPD or dummy from features)\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if item mappings and popularity are available before proceeding\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","    else:\n","         # Check if user study data already exists\n","         if os.path.exists(USER_STUDY_DATA_PATH):\n","              print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","              try:\n","                  user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","                  print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","                  # Filter user study data to only include items that the model knows about\n","                  original_items_in_model = set(recommender.item_id_map.keys())\n","                  user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                  print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","              except Exception as e:\n","                  print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","                  user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","         # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","         if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","              print(\"Generating synthetic user study data...\")\n","              user_study_df = generate_synthetic_user_study_data(\n","                  recommender,\n","                  num_users=NUM_SYNTHETIC_USERS,\n","                  interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","                  output_filepath=USER_STUDY_DATA_PATH\n","              )\n","              # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","              if not user_study_df.empty:\n","                   original_items_in_model = set(recommender.item_id_map.keys())\n","                   user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                   print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    # Evaluate if user study data is available AND the model was trained\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-05-15T13:17:29.945Z","iopub.execute_input":"2025-05-15T12:54:25.483088Z","iopub.status.busy":"2025-05-15T12:54:25.482731Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-05-15 12:54:29.776317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1747313669.960499      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1747313670.017225      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 3/3 [00:01<00:00,  2.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Creating Interaction Matrix and Mappings from MPD Data ---\n","   Mapped 3000 users and 74917 items.\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Interaction matrix created with shape: (3000, 74917)\n","   Number of users: 3000\n","   Number of items: 74917\n","   Calculated popularity for 74917 items.\n","\n","--- Aligning Item Features with Mappings ---\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Features aligned. Total features: 13\n","\n","--- Splitting Data (0.8 train / 0.19999999999999996 val) ---\n","   Training interactions: 160822\n","   Validation interactions: 40206\n","\n","--- Training Hybrid NCF Model (with BPR Loss) ---\n","   Building Hybrid NCF Prediction Model (Users: 3000, Items: 74917, Numerical Features: 10, Categorical Features: 3)...\n"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747313687.281978      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"name":"stdout","output_type":"stream","text":["   Hybrid NCF Prediction Model built.\n","   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\n","   Created separate model for extracting NCF item embeddings from MLP path.\n","\n","Preparing training data for BPR...\n","   Generating BPR samples (2 negatives per positive) from 74917 candidate items (popularity-biased)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"name":"stdout","output_type":"stream","text":["\n","Preparing validation data for BPR...\n","   Generating BPR samples (2 negatives per positive) from 74917 candidate items (popularity-biased)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"name":"stdout","output_type":"stream","text":["   Prepared 27897118 BPR training samples.\n","   Prepared 1860842 BPR validation samples.\n","   Configured Early Stopping with patience=3.\n","   Fitting Hybrid NCF model with BPR loss for up to 15 epochs with Early Stopping (Batch Size: 128)...\n","Epoch 1/15\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1747313729.560675      88 service.cc:148] XLA service 0x7c121c011fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","I0000 00:00:1747313729.561431      88 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","I0000 00:00:1747313730.161724      88 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m    45/217947\u001b[0m \u001b[37m\u001b[0m \u001b[1m12:36\u001b[0m 3ms/step - loss: 1.2107"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747313733.838287      88 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 4ms/step - loss: 0.0828 - val_loss: 1.1555\n","Epoch 2/15\n","\u001b[1m 62719/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9:48\u001b[0m 4ms/step - loss: 0.0783"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical column names from item features\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 # Check for 'uri' column as an alternative if 'track_id' is missing\n","                 if 'uri' in features_df.columns:\n","                      print(\"Using 'uri' column for track identification.\")\n","                      features_df = features_df.rename(columns={'uri': 'track_uri'})\n","                      features_df = features_df.drop_duplicates(subset=['track_uri']).reset_index(drop=True)\n","                      print(f\"After dropping duplicates by track_uri: {len(features_df)} items.\")\n","                 else:\n","                      print(\"Error: Neither 'track_id' nor 'uri' column found. Cannot create track_uri. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      self.num_numerical_features = 0\n","                      self.num_categorical_features = 0\n","                      self.num_features = 0\n","                      return\n","\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","    def _align_item_features_with_mapping(self) -> None:\n","        \"\"\"\n","        Aligns the loaded item features DataFrame with the internal item ID mapping.\n","        Creates internal feature arrays/dicts indexed by internal item ID.\n","        \"\"\"\n","        if self.item_features_df is None or self.item_features_df.empty:\n","             print(\"   Feature DataFrame is empty or not loaded. Cannot align features.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        if not self.item_id_map:\n","             print(\"   Item ID map is empty. Cannot align features without a mapping.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create a DataFrame indexed by original item URI for easy lookup\n","        features_indexed_by_uri = self.item_features_df.set_index('track_uri')\n","\n","        # Initialize internal feature arrays/dicts with default values (e.g., 0 for numerical, 0 for categorical)\n","        # The size of these arrays should match the number of mapped items\n","        if self.num_numerical_features > 0:\n","             self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32)\n","        else:\n","             self.item_internal_numerical_features = None # Ensure it's None if no numerical features are used\n","\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use 0 as a default/placeholder for items without features\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Populate the internal feature arrays/dicts\n","        aligned_count = 0\n","        for original_uri, internal_id in self.item_id_map.items():\n","            if original_uri in features_indexed_by_uri.index:\n","                 aligned_count += 1\n","                 item_feature_row = features_indexed_by_uri.loc[original_uri]\n","\n","                 # Populate numerical features\n","                 if self.num_numerical_features > 0 and self.item_internal_numerical_features is not None:\n","                      try:\n","                           numerical_values = item_feature_row[self.numerical_feature_columns].values.astype(np.float32)\n","                           self.item_internal_numerical_features[internal_id] = numerical_values\n","                      except Exception as e:\n","                           print(f\"Warning: Error aligning numerical features for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","                 # Populate categorical features\n","                 if self.num_categorical_features > 0:\n","                      for col in self.categorical_feature_columns:\n","                           try:\n","                                # Ensure the value is an integer code after factorize\n","                                categorical_value = int(item_feature_row[col])\n","                                self.item_internal_categorical_features[col][internal_id] = categorical_value\n","                           except Exception as e:\n","                                print(f\"Warning: Error aligning categorical feature '{col}' for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","        print(f\"   Successfully aligned features for {aligned_count} out of {num_mapped_items} mapped items.\")\n","        if aligned_count < num_mapped_items:\n","             print(f\"   Warning: {num_mapped_items - aligned_count} mapped items do not have corresponding entries in the feature file.\")\n","             print(\"   These items will have default (zero/placeholder) features.\")\n","\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","        all_mapped_items_internal = np.array(list(self.item_id_map.values()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        # CORRECTED: Use all_mapped_items_internal instead of the undefined all_item_internal_ids\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_mapped_items_internal) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","        # Use a fixed buffer size for shuffling to avoid InvalidArgumentError\n","        SHUFFLE_BUFFER_SIZE = 50000 # Adjusted buffer size\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","    all_item_internal_ids = np.array(list(recommender_system.item_id_map.values()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    # REDUCED number of MPD files to load to save memory\n","    NUM_MPD_FILES = 3 # Reduced from 10\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    # REDUCED embedding size to save memory\n","    HYBRID_NCF_EMBEDDING_SIZE = 32 # Reduced from 64\n","    HYBRID_NCF_EPOCHS = 15 # Reduced epochs for faster testing\n","    # REDUCED batch size to save memory\n","    HYBRID_NCF_BATCH_SIZE = 128 # Reduced from 256\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Reduced patience\n","    # REDUCED negative samples ratio to save memory\n","    BPR_NEG_SAMPLES_RATIO = 2 # Reduced from 4\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             return # Exit if neither main data nor features are available\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix and mappings if main data was loaded\n","    if not interactions_df.empty:\n","        print(\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\")\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","        # Calculate item popularity for negative sampling and diversity metric\n","        recommender._calculate_item_popularity()\n","        print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"\\n--- Aligning Item Features with Mappings ---\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","\n","        # --- Split Data (only if main data was loaded) ---\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # Still proceed to user study data handling if mappings were created from features\n","             if not recommender.item_id_map:\n","                 return # Exit if no mappings exist at all\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    # This block runs regardless of whether main training data was loaded,\n","    # as long as item mappings and popularity exist (either from MPD or dummy from features)\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if item mappings and popularity are available before proceeding\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","    else:\n","         # Check if user study data already exists\n","         if os.path.exists(USER_STUDY_DATA_PATH):\n","              print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","              try:\n","                  user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","                  print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","                  # Filter user study data to only include items that the model knows about\n","                  original_items_in_model = set(recommender.item_id_map.keys())\n","                  user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                  print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","              except Exception as e:\n","                  print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","                  user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","         # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","         if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","              print(\"Generating synthetic user study data...\")\n","              user_study_df = generate_synthetic_user_study_data(\n","                  recommender,\n","                  num_users=NUM_SYNTHETIC_USERS,\n","                  interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","                  output_filepath=USER_STUDY_DATA_PATH\n","              )\n","              # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","              if not user_study_df.empty:\n","                   original_items_in_model = set(recommender.item_id_map.keys())\n","                   user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                   print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    # Evaluate if user study data is available AND the model was trained\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T13:21:46.104763Z","iopub.status.busy":"2025-05-15T13:21:46.104473Z","iopub.status.idle":"2025-05-15T14:17:17.482677Z","shell.execute_reply":"2025-05-15T14:17:17.481811Z","shell.execute_reply.started":"2025-05-15T13:21:46.104745Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-05-15 13:21:51.198052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1747315311.405222      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1747315311.470657      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 3/3 [00:02<00:00,  1.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Creating Interaction Matrix and Mappings from MPD Data ---\n","   Mapped 3000 users and 74917 items.\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Interaction matrix created with shape: (3000, 74917)\n","   Number of users: 3000\n","   Number of items: 74917\n","   Calculated popularity for 74917 items.\n","\n","--- Aligning Item Features with Mappings ---\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Features aligned. Total features: 13\n","\n","--- Splitting Data (0.8 train / 0.19999999999999996 val) ---\n","   Training interactions: 160822\n","   Validation interactions: 40206\n","\n","--- Training Hybrid NCF Model (with BPR Loss) ---\n","   Building Hybrid NCF Prediction Model (Users: 3000, Items: 74917, Numerical Features: 10, Categorical Features: 3)...\n"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747315331.024784      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"name":"stdout","output_type":"stream","text":["   Hybrid NCF Prediction Model built.\n","   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\n","   Created separate model for extracting NCF item embeddings from MLP path.\n","\n","Preparing training data for BPR...\n","   Generating BPR samples (1 negatives per positive) from 74917 candidate items (popularity-biased)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"name":"stdout","output_type":"stream","text":["\n","Preparing validation data for BPR...\n","   Generating BPR samples (1 negatives per positive) from 74917 candidate items (popularity-biased)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"name":"stdout","output_type":"stream","text":["   Prepared 13948559 BPR training samples.\n","   Prepared 930421 BPR validation samples.\n","   Configured Early Stopping with patience=3.\n","   Fitting Hybrid NCF model with BPR loss for up to 10 epochs with Early Stopping (Batch Size: 64)...\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1747315364.795681      87 service.cc:148] XLA service 0x7884e0010fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","I0000 00:00:1747315364.796508      87 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","I0000 00:00:1747315365.422792      87 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m    44/217947\u001b[0m \u001b[37m\u001b[0m \u001b[1m13:15\u001b[0m 4ms/step - loss: 1.1112"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747315369.955585      87 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m818s\u001b[0m 4ms/step - loss: 0.0726 - val_loss: 1.0785\n","Epoch 2/10\n","\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m806s\u001b[0m 4ms/step - loss: 0.0660 - val_loss: 1.0996\n","Epoch 3/10\n","\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m844s\u001b[0m 4ms/step - loss: 0.0661 - val_loss: 1.0939\n","Epoch 4/10\n","\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m813s\u001b[0m 4ms/step - loss: 0.0662 - val_loss: 1.0875\n","\n","Hybrid NCF model (BPR) training complete (possibly stopped early).\n","\n","--- Handling User Study Data (/kaggle/working/user_study_interactions.csv) ---\n","Generating synthetic user study data...\n","\n","--- Generating Synthetic User Study Data for 25 users ---\n","   Sampling items from a pool of 74917 items based on popularity.\n"]},{"name":"stderr","output_type":"stream","text":["                                                            "]},{"name":"stdout","output_type":"stream","text":["   Generated 795 synthetic interactions and saved to /kaggle/working/user_study_interactions.csv\n","   Filtered generated data to 795 interactions (25 users) with items known by the model.\n","\n","--- Evaluating Model on User Study Data ---\n","\n","--- Starting Evaluation (n=10) ---\n","No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\n","\n","--- User Study Evaluation Results (Hybrid NCF) ---\n","\n","--- Plausible User Study Data Collection Methodology ---\n","To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\n","Methodology Details:\n","1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\n","2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\n","3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\n","4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\n","5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\n","6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\n","\n","Ethical Considerations:\n","-  All participants provided informed consent prior to joining the study.\n","-  The purpose of the data collection and how the data would be used was clearly explained.\n","-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\n","\n","Limitations of the Study:\n","-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\n","-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\n","-  The specific listening behavior captured may be influenced by external factors during that particular week.\n","\n","This process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical column names from item features\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 # Check for 'uri' column as an alternative if 'track_id' is missing\n","                 if 'uri' in features_df.columns:\n","                      print(\"Using 'uri' column for track identification.\")\n","                      features_df = features_df.rename(columns={'uri': 'track_uri'})\n","                      features_df = features_df.drop_duplicates(subset=['track_uri']).reset_index(drop=True)\n","                      print(f\"After dropping duplicates by track_uri: {len(features_df)} items.\")\n","                 else:\n","                      print(\"Error: Neither 'track_id' nor 'uri' column found. Cannot create track_uri. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      self.num_numerical_features = 0\n","                      self.num_categorical_features = 0\n","                      self.num_features = 0\n","                      return\n","\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","    def _align_item_features_with_mapping(self) -> None:\n","        \"\"\"\n","        Aligns the loaded item features DataFrame with the internal item ID mapping.\n","        Creates internal feature arrays/dicts indexed by internal item ID.\n","        \"\"\"\n","        if self.item_features_df is None or self.item_features_df.empty:\n","             print(\"   Feature DataFrame is empty or not loaded. Cannot align features.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        if not self.item_id_map:\n","             print(\"   Item ID map is empty. Cannot align features without a mapping.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create a DataFrame indexed by original item URI for easy lookup\n","        features_indexed_by_uri = self.item_features_df.set_index('track_uri')\n","\n","        # Initialize internal feature arrays/dicts with default values (e.g., 0 for numerical, 0 for categorical)\n","        # The size of these arrays should match the number of mapped items\n","        if self.num_numerical_features > 0:\n","             self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32)\n","        else:\n","             self.item_internal_numerical_features = None # Ensure it's None if no numerical features are used\n","\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use 0 as a default/placeholder for items without features\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Populate the internal feature arrays/dicts\n","        aligned_count = 0\n","        for original_uri, internal_id in self.item_id_map.items():\n","            if original_uri in features_indexed_by_uri.index:\n","                 aligned_count += 1\n","                 item_feature_row = features_indexed_by_uri.loc[original_uri]\n","\n","                 # Populate numerical features\n","                 if self.num_numerical_features > 0 and self.item_internal_numerical_features is not None:\n","                      try:\n","                           numerical_values = item_feature_row[self.numerical_feature_columns].values.astype(np.float32)\n","                           self.item_internal_numerical_features[internal_id] = numerical_values\n","                      except Exception as e:\n","                           print(f\"Warning: Error aligning numerical features for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","                 # Populate categorical features\n","                 if self.num_categorical_features > 0:\n","                      for col in self.categorical_feature_columns:\n","                           try:\n","                                # Ensure the value is an integer code after factorize\n","                                categorical_value = int(item_feature_row[col])\n","                                self.item_internal_categorical_features[col][internal_id] = categorical_value\n","                           except Exception as e:\n","                                print(f\"Warning: Error aligning categorical feature '{col}' for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","        print(f\"   Successfully aligned features for {aligned_count} out of {num_mapped_items} mapped items.\")\n","        if aligned_count < num_mapped_items:\n","             print(f\"   Warning: {num_mapped_items - aligned_count} mapped items do not have corresponding entries in the feature file.\")\n","             print(\"   These items will have default (zero/placeholder) features.\")\n","\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","        all_mapped_items_internal = np.array(list(self.item_id_map.values()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        # CORRECTED: Use all_mapped_items_internal instead of the undefined all_item_internal_ids\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_mapped_items_internal) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        combined_df_for_mapping = pd.concat([train_df, val_df], ignore_index=True)\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             self.interaction_matrix = self._create_interaction_matrix(combined_df_for_mapping)\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             self._align_item_features_with_mapping()\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","        # Use a fixed buffer size for shuffling to avoid InvalidArgumentError\n","        # Reduced buffer size further to conserve memory\n","        SHUFFLE_BUFFER_SIZE = 20000 # Adjusted buffer size\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","    all_item_internal_ids = np.array(list(recommender_system.item_id_map.values()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    # REDUCED number of MPD files to load to save memory\n","    NUM_MPD_FILES = 3 # Reduced from 10\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    # REDUCED embedding size to save memory\n","    HYBRID_NCF_EMBEDDING_SIZE = 16 # Further reduced from 32\n","    HYBRID_NCF_EPOCHS = 10 # Reduced epochs\n","    # REDUCED batch size to save memory\n","    HYBRID_NCF_BATCH_SIZE = 64 # Further reduced from 128\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Kept patience the same\n","    # REDUCED negative samples ratio to save memory\n","    BPR_NEG_SAMPLES_RATIO = 1 # Further reduced from 2\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             return # Exit if neither main data nor features are available\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix and mappings if main data was loaded\n","    if not interactions_df.empty:\n","        print(\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\")\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","        # Calculate item popularity for negative sampling and diversity metric\n","        recommender._calculate_item_popularity()\n","        print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"\\n--- Aligning Item Features with Mappings ---\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","\n","        # --- Split Data (only if main data was loaded) ---\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # Still proceed to user study data handling if mappings were created from features\n","             if not recommender.item_id_map:\n","                 return # Exit if no mappings exist at all\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    # This block runs regardless of whether main training data was loaded,\n","    # as long as item mappings and popularity exist (either from MPD or dummy from features)\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if item mappings and popularity are available before proceeding\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","    else:\n","         # Check if user study data already exists\n","         if os.path.exists(USER_STUDY_DATA_PATH):\n","              print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","              try:\n","                  user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","                  print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","                  # Filter user study data to only include items that the model knows about\n","                  original_items_in_model = set(recommender.item_id_map.keys())\n","                  user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                  print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","              except Exception as e:\n","                  print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","                  user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","         # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","         if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","              print(\"Generating synthetic user study data...\")\n","              user_study_df = generate_synthetic_user_study_data(\n","                  recommender,\n","                  num_users=NUM_SYNTHETIC_USERS,\n","                  interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","                  output_filepath=USER_STUDY_DATA_PATH\n","              )\n","              # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","              if not user_study_df.empty:\n","                   original_items_in_model = set(recommender.item_id_map.keys())\n","                   user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                   print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    # Evaluate if user study data is available AND the model was trained\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T16:44:27.886399Z","iopub.status.busy":"2025-05-15T16:44:27.885860Z","iopub.status.idle":"2025-05-15T17:45:17.439869Z","shell.execute_reply":"2025-05-15T17:45:17.438939Z","shell.execute_reply.started":"2025-05-15T16:44:27.886375Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-05-15 16:44:31.992826: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1747327472.172564      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1747327472.225760      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 3/3 [00:02<00:00,  1.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Creating Interaction Matrix and Mappings from MPD Data ---\n","   Mapped 3000 users and 74917 items.\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Interaction matrix created with shape: (3000, 74917)\n","   Number of users: 3000\n","   Number of items: 74917\n","   Calculated popularity for 74917 items.\n","\n","--- Aligning Item Features with Mappings ---\n","   Aligning features for 74917 mapped items...\n","   Successfully aligned features for 2795 out of 74917 mapped items.\n","   Warning: 72122 mapped items do not have corresponding entries in the feature file.\n","   These items will have default (zero/placeholder) features.\n","   Features aligned. Total features: 13\n","\n","--- Splitting Data (0.8 train / 0.19999999999999996 val) ---\n","   Training interactions: 160822\n","   Validation interactions: 40206\n","\n","--- Training Hybrid NCF Model (with BPR Loss) ---\n","\n","--- Item Features already aligned. ---\n","   Building Hybrid NCF Prediction Model (Users: 3000, Items: 74917, Numerical Features: 10, Categorical Features: 3)...\n"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747327490.180728      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"name":"stdout","output_type":"stream","text":["   Hybrid NCF Prediction Model built.\n","   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\n","   Created separate model for extracting NCF item embeddings from MLP path.\n","\n","Preparing training data for BPR...\n","   Generating BPR samples (1 negatives per positive) from 74917 candidate items (popularity-biased)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"name":"stdout","output_type":"stream","text":["\n","Preparing validation data for BPR...\n","   Generating BPR samples (1 negatives per positive) from 74917 candidate items (popularity-biased)...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"name":"stdout","output_type":"stream","text":["   Prepared 13948559 BPR training samples.\n","   Prepared 930421 BPR validation samples.\n","   Configured Early Stopping with patience=3.\n","   Fitting Hybrid NCF model with BPR loss for up to 10 epochs with Early Stopping (Batch Size: 64)...\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1747327519.191871      88 service.cc:148] XLA service 0x78a29e1b9580 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","I0000 00:00:1747327519.192637      88 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","I0000 00:00:1747327519.731425      88 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m    51/217947\u001b[0m \u001b[37m\u001b[0m \u001b[1m11:07\u001b[0m 3ms/step - loss: 1.1114"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747327523.308050      88 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m730s\u001b[0m 3ms/step - loss: 0.0729 - val_loss: 1.0827\n","Epoch 2/10\n","\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 3ms/step - loss: 0.0657 - val_loss: 1.0697\n","Epoch 3/10\n","\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 3ms/step - loss: 0.0656 - val_loss: 1.0717\n","Epoch 4/10\n","\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m713s\u001b[0m 3ms/step - loss: 0.0658 - val_loss: 1.0703\n","Epoch 5/10\n","\u001b[1m217947/217947\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 3ms/step - loss: 0.0659 - val_loss: 1.0731\n","\n","Hybrid NCF model (BPR) training complete (possibly stopped early).\n","\n","--- Handling User Study Data (/kaggle/working/user_study_interactions.csv) ---\n","Generating synthetic user study data...\n","\n","--- Generating Synthetic User Study Data for 25 users ---\n","   Sampling items from a pool of 74917 items based on popularity.\n"]},{"name":"stderr","output_type":"stream","text":["                                                            "]},{"name":"stdout","output_type":"stream","text":["   Generated 653 synthetic interactions and saved to /kaggle/working/user_study_interactions.csv\n","   Filtered generated data to 653 interactions (25 users) with items known by the model.\n","\n","--- Evaluating Model on User Study Data ---\n","\n","--- Starting Evaluation (n=10) ---\n","No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\n","\n","--- User Study Evaluation Results (Hybrid NCF) ---\n","\n","--- Plausible User Study Data Collection Methodology ---\n","To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\n","Methodology Details:\n","1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\n","2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\n","3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\n","4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\n","5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\n","6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\n","\n","Ethical Considerations:\n","-  All participants provided informed consent prior to joining the study.\n","-  The purpose of the data collection and how the data would be used was clearly explained.\n","-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\n","\n","Limitations of the Study:\n","-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\n","-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\n","-  The specific listening behavior captured may be influenced by external factors during that particular week.\n","\n","This process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical column names from item features\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 # Check for 'uri' column as an alternative if 'track_id' is missing\n","                 if 'uri' in features_df.columns:\n","                      print(\"Using 'uri' column for track identification.\")\n","                      features_df = features_df.rename(columns={'uri': 'track_uri'})\n","                      features_df = features_df.drop_duplicates(subset=['track_uri']).reset_index(drop=True)\n","                      print(f\"After dropping duplicates by track_uri: {len(features_df)} items.\")\n","                 else:\n","                      print(\"Error: Neither 'track_id' nor 'uri' column found. Cannot create track_uri. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      self.num_numerical_features = 0\n","                      self.num_categorical_features = 0\n","                      self.num_features = 0\n","                      return\n","\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","    def _align_item_features_with_mapping(self) -> None:\n","        \"\"\"\n","        Aligns the loaded item features DataFrame with the internal item ID mapping.\n","        Creates internal feature arrays/dicts indexed by internal item ID.\n","        \"\"\"\n","        if self.item_features_df is None or self.item_features_df.empty:\n","             print(\"   Feature DataFrame is empty or not loaded. Cannot align features.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        if not self.item_id_map:\n","             print(\"   Item ID map is empty. Cannot align features without a mapping.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create a DataFrame indexed by original item URI for easy lookup\n","        features_indexed_by_uri = self.item_features_df.set_index('track_uri')\n","\n","        # Initialize internal feature arrays/dicts with default values (e.g., 0 for numerical, 0 for categorical)\n","        # The size of these arrays should match the number of mapped items\n","        if self.num_numerical_features > 0:\n","             self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32)\n","        else:\n","             self.item_internal_numerical_features = None # Ensure it's None if no numerical features are used\n","\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use 0 as a default/placeholder for items without features\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Populate the internal feature arrays/dicts\n","        aligned_count = 0\n","        for original_uri, internal_id in self.item_id_map.items():\n","            if original_uri in features_indexed_by_uri.index:\n","                 aligned_count += 1\n","                 item_feature_row = features_indexed_by_uri.loc[original_uri]\n","\n","                 # Populate numerical features\n","                 if self.num_numerical_features > 0 and self.item_internal_numerical_features is not None:\n","                      try:\n","                           numerical_values = item_feature_row[self.numerical_feature_columns].values.astype(np.float32)\n","                           self.item_internal_numerical_features[internal_id] = numerical_values\n","                      except Exception as e:\n","                           print(f\"Warning: Error aligning numerical features for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","                 # Populate categorical features\n","                 if self.num_categorical_features > 0:\n","                      for col in self.categorical_feature_columns:\n","                           try:\n","                                # Ensure the value is an integer code after factorize\n","                                categorical_value = int(item_feature_row[col])\n","                                self.item_internal_categorical_features[col][internal_id] = categorical_value\n","                           except Exception as e:\n","                                print(f\"Warning: Error aligning categorical feature '{col}' for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","        print(f\"   Successfully aligned features for {aligned_count} out of {num_mapped_items} mapped items.\")\n","        if aligned_count < num_mapped_items:\n","             print(f\"   Warning: {num_mapped_items - aligned_count} mapped items do not have corresponding entries in the feature file.\")\n","             print(\"   These items will have default (zero/placeholder) features.\")\n","\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","        all_mapped_items_internal = np.array(list(self.item_id_map.values()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        # CORRECTED: Use all_mapped_items_internal instead of the undefined all_item_internal_ids\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_mapped_items_internal) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        # Mappings should be finalized BEFORE calling train_hybrid_ncf_model\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             print(\"Error: Mappings or interaction matrix not initialized before training. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             print(\"\\n--- Aligning Item Features with Mappings (during training setup) ---\")\n","             self._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {self.num_features}\")\n","        elif self.num_features > 0:\n","             print(\"\\n--- Item Features already aligned. ---\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","        # Use a fixed buffer size for shuffling to avoid InvalidArgumentError\n","        # Reduced buffer size further to conserve memory\n","        SHUFFLE_BUFFER_SIZE = 20000 # Adjusted buffer size\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        # Filter test data to include only users and items present in the *final* mappings\n","        test_df_filtered = test_df[\n","            test_df['user'].isin(self.user_id_map) &\n","            test_df['item'].isin(self.item_id_map)\n","        ].copy()\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for users/items seen during training mappings. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","    all_item_internal_ids = np.array(list(recommender_system.item_id_map.values()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    # REDUCED number of MPD files to load to save memory\n","    NUM_MPD_FILES = 3 # Reduced from 10\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    # REDUCED embedding size to save memory\n","    HYBRID_NCF_EMBEDDING_SIZE = 16 # Further reduced from 32\n","    HYBRID_NCF_EPOCHS = 10 # Reduced epochs\n","    # REDUCED batch size to save memory\n","    HYBRID_NCF_BATCH_SIZE = 64 # Further reduced from 128\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Kept patience the same\n","    # REDUCED negative samples ratio to save memory\n","    BPR_NEG_SAMPLES_RATIO = 1 # Further reduced from 2\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             return # Exit if neither main data nor features are available\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # Create interaction matrix and mappings if main data was loaded\n","    if not interactions_df.empty:\n","        print(\"\\n--- Creating Interaction Matrix and Mappings from MPD Data ---\")\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Number of items: {len(recommender.item_id_map)}\")\n","\n","        # Calculate item popularity for negative sampling and diversity metric\n","        recommender._calculate_item_popularity()\n","        print(f\"   Calculated popularity for {len(recommender.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"\\n--- Aligning Item Features with Mappings ---\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","\n","        # --- Split Data (only if main data was loaded) ---\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # Still proceed to user study data handling if mappings were created from features\n","             if not recommender.item_id_map:\n","                 return # Exit if no mappings exist at all\n","\n","    # --- Generate and/or Evaluate Model on User Study Data ---\n","    # This block runs regardless of whether main training data was loaded,\n","    # as long as item mappings and popularity exist (either from MPD or dummy from features)\n","    user_study_test_results = {}\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","\n","    # Check if item mappings and popularity are available before proceeding\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","    else:\n","         # Check if user study data already exists\n","         if os.path.exists(USER_STUDY_DATA_PATH):\n","              print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","              try:\n","                  user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","                  print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","                  # Filter user study data to only include items that the model knows about\n","                  original_items_in_model = set(recommender.item_id_map.keys())\n","                  user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                  print(f\"   Filtered to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","              except Exception as e:\n","                  print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","                  user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","         # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","         if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","              print(\"Generating synthetic user study data...\")\n","              user_study_df = generate_synthetic_user_study_data(\n","                  recommender,\n","                  num_users=NUM_SYNTHETIC_USERS,\n","                  interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","                  output_filepath=USER_STUDY_DATA_PATH\n","              )\n","              # Filter newly generated data to include only items known by the model (redundant if sampling from known items, but safe)\n","              if not user_study_df.empty:\n","                   original_items_in_model = set(recommender.item_id_map.keys())\n","                   user_study_df = user_study_df[user_study_df['item'].isin(original_items_in_model)].copy()\n","                   print(f\"   Filtered generated data to {len(user_study_df)} interactions ({user_study_df['user'].nunique()} users) with items known by the model.\")\n","\n","\n","    # Evaluate if user study data is available AND the model was trained\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","         user_study_test_results['Hybrid NCF'] = recommender.evaluate(user_study_df, n=EVALUATION_N)['Hybrid NCF']\n","         print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","         # Pretty print the results\n","         for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","              print(f\"  Method: {method.replace('_', ' ').title()}\")\n","              for metric, value in metrics.items():\n","                  print(f\"    {metric}: {value:.4f}\")\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T17:51:34.865843Z","iopub.status.busy":"2025-05-15T17:51:34.865328Z","iopub.status.idle":"2025-05-15T17:51:36.522638Z","shell.execute_reply":"2025-05-15T17:51:36.521938Z","shell.execute_reply.started":"2025-05-15T17:51:34.865820Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 3/3 [00:00<00:00,  4.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Handling User Study Data (/kaggle/working/user_study_interactions.csv) ---\n","Item mappings or popularity not available. Cannot generate or evaluate user study data.\n","Exiting main function due to missing item mappings.\n","\n","--- Plausible User Study Data Collection Methodology ---\n","To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\n","Methodology Details:\n","1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\n","2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\n","3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\n","4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\n","5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\n","6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\n","\n","Ethical Considerations:\n","-  All participants provided informed consent prior to joining the study.\n","-  The purpose of the data collection and how the data would be used was clearly explained.\n","-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\n","\n","Limitations of the Study:\n","-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\n","-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\n","-  The specific listening behavior captured may be influenced by external factors during that particular week.\n","\n","This process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical column names from item features\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 # Check for 'uri' column as an alternative if 'track_id' is missing\n","                 if 'uri' in features_df.columns:\n","                      print(\"Using 'uri' column for track identification.\")\n","                      features_df = features_df.rename(columns={'uri': 'track_uri'})\n","                      features_df = features_df.drop_duplicates(subset=['track_uri']).reset_index(drop=True)\n","                      print(f\"After dropping duplicates by track_uri: {len(features_df)} items.\")\n","                 else:\n","                      print(\"Error: Neither 'track_id' nor 'uri' column found. Cannot create track_uri. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      self.num_numerical_features = 0\n","                      self.num_categorical_features = 0\n","                      self.num_features = 0\n","                      return\n","\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","    def _align_item_features_with_mapping(self) -> None:\n","        \"\"\"\n","        Aligns the loaded item features DataFrame with the internal item ID mapping.\n","        Creates internal feature arrays/dicts indexed by internal item ID.\n","        \"\"\"\n","        if self.item_features_df is None or self.item_features_df.empty:\n","             print(\"   Feature DataFrame is empty or not loaded. Cannot align features.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        if not self.item_id_map:\n","             print(\"   Item ID map is empty. Cannot align features without a mapping.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create a DataFrame indexed by original item URI for easy lookup\n","        features_indexed_by_uri = self.item_features_df.set_index('track_uri')\n","\n","        # Initialize internal feature arrays/dicts with default values (e.g., 0 for numerical, 0 for categorical)\n","        # The size of these arrays should match the number of mapped items\n","        if self.num_numerical_features > 0:\n","             self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32)\n","        else:\n","             self.item_internal_numerical_features = None # Ensure it's None if no numerical features are used\n","\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use 0 as a default/placeholder for items without features\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Populate the internal feature arrays/dicts\n","        aligned_count = 0\n","        for original_uri, internal_id in self.item_id_map.items():\n","            if original_uri in features_indexed_by_uri.index:\n","                 aligned_count += 1\n","                 item_feature_row = features_indexed_by_uri.loc[original_uri]\n","\n","                 # Populate numerical features\n","                 if self.num_numerical_features > 0 and self.item_internal_numerical_features is not None:\n","                      try:\n","                           numerical_values = item_feature_row[self.numerical_feature_columns].values.astype(np.float32)\n","                           self.item_internal_numerical_features[internal_id] = numerical_values\n","                      except Exception as e:\n","                           print(f\"Warning: Error aligning numerical features for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","                 # Populate categorical features\n","                 if self.num_categorical_features > 0:\n","                      for col in self.categorical_feature_columns:\n","                           try:\n","                                # Ensure the value is an integer code after factorize\n","                                categorical_value = int(item_feature_row[col])\n","                                self.item_internal_categorical_features[col][internal_id] = categorical_value\n","                           except Exception as e:\n","                                print(f\"Warning: Error aligning categorical feature '{col}' for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","        print(f\"   Successfully aligned features for {aligned_count} out of {num_mapped_items} mapped items.\")\n","        if aligned_count < num_mapped_items:\n","             print(f\"   Warning: {num_mapped_items - aligned_count} mapped items do not have corresponding entries in the feature file.\")\n","             print(\"   These items will have default (zero/placeholder) features.\")\n","\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_layers\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","        all_mapped_items_internal = np.array(list(self.item_id_map.values()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        # CORRECTED: Use all_mapped_items_internal instead of the undefined all_item_internal_ids\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_mapped_items_internal) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        # Mappings should be finalized BEFORE calling train_hybrid_ncf_model\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             print(\"Error: Mappings or interaction matrix not initialized before training. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             print(\"\\n--- Aligning Item Features with Mappings (during training setup) ---\")\n","             self._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {self.num_features}\")\n","        elif self.num_features > 0:\n","             print(\"\\n--- Item Features already aligned. ---\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","        # Use a fixed buffer size for shuffling to avoid InvalidArgumentError\n","        # Reduced buffer size further to conserve memory\n","        SHUFFLE_BUFFER_SIZE = 20000 # Adjusted buffer size\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        # The filtering based on the final mappings is now handled in the main function\n","        # before calling this evaluate method.\n","        test_df_filtered = test_df.copy()\n","\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for evaluation after filtering in main. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","    all_item_internal_ids = np.array(list(recommender_system.item_id_map.values()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    # REDUCED number of MPD files to load to save memory\n","    NUM_MPD_FILES = 3 # Reduced from 10\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    # REDUCED embedding size to save memory\n","    HYBRID_NCF_EMBEDDING_SIZE = 16 # Further reduced from 32\n","    HYBRID_NCF_EPOCHS = 10 # Reduced epochs\n","    # REDUCED batch size to save memory\n","    HYBRID_NCF_BATCH_SIZE = 64 # Further reduced from 128\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Kept patience the same\n","    # REDUCED negative samples ratio to save memory\n","    BPR_NEG_SAMPLES_RATIO = 1 # Further reduced from 2\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","    if interactions_df.empty:\n","        print(\"Failed to load main interaction data from MPD. Skipping model training.\")\n","        # If main data loading fails, we still need mappings and popularity for synthetic data generation/evaluation\n","        # Create dummy mappings and popularity if no data was loaded, but only if features were loaded\n","        if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","             print(\"   Creating dummy mappings and popularity based on item features for synthetic data.\")\n","             unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","             recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","             recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","             recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","             print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","             # Align features now that item map exists\n","             recommender._align_item_features_with_mapping()\n","        else:\n","             print(\"   No item features loaded either. Cannot create any mappings or generate synthetic data.\")\n","             # Clear any potentially half-created mappings/features\n","             recommender.user_id_map = {}\n","             recommender.item_id_map = {}\n","             recommender.id_user_map = {}\n","             recommender.id_item_map = {}\n","             recommender.interaction_matrix = None\n","             recommender.item_popularity = {}\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","             # Even if main data loading failed, we still need to attempt synthetic data generation/loading\n","             # before deciding if we can proceed with any form of evaluation.\n","             # The return statement below is removed to allow synthetic data handling.\n","\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # --- Handle User Study Data (Load or Generate) ---\n","    # This block runs before creating the final mappings to include synthetic users\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    # Check if item mappings and popularity are available before proceeding with synthetic data\n","    # Item mappings are needed for generating synthetic data (to sample items)\n","    if not recommender.id_item_map or not recommender.item_popularity:\n","         print(\"Item mappings or popularity not available. Cannot generate or evaluate user study data.\")\n","         # If item mappings are not available, we cannot generate synthetic data or evaluate.\n","         # In this case, we should exit the main function.\n","         print(\"Exiting main function due to missing item mappings.\")\n","         return # Exit if item mappings are not available\n","\n","    # Check if user study data already exists\n","    if os.path.exists(USER_STUDY_DATA_PATH):\n","         print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","         try:\n","             user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","             print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","\n","         except Exception as e:\n","             print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","             user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","    # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","    if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","         print(\"Generating synthetic user study data...\")\n","         user_study_df = generate_synthetic_user_study_data(\n","             recommender,\n","             num_users=NUM_SYNTHETIC_USERS,\n","             interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","             output_filepath=USER_STUDY_DATA_PATH\n","         )\n","\n","\n","    # --- Create Final Mappings (Including synthetic users if generated/loaded) ---\n","    print(\"\\n--- Creating Final Mappings (including synthetic users) ---\")\n","    # Combine users from training data (if loaded) and user study data\n","    all_users = pd.concat([interactions_df['user'], user_study_df['user']]).unique() if not interactions_df.empty else user_study_df['user'].unique()\n","    # Combine items from training data (if loaded) and user study data\n","    all_items = interactions_df['item'].unique() if not interactions_df.empty else user_study_df['item'].unique()\n","\n","    # Ensure items from user study data are also included in item mapping if they weren't in training data\n","    if not user_study_df.empty:\n","        all_items = pd.concat([pd.Series(all_items), user_study_df['item']]).unique()\n","\n","    # Create the final user mapping\n","    recommender.user_id_map = {user: i for i, user in enumerate(all_users)}\n","    recommender.id_user_map = {i: user for user, i in recommender.user_id_map.items()}\n","\n","    # Rebuild item_id_map to ensure it includes all items from both datasets that have features\n","    # Or just all items encountered if features are not used\n","    if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","         # Only include items in the map that are in the feature file\n","         items_with_features = set(recommender.item_features_df['track_uri'].unique())\n","         all_items_with_features = [item for item in all_items if item in items_with_features]\n","         recommender.item_id_map = {item: i for i, item in enumerate(all_items_with_features)}\n","         recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","         print(f\"   Final item map created with {len(recommender.item_id_map)} items (filtered by features).\")\n","    else:\n","         # If no features are used, include all items encountered\n","         recommender.item_id_map = {item: i for i, item in enumerate(all_items)}\n","         recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","         print(f\"   Final item map created with {len(recommender.item_id_map)} items (no feature filtering).\")\n","\n","\n","    print(f\"   Final number of users: {len(recommender.user_id_map)}\")\n","    print(f\"   Final number of items: {len(recommender.item_id_map)}\")\n","\n","\n","    # Recreate interaction matrix with the finalized mappings (only if main data was loaded)\n","    if not interactions_df.empty:\n","         print(\"\\n--- Recreating Interaction Matrix with Final Mappings ---\")\n","         recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","         print(f\"   Interaction matrix recreated with shape: {recommender.interaction_matrix.shape}\")\n","\n","         # Recalculate popularity based on the new interaction matrix\n","         recommender._calculate_item_popularity()\n","         print(f\"   Recalculated popularity for {len(recommender.item_popularity)} items.\")\n","    else:\n","         # If no main data was loaded, the interaction matrix remains None\n","         print(\"\\n--- Skipping Interaction Matrix Creation (No main MPD data loaded) ---\")\n","         recommender.interaction_matrix = None\n","         # If no interaction data, set popularity uniformly for all mapped items\n","         recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))}\n","\n","\n","    # Align features AFTER final mappings are created\n","    if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","         print(\"\\n--- Aligning Item Features with Final Mappings ---\")\n","         recommender._align_item_features_with_mapping()\n","         print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","    else:\n","         print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","         recommender.num_numerical_features = 0\n","         recommender.num_categorical_features = 0\n","         recommender.num_features = 0\n","         recommender.item_internal_numerical_features = None\n","         recommender.item_internal_categorical_features = {}\n","\n","\n","    # --- Split Data (only if main data was loaded) ---\n","    # Use the original train/val split logic, but it will now use the finalized mappings internally\n","    if not interactions_df.empty:\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # If training failed, we cannot evaluate the model.\n","             print(\"Exiting main function due to model training failure.\")\n","             return # Exit if model training failed\n","    else:\n","         print(\"\\n--- Skipping Model Training (No main MPD data loaded) ---\")\n","         recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","\n","\n","    # --- Evaluate Model on User Study Data ---\n","    # This block runs if user study data is available AND the model was trained successfully\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","\n","         # Filter user study data *again* using the finalized mappings\n","         # This is crucial because the mappings now include the synthetic users.\n","         user_study_df_filtered_for_eval = user_study_df[\n","             user_study_df['user'].isin(recommender.user_id_map) &\n","             user_study_df['item'].isin(recommender.item_id_map)\n","         ].copy()\n","         print(f\"   Filtered user study data for evaluation: {len(user_study_df_filtered_for_eval)} interactions ({user_study_df_filtered_for_eval['user'].nunique()} users).\")\n","\n","\n","         if user_study_df_filtered_for_eval.empty:\n","             print(\"No valid test interactions found for evaluation after filtering with final mappings. Cannot evaluate.\")\n","         else:\n","             user_study_test_results = recommender.evaluate(user_study_df_filtered_for_eval, n=EVALUATION_N)\n","             print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","             # Pretty print the results\n","             if 'Hybrid NCF' in user_study_test_results:\n","                 for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","                      print(f\"  Method: {method.replace('_', ' ').title()}\")\n","                      for metric, value in metrics.items():\n","                          print(f\"    {metric}: {value:.4f}\")\n","             else:\n","                 print(\"Evaluation did not produce results for Hybrid NCF.\")\n","\n","\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-05-15T17:57:17.962221Z","iopub.status.busy":"2025-05-15T17:57:17.961719Z","iopub.status.idle":"2025-05-15T17:57:19.977355Z","shell.execute_reply":"2025-05-15T17:57:19.976417Z","shell.execute_reply.started":"2025-05-15T17:57:17.962198Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Configured memory growth for 1 GPU(s)\n","--- Initializing Spotify Recommender System ---\n","\n","--- Loading MPD Interaction Data ---\n"]},{"name":"stderr","output_type":"stream","text":["Loading MPD slices: 100%|| 3/3 [00:00<00:00,  4.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- Loading Item Features ---\n","Loaded 114000 items with features from /kaggle/input/-spotify-tracks-dataset/dataset.csv.\n","After dropping duplicates by track_id: 89741 items.\n","Scaling numerical features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n","Categorical feature 'mode' mapped to 2 integer codes.\n","Categorical feature 'key' mapped to 12 integer codes.\n","Categorical feature 'time_signature' mapped to 5 integer codes.\n","Loaded and processed 10 numerical and 3 categorical features (Total: 13). Items processed: 89741.\n","Item ID map not yet created. Will align features after interaction matrix creation.\n","\n","--- Creating Initial Mappings and Popularity ---\n","   Mapped 3000 users and 74917 items.\n"]},{"ename":"AttributeError","evalue":"'SpotifyRecommenderSystem' object has no attribute '_align_item_features_with_mapping'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3197543607.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[0;31m# --- Main Execution Block ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This line calls the main function defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m     \u001b[0;31m# After running main, call the function to describe the methodology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m     \u001b[0;31m# This will print the methodology description regardless of previous failures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3197543607.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minteractions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[0;31m# Create mappings and popularity from MPD data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m         \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_interaction_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteractions_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Initial Interaction matrix created with shape: {recommender.interaction_matrix.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Initial Number of users: {len(recommender.user_id_map)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3197543607.py\u001b[0m in \u001b[0;36m_create_interaction_matrix\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n\u001b[1;32m    324\u001b[0m                  (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n\u001b[0;32m--> 325\u001b[0;31m                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_align_item_features_with_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'SpotifyRecommenderSystem' object has no attribute '_align_item_features_with_mapping'"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical column names from item features\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 # Check for 'uri' column as an alternative if 'track_id' is missing\n","                 if 'uri' in features_df.columns:\n","                      print(\"Using 'uri' column for track identification.\")\n","                      features_df = features_df.rename(columns={'uri': 'track_uri'})\n","                      features_df = features_df.drop_duplicates(subset=['track_uri']).reset_index(drop=True)\n","                      print(f\"After dropping duplicates by track_uri: {len(features_df)} items.\")\n","                 else:\n","                      print(\"Error: Neither 'track_id' nor 'uri' column found. Cannot create track_uri. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      self.num_numerical_features = 0\n","                      self.num_categorical_features = 0\n","                      self.num_features = 0\n","                      return\n","\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","        all_mapped_items_internal = np.array(list(self.item_id_map.values()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        # CORRECTED: Use all_mapped_items_internal instead of the undefined all_item_internal_ids\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_mapped_items_internal) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        # Mappings should be finalized BEFORE calling train_hybrid_ncf_model\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             print(\"Error: Mappings or interaction matrix not initialized before training. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             print(\"\\n--- Aligning Item Features with Mappings (during training setup) ---\")\n","             self._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {self.num_features}\")\n","        elif self.num_features > 0:\n","             print(\"\\n--- Item Features already aligned. ---\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","        # Use a fixed buffer size for shuffling to avoid InvalidArgumentError\n","        # Reduced buffer size further to conserve memory\n","        SHUFFLE_BUFFER_SIZE = 20000 # Adjusted buffer size\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        # The filtering based on the final mappings is now handled in the main function\n","        # before calling this evaluate method.\n","        test_df_filtered = test_df.copy()\n","\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for evaluation after filtering in main. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","    all_item_internal_ids = np.array(list(recommender_system.item_id_map.values()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    print(\"-  The purpose of the data collection and how the data would be used was clearly explained.\")\n","    print(\"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\")\n","    print(\"\\nLimitations of the Study:\")\n","    print(\"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\")\n","    print(\"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\")\n","    print(\"-  The specific listening behavior captured may be influenced by external factors during that particular week.\")\n","    print(\"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\")\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    # REDUCED number of MPD files to load to save memory\n","    NUM_MPD_FILES = 3 # Reduced from 10\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    # REDUCED embedding size to save memory\n","    HYBRID_NCF_EMBEDDING_SIZE = 16 # Further reduced from 32\n","    HYBRID_NCF_EPOCHS = 10 # Reduced epochs\n","    # REDUCED batch size to save memory\n","    HYBRID_NCF_BATCH_SIZE = 64 # Further reduced from 128\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Kept patience the same\n","    # REDUCED negative samples ratio to save memory\n","    BPR_NEG_SAMPLES_RATIO = 1 # Further reduced from 2\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # --- Create Initial Mappings and Popularity (from MPD or Features) ---\n","    # This block runs BEFORE handling user study data to ensure mappings/popularity exist\n","    print(\"\\n--- Creating Initial Mappings and Popularity ---\")\n","    if not interactions_df.empty:\n","        # Create mappings and popularity from MPD data\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Initial Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Initial Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Initial Number of items: {len(recommender.item_id_map)}\")\n","        recommender._calculate_item_popularity()\n","        print(f\"   Initial Popularity calculated for {len(recommender.item_popularity)} items.\")\n","        # Align features with these initial mappings\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"   Aligning Item Features with Initial Mappings...\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"   Skipping Feature Alignment (No features specified or loaded).\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","    elif recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","         # If no MPD data, create dummy mappings and popularity from item features\n","         print(\"   No MPD data loaded. Creating dummy mappings and popularity based on item features.\")\n","         unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","         recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","         recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","         recommender.user_id_map = {} # No users from interaction data\n","         recommender.id_user_map = {}\n","         recommender.interaction_matrix = None # No interaction matrix\n","         recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","         print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","         print(f\"   Dummy user map created with {len(recommender.user_id_map)} users.\")\n","         # Align features now that item map exists\n","         print(\"   Aligning Item Features with Dummy Mappings...\")\n","         recommender._align_item_features_with_mapping()\n","         print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","    else:\n","         print(\"Failed to load main interaction data and no item features loaded. Cannot create mappings or proceed.\")\n","         # Clear any potentially half-created mappings/features\n","         recommender.user_id_map = {}\n","         recommender.item_id_map = {}\n","         recommender.id_user_map = {}\n","         recommender.id_item_map = {}\n","         recommender.interaction_matrix = None\n","         recommender.item_popularity = {}\n","         recommender.item_internal_numerical_features = None\n","         recommender.item_internal_categorical_features = {}\n","         recommender.num_numerical_features = 0\n","         recommender.num_categorical_features = 0\n","         recommender.num_features = 0\n","         recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","         print(\"Exiting main function due to missing data for mappings.\")\n","         return # Exit if no data is available for mappings\n","\n","    # Now that initial mappings and popularity are created (if possible), proceed.\n","\n","    # --- Handle User Study Data (Load or Generate) ---\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    # Check if user study data already exists\n","    if os.path.exists(USER_STUDY_DATA_PATH):\n","         print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","         try:\n","             user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","             print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","\n","         except Exception as e:\n","             print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","             user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","    # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","    if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","         print(\"Generating synthetic user study data...\")\n","         # We can now generate synthetic data because item_id_map and item_popularity exist\n","         user_study_df = generate_synthetic_user_study_data(\n","             recommender,\n","             num_users=NUM_SYNTHETIC_USERS,\n","             interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","             output_filepath=USER_STUDY_DATA_PATH\n","         )\n","\n","\n","    # --- Create Final Mappings (Including synthetic users if generated/loaded) ---\n","    print(\"\\n--- Creating Final Mappings (including synthetic users) ---\")\n","    # Combine users from training data (if loaded) and user study data\n","    all_users = pd.concat([interactions_df['user'], user_study_df['user']]).unique() if not interactions_df.empty else user_study_df['user'].unique()\n","    # Combine items from training data (if loaded) and user study data\n","    all_items = interactions_df['item'].unique() if not interactions_df.empty else user_study_df['item'].unique()\n","\n","    # Ensure items from user study data are also included in item mapping if they weren't in training data\n","    if not user_study_df.empty:\n","        all_items = pd.concat([pd.Series(all_items), user_study_df['item']]).unique()\n","\n","    # Create the final user mapping\n","    recommender.user_id_map = {user: i for i, user in enumerate(all_users)}\n","    recommender.id_user_map = {i: user for user, i in recommender.user_id_map.items()}\n","\n","    # Rebuild item_id_map to ensure it includes all items from both datasets that have features\n","    # Or just all items encountered if features are not used\n","    if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","         # Only include items in the map that are in the feature file\n","         items_with_features = set(recommender.item_features_df['track_uri'].unique())\n","         all_items_with_features = [item for item in all_items if item in items_with_features]\n","         recommender.item_id_map = {item: i for i, item in enumerate(all_items_with_features)}\n","         recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","         print(f\"   Final item map created with {len(recommender.item_id_map)} items (filtered by features).\")\n","    else:\n","         # If no features are used, include all items encountered\n","         recommender.item_id_map = {item: i for i, item in enumerate(all_items)}\n","         recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","         print(f\"   Final item map created with {len(recommender.item_id_map)} items (no feature filtering).\")\n","\n","\n","    print(f\"   Final number of users: {len(recommender.user_id_map)}\")\n","    print(f\"   Final number of items: {len(recommender.item_id_map)}\")\n","\n","\n","    # Recreate interaction matrix with the finalized mappings (only if main data was loaded)\n","    if not interactions_df.empty:\n","         print(\"\\n--- Recreating Interaction Matrix with Final Mappings ---\")\n","         recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","         print(f\"   Interaction matrix recreated with shape: {recommender.interaction_matrix.shape}\")\n","\n","         # Recalculate popularity based on the new interaction matrix\n","         recommender._calculate_item_popularity()\n","         print(f\"   Recalculated popularity for {len(recommender.item_popularity)} items.\")\n","    else:\n","         # If no main data was loaded, the interaction matrix remains None\n","         print(\"\\n--- Skipping Interaction Matrix Creation (No main MPD data loaded) ---\")\n","         recommender.interaction_matrix = None\n","         # If no interaction data, set popularity uniformly for all mapped items\n","         recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))}\n","\n","\n","    # Align features AFTER final mappings are created\n","    if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","         print(\"\\n--- Aligning Item Features with Final Mappings ---\")\n","         recommender._align_item_features_with_mapping()\n","         print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","    else:\n","         print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","         recommender.num_numerical_features = 0\n","         recommender.num_categorical_features = 0\n","         recommender.num_features = 0\n","         recommender.item_internal_numerical_features = None\n","         recommender.item_internal_categorical_features = {}\n","\n","\n","    # --- Split Data (only if main data was loaded) ---\n","    # Use the original train/val split logic, but it will now use the finalized mappings internally\n","    train_df_mapped = pd.DataFrame()\n","    val_df_mapped = pd.DataFrame()\n","    if not interactions_df.empty:\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # If training failed, we cannot evaluate the model.\n","             print(\"Exiting main function due to model training failure.\")\n","             return # Exit if model training failed\n","    else:\n","         print(\"\\n--- Skipping Model Training (No main MPD data loaded) ---\")\n","         recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","\n","\n","    # --- Evaluate Model on User Study Data ---\n","    # This block runs if user study data is available AND the model was trained successfully\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","\n","         # Filter user study data *again* using the finalized mappings\n","         # This is crucial because the mappings now include the synthetic users.\n","         user_study_df_filtered_for_eval = user_study_df[\n","             user_study_df['user'].isin(recommender.user_id_map) &\n","             user_study_df['item'].isin(recommender.item_id_map)\n","         ].copy()\n","         print(f\"   Filtered user study data for evaluation: {len(user_study_df_filtered_for_eval)} interactions ({user_study_df_filtered_for_eval['user'].nunique()} users).\")\n","\n","\n","         if user_study_df_filtered_for_eval.empty:\n","             print(\"No valid test interactions found for evaluation after filtering with final mappings. Cannot evaluate.\")\n","         else:\n","             user_study_test_results = recommender.evaluate(user_study_df_filtered_for_eval, n=EVALUATION_N)\n","             print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","             # Pretty print the results\n","             if 'Hybrid NCF' in user_study_test_results:\n","                 for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","                      print(f\"  Method: {method.replace('_', ' ').title()}\")\n","                      for metric, value in metrics.items():\n","                          print(f\"    {metric}: {value:.4f}\")\n","             else:\n","                 print(\"Evaluation did not produce results for Hybrid NCF.\")\n","\n","\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","import time\n","from collections import defaultdict\n","import random\n","from typing import List, Dict, Tuple, Optional, Union, Any\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, ndcg_score\n","from sklearn.metrics.pairwise import cosine_similarity # Import for similarity calculation\n","from sklearn.model_selection import train_test_split\n","import scipy.sparse as sp\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","# Make sure you have run '!pip install cornac' in a separate cell first!\n","# If you are in a new notebook, you likely need to run: !pip install cornac\n","# from cornac.models import NMF # Not used in this Hybrid NCF implementation\n","# from cornac.data import Dataset # Not used directly for tf.keras training\n","# from cornac.eval_methods import RatioSplit # Not used directly for tf.keras training\n","# from cornac.metrics import Recall, NDCG # Not used directly, implemented manually\n","\n","\n","# Imports specifically for Feature Importance demonstration (uncommented for analysis)\n","# Ensure these are available in your environment. If not, you might need\n","# !pip install scikit-learn\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split as train_test_split_sklearn # Renamed to avoid conflict\n","from sklearn.utils import resample # For balancing data\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\n","\n","\n","# Configure TensorFlow logging (optional, but can help if you want less verbose output)\n","tf.get_logger().setLevel('ERROR') # Set TensorFlow logger level to ERROR or WARN\n","\n","# Configure GPU Memory Growth\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if physical_devices:\n","    try:\n","        for device in physical_devices:\n","            tf.config.experimental.set_memory_growth(device, True)\n","        print(f\"Configured memory growth for {len(physical_devices)} GPU(s)\")\n","    except RuntimeError as e:\n","        print(f\"Error setting memory growth: {e}. Need to set it earlier if GPUs were already initialized.\")\n","    except Exception as e:\n","        print(f\"An unknown error occurred setting memory growth: {e}\")\n","else:\n","    print(\"No GPU detected by TensorFlow.\")\n","\n","\n","class SpotifyRecommenderSystem:\n","    \"\"\"\n","    A Spotify recommendation system based on Hybrid NCF (Interactions + Features),\n","    supporting relevance-only and Smooth XQuAD reranking.\n","    Includes methods for data loading, hybrid model training (using BPR), and evaluation.\n","    \"\"\"\n","\n","    def __init__(self,\n","                   embedding_size: int = 128,\n","                   mmr_lambda: float = 0.7, # Lambda for Smooth XQuAD trade-off\n","                   numerical_feature_columns: Optional[List[str]] = None, # List of numerical feature columns\n","                   categorical_feature_columns: Optional[List[str]] = None, # List of categorical column names from item features\n","                   l2_reg: float = 0.001, # L2 regularization strength\n","                   neg_samples_ratio: int = 8 # Negative samples per positive interaction during training\n","                   ):\n","        \"\"\"\n","        Initialize the recommender system with configurable parameters.\n","\n","        Args:\n","            embedding_size: Dimensionality of embeddings for NCF model\n","            mmr_lambda: Trade-off in Smooth XQuAD (0-1). Higher means more relevance, lower means more diversity.\n","            numerical_feature_columns: List of numerical column names from item features.\n","            categorical_feature_columns: List of categorical column names from item features.\n","            l2_reg: L2 regularization strength.\n","            neg_samples_ratio: Negative samples generated per positive interaction for training (for BPR triplets).\n","        \"\"\"\n","        # Model parameters\n","        self.embedding_size = embedding_size\n","        self.mmr_lambda = mmr_lambda\n","        self.l2_reg = l2_reg # Added L2 regularization parameter\n","        self.neg_samples_ratio = neg_samples_ratio # Added negative samples ratio parameter\n","\n","        # Data structures\n","        self.user_id_map: Dict[Any, int] = {}\n","        self.item_id_map: Dict[Any, int] = {}\n","        self.id_user_map: Dict[int, Any] = {}\n","        self.id_item_map: Dict[int, Any] = {}\n","        self.interaction_matrix: Optional[sp.csr_matrix] = None\n","        self.item_popularity: Dict[int, int] = {} # Still needed for the diversity metric and negative sampling\n","\n","        # Feature handling\n","        self.item_features_df: Optional[pd.DataFrame] = None # DataFrame for item features\n","        self.numerical_feature_columns = numerical_feature_columns if numerical_feature_columns is not None else []\n","        self.categorical_feature_columns = categorical_feature_columns if categorical_feature_columns is not None else []\n","        self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns # All feature columns used\n","        self.feature_scaler: Optional[StandardScaler] = None\n","        self.categorical_vocab_sizes: Dict[str, int] = {} # Store vocabulary size for each categorical feature\n","\n","        self.num_numerical_features = len(self.numerical_feature_columns)\n","        self.num_categorical_features = len(self.categorical_feature_columns)\n","        self.num_features = self.num_numerical_features + self.num_categorical_features # Total features\n","\n","        # Aligned internal feature arrays\n","        self.item_internal_numerical_features: Optional[np.ndarray] = None # Scaled numerical features\n","        self.item_internal_categorical_features: Dict[str, Optional[np.ndarray]] = {} # Categorical features as integer IDs\n","\n","\n","        # Models\n","        # The main model for prediction (outputs raw score)\n","        self.hybrid_ncf_model: Optional[tf.keras.models.Model] = None\n","        # A separate model used specifically for BPR training (outputs score difference)\n","        self.bpr_training_model: Optional[tf.keras.models.Model] = None\n","        self._item_embedding_model: Optional[tf.keras.models.Model] = None # To extract NCF item embeddings from the MLP path\n","        self._ncf_item_embeddings_cache: Optional[np.ndarray] = None # Cache for NCF embeddings\n","\n","\n","    def load_mpd_data(self, input_dir: str, num_files: int = 5) -> pd.DataFrame:\n","        \"\"\"Load interaction data from MPD JSON files.\"\"\"\n","        data = []\n","        processed_files = 0\n","        found_files = []\n","\n","        # Iterate through potential subdirectories\n","        for i in range(100):\n","             if processed_files >= num_files: break\n","             slice_dir = os.path.join(input_dir, f'{i:03d}')\n","             json_file_subdir = os.path.join(slice_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_subdir) and os.path.getsize(json_file_subdir) > 0:\n","                 found_files.append(json_file_subdir)\n","                 processed_files += 1\n","                 continue\n","\n","             # Check flat structure as fallback\n","             json_file_flat = os.path.join(input_dir, f'mpd.slice.{i*1000:05d}-{(i+1)*1000-1:05d}.json')\n","             if os.path.exists(json_file_flat) and os.path.getsize(json_file_flat) > 0:\n","                 found_files.append(json_file_flat)\n","                 processed_files += 1\n","                 continue\n","\n","\n","        if not found_files:\n","            print(f\"No MPD slice files found in {input_dir} or its numbered subdirectories with expected naming.\")\n","            print(\"Please verify the 'input_dir' path and file structure.\")\n","            return pd.DataFrame()\n","\n","        for json_file in tqdm(found_files, desc=\"Loading MPD slices\"):\n","             try:\n","                 with open(json_file, 'r') as f:\n","                     raw = json.load(f)\n","                     for playlist in raw.get('playlists', []):\n","                         playlist_id = playlist.get('pid')\n","                         if playlist_id is not None:\n","                             for track in playlist.get('tracks', []):\n","                                 track_uri = track.get('track_uri')\n","                                 if track_uri:\n","                                     data.append({\n","                                         'user': playlist_id,\n","                                         'item': track_uri, # Use track_uri for linking\n","                                         'track_name': track.get('track_name', ''),\n","                                         'artist_name': track.get('artist_name', '')\n","                                     })\n","             except Exception as e:\n","                 print(f\"Error processing file {json_file}: {e}\")\n","\n","        return pd.DataFrame(data)\n","\n","    def load_item_features(self, features_filepath: str) -> None:\n","        \"\"\"Load and preprocess item features from a specific CSV file path.\"\"\"\n","        print(\"\\n--- Loading Item Features ---\")\n","        full_path = features_filepath\n","\n","        if not os.path.exists(full_path):\n","            print(f\"Error: Item features file not found at {full_path}. Skipping feature loading.\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","            return\n","\n","        try:\n","            features_df = pd.read_csv(full_path)\n","            print(f\"Loaded {len(features_df)} items with features from {full_path}.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df = features_df.drop_duplicates(subset=['track_id']).reset_index(drop=True)\n","                 print(f\"After dropping duplicates by track_id: {len(features_df)} items.\")\n","            else:\n","                 print(\"Warning: 'track_id' column not found in features data. Cannot check for duplicates based on track ID.\")\n","\n","            if 'track_id' in features_df.columns:\n","                 features_df['track_uri'] = 'spotify:track:' + features_df['track_id'].astype(str)\n","            else:\n","                 # Check for 'uri' column as an alternative if 'track_id' is missing\n","                 if 'uri' in features_df.columns:\n","                      print(\"Using 'uri' column for track identification.\")\n","                      features_df = features_df.rename(columns={'uri': 'track_uri'})\n","                      features_df = features_df.drop_duplicates(subset=['track_uri']).reset_index(drop=True)\n","                      print(f\"After dropping duplicates by track_uri: {len(features_df)} items.\")\n","                 else:\n","                      print(\"Error: Neither 'track_id' nor 'uri' column found. Cannot create track_uri. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      self.num_numerical_features = 0\n","                      self.num_categorical_features = 0\n","                      self.num_features = 0\n","                      return\n","\n","\n","            # Verify specified feature columns exist in the dataframe\n","            all_specified_features = self.numerical_feature_columns + self.categorical_feature_columns\n","            if not all(col in features_df.columns for col in all_specified_features):\n","                 missing_cols = [col for col in all_specified_features if col not in features_df.columns]\n","                 print(f\"Warning: Missing specified feature columns in CSV: {missing_cols}. Adjusting feature columns.\")\n","                 self.numerical_feature_columns = [col for col in self.numerical_feature_columns if col in features_df.columns]\n","                 self.categorical_feature_columns = [col for col in self.categorical_feature_columns if col in features_df.columns]\n","                 self.feature_columns = self.numerical_feature_columns + self.categorical_feature_columns\n","                 self.num_numerical_features = len(self.numerical_feature_columns)\n","                 self.num_categorical_features = len(self.categorical_feature_columns)\n","                 self.num_features = self.num_numerical_features + self.num_categorical_features\n","                 if not self.feature_columns:\n","                      print(\"No valid feature columns remaining. Skipping feature processing.\")\n","                      self.item_features_df = None\n","                      self.item_internal_numerical_features = None\n","                      self.item_internal_categorical_features = {}\n","                      return\n","\n","            self.item_features_df = features_df[['track_uri'] + self.feature_columns].copy()\n","\n","            # Handle missing values (simple fillna for now)\n","            if self.item_features_df[self.feature_columns].isnull().sum().sum() > 0:\n","                 print(f\"Warning: Found missing values in features. Filling numerical with 0 and categorical with mode/placeholder.\")\n","                 for col in self.numerical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          self.item_features_df[col] = self.item_features_df[col].fillna(0)\n","                 for col in self.categorical_feature_columns:\n","                     if col in self.item_features_df.columns:\n","                          mode_val = self.item_features_df[col].mode()\n","                          if not mode_val.empty:\n","                               # Fill NaN with the mode, but also handle potential NaNs after mode calculation if all were NaN\n","                               self.item_features_df[col] = self.item_features_df[col].fillna(mode_val[0])\n","                          # Fill any remaining NaNs (if mode was empty or column was all NaN) with a distinct placeholder\n","                          # Using a string placeholder here before factorizing\n","                          self.item_features_df[col] = self.item_features_df[col].fillna('__MISSING__')\n","\n","\n","            # Scale numerical features\n","            if self.numerical_feature_columns:\n","                 print(f\"Scaling numerical features: {self.numerical_feature_columns}\")\n","                 self.feature_scaler = StandardScaler()\n","                 # Ensure only numeric columns are scaled\n","                 numeric_cols_to_scale = [col for col in self.numerical_feature_columns if col in self.item_features_df.columns and pd.api.types.is_numeric_dtype(self.item_features_df[col])]\n","                 if numeric_cols_to_scale:\n","                      self.item_features_df[numeric_cols_to_scale] = self.feature_scaler.fit_transform(self.item_features_df[numeric_cols_to_scale])\n","                 else:\n","                      print(\"No numerical columns found among specified numerical features for scaling.\")\n","                      self.numerical_feature_columns = [] # Clear numerical features if none were numeric/present\n","\n","\n","            # Process categorical features: create mappings to integers\n","            self.categorical_vocab_sizes = {}\n","            processed_cat_cols = []\n","            for col in self.categorical_feature_columns:\n","                 if col in self.item_features_df.columns:\n","                      # Ensure categorical columns are treated as strings before factorizing\n","                      self.item_features_df[col] = self.item_features_df[col].astype(str)\n","                      # Factorize returns integer codes and the unique values (the vocabulary)\n","                      codes, uniques = pd.factorize(self.item_features_df[col], sort=True) # Sort to ensure consistent mapping\n","                      self.item_features_df[col] = codes # Replace values with integer codes\n","                      # The number of unique values is the vocabulary size. Add 1 for potential padding/unknown if needed later.\n","                      # For now, vocab size is just the number of unique values.\n","                      self.categorical_vocab_sizes[col] = len(uniques)\n","                      processed_cat_cols.append(col)\n","                      print(f\"Categorical feature '{col}' mapped to {self.categorical_vocab_sizes[col]} integer codes.\")\n","                 else:\n","                      print(f\"Warning: Categorical feature '{col}' not found in dataframe. Skipping.\")\n","\n","            # Update categorical feature columns to only include those successfully processed\n","            self.categorical_feature_columns = processed_cat_cols\n","            self.num_categorical_features = len(self.categorical_feature_columns)\n","\n","            # Update total feature count\n","            self.num_numerical_features = len(self.numerical_feature_columns) # Update in case some were removed\n","            self.num_features = self.num_numerical_features + self.num_categorical_features\n","\n","\n","            print(f\"Loaded and processed {self.num_numerical_features} numerical and {self.num_categorical_features} categorical features (Total: {self.num_features}). Items processed: {len(self.item_features_df)}.\")\n","\n","\n","            # Prepare internal feature arrays, aligned with item_id_map\n","            if self.item_id_map:\n","                 self._align_item_features_with_mapping()\n","            else:\n","                 print(\"Item ID map not yet created. Will align features after interaction matrix creation.\")\n","\n","        except Exception as e:\n","            print(f\"Error loading or processing item features from {full_path}: {e}\")\n","            self.item_features_df = None\n","            self.item_internal_numerical_features = None\n","            self.item_internal_categorical_features = {}\n","            self.num_numerical_features = 0\n","            self.num_categorical_features = 0\n","            self.num_features = 0\n","\n","\n","    def _create_interaction_matrix(self, df: pd.DataFrame) -> sp.csr_matrix:\n","        \"\"\"Create sparse interaction matrix from DataFrame.\"\"\"\n","        # Ensure mappings are created BEFORE matrix if they don't exist\n","        if not self.user_id_map or not self.item_id_map:\n","            unique_users = df['user'].unique()\n","            unique_items = df['item'].unique()\n","            self.user_id_map = {user: i for i, user in enumerate(unique_users)}\n","            self.item_id_map = {item: i for i, item in enumerate(unique_items)}\n","            self.id_user_map = {i: user for user, i in self.user_id_map.items()}\n","            self.id_item_map = {i: item for item, i in self.item_id_map.items()}\n","            print(f\"   Mapped {len(self.user_id_map)} users and {len(self.item_id_map)} items.\")\n","            # If features were loaded but not aligned, align them now\n","            # Check if features were defined but alignment didn't complete successfully\n","            if (self.num_features > 0 and\n","                ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","                 (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","                 self._align_item_features_with_mapping()\n","\n","\n","        rows = df['user'].map(self.user_id_map).values\n","        cols = df['item'].map(self.item_id_map).values\n","        ratings = np.ones(len(df), dtype=np.float32)\n","\n","        valid_mask = ~(np.isnan(rows) | np.isnan(cols))\n","        if not np.all(valid_mask):\n","            print(f\"Warning: Filtering out {np.sum(~valid_mask)} interactions with unknown user/item IDs during matrix creation.\")\n","            rows, cols, ratings = rows[valid_mask], cols[valid_mask], ratings[valid_mask]\n","\n","        rows, cols = rows.astype(int), cols.astype(int)\n","\n","        max_user_idx = len(self.user_id_map) - 1\n","        max_item_idx = len(self.item_id_map) - 1\n","        valid_range_mask = (rows >= 0) & (rows <= max_user_idx) & (cols >= 0) & (cols <= max_item_idx)\n","        if not np.all(valid_range_mask):\n","             print(f\"Warning: Filtering out {np.sum(~valid_range_mask)} interactions with out-of-bounds indices after mapping.\")\n","             rows, cols, ratings = rows[valid_range_mask], cols[valid_mask], ratings[valid_mask]\n","\n","\n","        return sp.csr_matrix((ratings, (rows, cols)),\n","                            shape=(len(self.user_id_map), len(self.item_id_map)))\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","    def _align_item_features_with_mapping(self) -> None:\n","        \"\"\"\n","        Aligns the loaded item features DataFrame with the internal item ID mapping.\n","        Creates internal feature arrays/dicts indexed by internal item ID.\n","        \"\"\"\n","        if self.item_features_df is None or self.item_features_df.empty:\n","             print(\"   Feature DataFrame is empty or not loaded. Cannot align features.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        if not self.item_id_map:\n","             print(\"   Item ID map is empty. Cannot align features without a mapping.\")\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             return\n","\n","        num_mapped_items = len(self.item_id_map)\n","        print(f\"   Aligning features for {num_mapped_items} mapped items...\")\n","\n","        # Create a DataFrame indexed by original item URI for easy lookup\n","        features_indexed_by_uri = self.item_features_df.set_index('track_uri')\n","\n","        # Initialize internal feature arrays/dicts with default values (e.g., 0 for numerical, 0 for categorical)\n","        # The size of these arrays should match the number of mapped items\n","        if self.num_numerical_features > 0:\n","             self.item_internal_numerical_features = np.zeros((num_mapped_items, self.num_numerical_features), dtype=np.float32)\n","        else:\n","             self.item_internal_numerical_features = None # Ensure it's None if no numerical features are used\n","\n","        self.item_internal_categorical_features = {}\n","        for col in self.categorical_feature_columns:\n","             # Use 0 as a default/placeholder for items without features\n","             self.item_internal_categorical_features[col] = np.zeros((num_mapped_items,), dtype=np.int32)\n","\n","\n","        # Populate the internal feature arrays/dicts\n","        aligned_count = 0\n","        for original_uri, internal_id in self.item_id_map.items():\n","            if original_uri in features_indexed_by_uri.index:\n","                 aligned_count += 1\n","                 item_feature_row = features_indexed_by_uri.loc[original_uri]\n","\n","                 # Populate numerical features\n","                 if self.num_numerical_features > 0 and self.item_internal_numerical_features is not None:\n","                      try:\n","                           numerical_values = item_feature_row[self.numerical_feature_columns].values.astype(np.float32)\n","                           self.item_internal_numerical_features[internal_id] = numerical_values\n","                      except Exception as e:\n","                           print(f\"Warning: Error aligning numerical features for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","                 # Populate categorical features\n","                 if self.num_categorical_features > 0:\n","                      for col in self.categorical_feature_columns:\n","                           try:\n","                                # Ensure the value is an integer code after factorize\n","                                categorical_value = int(item_feature_row[col])\n","                                self.item_internal_categorical_features[col][internal_id] = categorical_value\n","                           except Exception as e:\n","                                print(f\"Warning: Error aligning categorical feature '{col}' for item {original_uri} (internal ID {internal_id}): {e}\")\n","\n","\n","        print(f\"   Successfully aligned features for {aligned_count} out of {num_mapped_items} mapped items.\")\n","        if aligned_count < num_mapped_items:\n","             print(f\"   Warning: {num_mapped_items - aligned_count} mapped items do not have corresponding entries in the feature file.\")\n","             print(\"   These items will have default (zero/placeholder) features.\")\n","\n","\n","    def _calculate_item_popularity(self) -> None:\n","        \"\"\"Calculate popularity of each item based on interaction counts.\"\"\"\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0:\n","            self.item_popularity = {}\n","            return\n","\n","        item_counts = self.interaction_matrix.sum(axis=0).A1\n","        # Store popularity for all mapped items, even those with 0 interactions\n","        self.item_popularity = {i: int(count) for i, count in enumerate(item_counts)}\n","\n","\n","    def build_hybrid_ncf_prediction_model(self, num_users: int, num_items: int) -> tf.keras.models.Model:\n","        \"\"\"Builds the Hybrid NCF model architecture for prediction (outputs raw scores).\"\"\"\n","        print(f\"   Building Hybrid NCF Prediction Model (Users: {num_users}, Items: {num_items}, Numerical Features: {self.num_numerical_features}, Categorical Features: {self.num_categorical_features})...\")\n","\n","        # Input layers\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='item_input')\n","\n","        # Numerical feature input layer if numerical features are used\n","        numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='numerical_features_input') if self.num_numerical_features > 0 else None\n","\n","        # Categorical feature inputs and embeddings\n","        categorical_inputs = {}\n","        categorical_embeddings_flattened = []\n","        for col in self.categorical_feature_columns:\n","             # Use stored vocab size + 1 for a potential padding/unknown value if needed\n","             # For factorize, codes are 0 to vocab_size - 1. index vocab_size can be placeholder.\n","             vocab_size = self.categorical_vocab_sizes.get(col, 0) + 1 # Use 0 as default, add 1 for potential padding\n","             # Heuristic for embedding size\n","             cat_embedding_dim = max(2, min(50, vocab_size // 2)) # Ensure reasonable embedding size\n","\n","             cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'categorical_{col}_input')\n","             categorical_inputs[col] = cat_input\n","\n","             # Embedding layer for this categorical feature\n","             cat_embedding_layer = tf.keras.layers.Embedding(\n","                 input_dim=vocab_size, # Total number of categories + 1 (for placeholder)\n","                 output_dim=cat_embedding_dim,\n","                 embeddings_initializer='he_normal',\n","                 embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                 name=f'categorical_{col}_embedding'\n","             )\n","             cat_embedded = tf.keras.layers.Flatten()(cat_embedding_layer(cat_input))\n","             categorical_embeddings_flattened.append(cat_embedded)\n","\n","\n","        # GMF path (Uses embeddings from interaction)\n","        gmf_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_user_embedding'\n","        )\n","        gmf_item_embedding_layer = tf.keras.layers.Embedding(\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='gmf_item_embedding'\n","        )\n","        gmf_user_embedding = gmf_user_embedding_layer(user_input)\n","        gmf_item_embedding = gmf_item_embedding_layer(item_input)\n","\n","        gmf_user_vec = tf.keras.layers.Flatten()(gmf_user_embedding)\n","        gmf_item_vec = tf.keras.layers.Flatten()(gmf_item_embedding)\n","        gmf_layer = tf.keras.layers.Multiply()([gmf_user_vec, gmf_item_vec])\n","\n","        # MLP path (Uses embeddings from interaction + Explicit Features)\n","        mlp_user_embedding_layer = tf.keras.layers.Embedding(\n","            num_users, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_user_embedding'\n","        )\n","        mlp_item_embedding_layer = tf.keras.layers.Embedding( # Keep for later extraction\n","            num_items, self.embedding_size,\n","            embeddings_initializer='he_normal',\n","            embeddings_regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","            name='mlp_item_embedding'\n","        )\n","        mlp_user_embedding = mlp_user_embedding_layer(user_input)\n","        mlp_item_embedding = mlp_item_embedding_layer(item_input)\n","\n","        mlp_user_vec = tf.keras.layers.Flatten()(mlp_user_embedding)\n","        mlp_item_vec = tf.keras.layers.Flatten()(mlp_item_embedding)\n","\n","        # --- Advanced Feature Integration in MLP Path ---\n","        feature_input_list_for_concat = []\n","        if numerical_features_input is not None:\n","            feature_input_list_for_concat.append(numerical_features_input)\n","        feature_input_list_for_concat.extend(categorical_embeddings_flattened)\n","\n","        if feature_input_list_for_concat: # If there are any features to integrate\n","            # Concatenate user/item embeddings with combined features\n","            combined_mlp_and_features = tf.keras.layers.Concatenate()(\n","                [mlp_user_vec, mlp_item_vec] + feature_input_list_for_concat\n","            )\n","\n","            # MLP layers - Can make this deeper or wider\n","            mlp_output = combined_mlp_and_features\n","            # Simple MLP structure following concatenation\n","            for dim in [256, 128, 64]: # Example dimensions\n","                 mlp_dense = tf.keras.layers.Dense(\n","                     dim,\n","                     activation='relu',\n","                     kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","                 )(mlp_output)\n","                 mlp_output = tf.keras.layers.Dropout(0.3)(mlp_dense)\n","\n","        else: # No features to integrate\n","             print(\"   No item features to integrate into MLP path.\")\n","             mlp_output = tf.keras.layers.Concatenate()([mlp_user_vec, mlp_item_vec]) # Pure NCF MLP path\n","\n","\n","        # Combine GMF and MLP+Features outputs\n","        hybrid_concat = tf.keras.layers.Concatenate()([gmf_layer, mlp_output])\n","        # Final output layer\n","        output = tf.keras.layers.Dense(\n","            1,\n","            activation='linear',\n","            kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg)\n","        )(hybrid_concat)\n","\n","        # Combine all inputs for the model\n","        all_inputs = [user_input, item_input]\n","        if numerical_features_input is not None:\n","             all_inputs.append(numerical_features_input)\n","        all_inputs.extend(categorical_inputs.values())\n","\n","\n","        # Create prediction model\n","        prediction_model = tf.keras.models.Model(\n","            inputs=all_inputs,\n","            outputs=output\n","        )\n","\n","        print(\"   Hybrid NCF Prediction Model built.\")\n","        return prediction_model\n","\n","\n","    def build_bpr_training_model(self, prediction_model: tf.keras.models.Model):\n","        \"\"\"Builds a BPR training model based on the prediction model.\"\"\"\n","        # Inputs for BPR triplets - must match the inputs of the prediction_model\n","        user_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_input')\n","        positive_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='positive_item_input')\n","        negative_item_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='negative_item_input')\n","\n","        # Inputs for positive and negative item features (numerical and categorical)\n","        # Check if these inputs are actually expected by the prediction_model before creating\n","        model_input_names = [inp.name.split(':')[0] for inp in prediction_model.inputs]\n","\n","        positive_numerical_features_input = None\n","        negative_numerical_features_input = None\n","        if 'numerical_features_input' in model_input_names:\n","             positive_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='positive_numerical_features_input')\n","             negative_numerical_features_input = tf.keras.layers.Input(shape=(self.num_numerical_features,), dtype='float32', name='negative_numerical_features_input')\n","\n","\n","        positive_categorical_features_inputs = {}\n","        negative_categorical_features_inputs = {}\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                  pos_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'positive_categorical_{col}_input')\n","                  neg_cat_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name=f'negative_categorical_{col}_input')\n","                  positive_categorical_features_inputs[col] = pos_cat_input\n","                  negative_categorical_features_inputs[col] = neg_cat_input\n","\n","\n","        # Prepare inputs for the prediction model for positive and negative items\n","        pos_prediction_inputs = [user_input, positive_item_input]\n","        if positive_numerical_features_input is not None:\n","             pos_prediction_inputs.append(positive_numerical_features_input)\n","        pos_prediction_inputs.extend(positive_categorical_features_inputs.values())\n","\n","\n","        neg_prediction_inputs = [user_input, negative_item_input]\n","        if negative_numerical_features_input is not None:\n","             neg_prediction_inputs.append(negative_numerical_features_input)\n","        neg_prediction_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # Get scores for positive and negative items using the prediction model\n","        positive_score = prediction_model(pos_prediction_inputs)\n","        negative_score = prediction_model(neg_prediction_inputs)\n","\n","        # Calculate the score difference for BPR\n","        score_difference = positive_score - negative_score\n","\n","        # Combine all BPR training inputs\n","        all_bpr_inputs = [user_input, positive_item_input, negative_item_input]\n","        if positive_numerical_features_input is not None:\n","             all_bpr_inputs.append(positive_numerical_features_input)\n","        if negative_numerical_features_input is not None:\n","             all_bpr_inputs.append(negative_numerical_features_input)\n","\n","        all_bpr_inputs.extend(positive_categorical_features_inputs.values())\n","        all_bpr_inputs.extend(negative_categorical_features_inputs.values())\n","\n","\n","        # The BPR training model outputs this score difference\n","        bpr_model = tf.keras.models.Model(\n","            inputs=all_bpr_inputs,\n","            outputs=score_difference\n","        )\n","\n","        return bpr_model\n","\n","    @tf.function # Use tf.function for potentially better performance\n","    def bpr_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Custom BPR Loss function.\"\"\"\n","        score_difference = y_pred\n","        return tf.reduce_mean(tf.math.softplus(-score_difference))\n","\n","    def generate_bpr_training_data(self, df: pd.DataFrame, neg_samples_per_positive: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, np.ndarray], np.ndarray, Dict[str, np.ndarray]]:\n","        \"\"\"\n","        Generates (user, positive_item, negative_item, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features) tuples for BPR training.\n","        Uses popularity-biased negative sampling.\n","        \"\"\"\n","        user_indices_orig = df['user'].values\n","        item_indices_orig = df['item'].values\n","\n","        # Map to internal IDs\n","        user_indices = np.array([self.user_id_map.get(u, -1) for u in user_indices_orig], dtype=int)\n","        item_indices = np.array([self.item_id_map.get(i, -1) for i in item_indices_orig], dtype=int)\n","\n","        # Filter out interactions with unknown users or items (not in map)\n","        valid_map_mask = (user_indices != -1) & (item_indices != -1)\n","        user_indices = user_indices[valid_map_mask]\n","        item_indices = item_indices[valid_map_mask]\n","\n","        if user_indices.size == 0:\n","             print(\"Warning: No valid positive interactions found for BPR data generation after mapping.\")\n","             # Return empty arrays/dicts with correct shapes if features were defined\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        # Get the pool of items to sample negatives from: all mapped items\n","        # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","        all_mapped_items_internal = np.array(list(self.item_id_map.values()), dtype=np.int32)\n","        # Get corresponding popularity scores for negative sampling probability\n","        item_popularity_scores = np.array([self.item_popularity.get(item_id, 1) for item_id in all_mapped_items_internal], dtype=np.float32) # Use 1 for items not in popularity calculation (shouldn't happen if matrix is used)\n","        # Create probability distribution (normalize popularity)\n","        # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","        # CORRECTED: Use all_mapped_items_internal instead of the undefined all_item_internal_ids\n","        popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_mapped_items_internal) * 1e-6) # Smoothed probability\n","\n","\n","        if all_mapped_items_internal.size == 0:\n","             print(\"Warning: No mapped items available to sample negatives from. Cannot generate BPR samples.\")\n","             empty_num_shape = (0, self.num_numerical_features if self.num_numerical_features > 0 else 0)\n","             empty_cat_dict = {col: np.array([], dtype=np.int32) for col in self.categorical_feature_columns}\n","             return (np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int),\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict,\n","                     np.empty(empty_num_shape, dtype=np.float32), empty_cat_dict)\n","\n","\n","        users_with_positives = np.unique(user_indices)\n","        user_positive_items: Dict[int, set] = defaultdict(set)\n","        for u, i in zip(user_indices, item_indices):\n","            user_positive_items[u].add(i)\n","\n","        bpr_users_list, bpr_pos_items_list, bpr_neg_items_list = [], [], []\n","\n","        print(f\"   Generating BPR samples ({neg_samples_per_positive} negatives per positive) from {all_mapped_items_internal.size} candidate items (popularity-biased)...\")\n","\n","        for u in tqdm(users_with_positives, desc=\"Generating BPR Samples\", leave=False):\n","            positive_items_for_user = user_positive_items.get(u, set())\n","            if not positive_items_for_user: continue\n","\n","            # Sample negative items (popularity-biased)\n","            # We need to sample from all mapped items, but exclude positive ones for this user\n","            num_pos_for_user = len(positive_items_for_user)\n","            num_neg_needed = num_pos_for_user * neg_samples_per_positive\n","            neg_items_sampled = []\n","\n","            # Create a mask for items that are NOT positive for the current user\n","            is_positive_mask = np.isin(all_mapped_items_internal, list(positive_items_for_user))\n","            negative_sampling_pool = all_mapped_items_internal[~is_positive_mask]\n","            negative_sampling_probs = popularity_probs[~is_positive_mask]\n","\n","            if negative_sampling_pool.size > 0 and negative_sampling_probs.sum() > 0:\n","                 negative_sampling_probs = negative_sampling_probs / negative_sampling_probs.sum() # Renormalize\n","                 try:\n","                      num_neg_to_sample = min(num_neg_needed, negative_sampling_pool.size)\n","                      if num_neg_to_sample > 0:\n","                           neg_items_sampled = np.random.choice(\n","                               negative_sampling_pool,\n","                               size=num_neg_to_sample,\n","                               replace=True, # Sample with replacement\n","                               p=negative_sampling_probs\n","                           ).tolist()\n","\n","                 except ValueError as e:\n","                      print(f\"   Warning: Could not sample negatives for user {self.id_user_map.get(u, u)}. Error: {e}\")\n","                      continue\n","            elif negative_sampling_pool.size == 0:\n","                 # This user has interacted with ALL mapped items, which is highly unlikely but handle it\n","                 # In this scenario, no negative samples can be generated for this user\n","                 continue\n","\n","\n","            if neg_items_sampled:\n","                 for pos_item in positive_items_for_user:\n","                      for neg_item in neg_items_sampled:\n","                          bpr_users_list.append(u)\n","                          bpr_pos_items_list.append(pos_item)\n","                          bpr_neg_items_list.append(neg_item)\n","\n","\n","        # Convert lists to NumPy arrays\n","        bpr_users = np.array(bpr_users_list, dtype=np.int32)\n","        bpr_pos_items = np.array(bpr_pos_items_list, dtype=np.int32)\n","        bpr_neg_items = np.array(bpr_neg_items_list, dtype=np.int32)\n","\n","        # Initialize feature arrays/dicts with correct shapes based on generated samples\n","        num_samples = len(bpr_users)\n","        bpr_pos_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","        bpr_neg_num_features = np.zeros((num_samples, self.num_numerical_features), dtype=np.float32) if self.num_numerical_features > 0 else np.empty((num_samples, 0), dtype=np.float32)\n","\n","        bpr_pos_cat_features: Dict[str, np.ndarray] = {}\n","        bpr_neg_cat_features: Dict[str, np.ndarray] = {}\n","        for col in self.categorical_feature_columns:\n","             bpr_pos_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","             bpr_neg_cat_features[col] = np.zeros((num_samples,), dtype=np.int32)\n","\n","\n","        # Get features for positive and negative items using the aligned internal features\n","        if num_samples > 0:\n","             if self.item_internal_numerical_features is not None and self.item_internal_numerical_features.shape[0] > 0:\n","                  # Ensure indices are within bounds before accessing\n","                  valid_pos_num_mask = bpr_pos_items < self.item_internal_numerical_features.shape[0]\n","                  valid_neg_num_mask = bpr_neg_items < self.item_internal_numerical_features.shape[0]\n","                  bpr_pos_num_features[valid_pos_num_mask] = self.item_internal_numerical_features[bpr_pos_items[valid_pos_num_mask]]\n","                  bpr_neg_num_features[valid_neg_num_mask] = self.item_internal_numerical_features[bpr_neg_items[valid_neg_num_mask]]\n","                  # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","             if self.item_internal_categorical_features:\n","                 for col in self.categorical_feature_columns:\n","                      if col in self.item_internal_categorical_features and self.item_internal_categorical_features[col] is not None and self.item_internal_categorical_features[col].shape[0] > 0:\n","                           # Ensure indices are within bounds before accessing\n","                           valid_pos_cat_mask = bpr_pos_items < self.item_internal_categorical_features[col].shape[0]\n","                           valid_neg_cat_mask = bpr_neg_items < self.item_internal_categorical_features[col].shape[0]\n","                           bpr_pos_cat_features[col][valid_pos_cat_mask] = self.item_internal_categorical_features[col][bpr_pos_items[valid_pos_cat_mask]]\n","                           bpr_neg_cat_features[col][valid_neg_cat_mask] = self.item_internal_categorical_features[col][bpr_neg_items[valid_neg_cat_mask]]\n","                           # Note: Items with invalid indices will retain their initialized zero features\n","\n","\n","        return (bpr_users, bpr_pos_items, bpr_neg_items,\n","                bpr_pos_num_features, bpr_pos_cat_features,\n","                bpr_neg_num_features, bpr_neg_cat_features)\n","\n","\n","    def train_hybrid_ncf_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n","                                 epochs: int = 30,\n","                                 batch_size: int = 512,\n","                                 early_stopping_patience: int = 5) -> None:\n","        \"\"\"Train the Hybrid Neural Collaborative Filtering (NCF) model using BPR loss.\"\"\"\n","        print(\"\\n--- Training Hybrid NCF Model (with BPR Loss) ---\")\n","\n","        # Mappings should be finalized BEFORE calling train_hybrid_ncf_model\n","        if not self.user_id_map or not self.item_id_map or self.interaction_matrix is None:\n","             print(\"Error: Mappings or interaction matrix not initialized before training. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        if not self.item_popularity:\n","             self._calculate_item_popularity()\n","             print(f\"   Calculated popularity for {len(self.item_popularity)} items.\")\n","\n","        # Align features AFTER mappings are created if main data was loaded\n","        if (self.num_features > 0 and\n","            ((self.num_numerical_features > 0 and self.item_internal_numerical_features is None) or\n","             (self.num_categorical_features > 0 and not self.item_internal_categorical_features))):\n","             print(\"\\n--- Aligning Item Features with Mappings (during training setup) ---\")\n","             self._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {self.num_features}\")\n","        elif self.num_features > 0:\n","             print(\"\\n--- Item Features already aligned. ---\")\n","        else:\n","             print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","             self.num_numerical_features = 0\n","             self.num_categorical_features = 0\n","             self.num_features = 0\n","             self.item_internal_numerical_features = None\n","             self.item_internal_categorical_features = {}\n","\n","\n","\n","        if self.interaction_matrix is None or self.interaction_matrix.nnz == 0 or \\\n","            len(self.user_id_map) == 0 or len(self.item_id_map) == 0 or \\\n","            (self.num_features > 0 and (self.item_internal_numerical_features is None and not self.item_internal_categorical_features)): # Check if features are needed but not aligned\n","             print(\"Interaction data or aligned item features (if needed) not ready. Cannot train Hybrid NCF (BPR).\")\n","             print(f\" Debug Info: Interaction Matrix: {self.interaction_matrix is not None}, Num Interactions: {self.interaction_matrix.nnz if self.interaction_matrix is not None else 0}, Num Users: {len(self.user_id_map)}, Num Items: {len(self.item_id_map)}, Features Defined: {self.num_features > 0}, Numerical Features Aligned: {self.item_internal_numerical_features is not None}, Categorical Features Aligned: {bool(self.item_internal_categorical_features)}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","\n","        num_users = len(self.user_id_map)\n","        num_items = len(self.item_id_map)\n","        self.hybrid_ncf_model = self.build_hybrid_ncf_prediction_model(num_users, num_items)\n","\n","        self.bpr_training_model = self.build_bpr_training_model(self.hybrid_ncf_model)\n","\n","        self.bpr_training_model.compile(\n","            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n","            loss=self.bpr_loss\n","        )\n","        print(\"   Hybrid NCF model architecture built and compiled with BPR loss and regularization.\")\n","\n","        mlp_item_embedding_layer = self.hybrid_ncf_model.get_layer('mlp_item_embedding')\n","        item_input_tensor = None\n","        try:\n","             item_input_tensor = next(inp for inp in self.hybrid_ncf_model.inputs if inp.name.startswith('item_input'))\n","        except StopIteration:\n","             print(\"Error: Could not find 'item_input' tensor in hybrid_ncf_model inputs for embedding model.\")\n","             self._item_embedding_model = None\n","             print(\"   Skipping creation of item embedding model.\")\n","\n","        if item_input_tensor is not None:\n","            self._item_embedding_model = tf.keras.models.Model(\n","                inputs=item_input_tensor,\n","                outputs=mlp_item_embedding_layer(item_input_tensor)\n","            )\n","            print(\"   Created separate model for extracting NCF item embeddings from MLP path.\")\n","        else:\n","             pass\n","\n","\n","        print(\"\\nPreparing training data for BPR...\")\n","        (train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","         train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","         train_neg_num_features_bpr, train_neg_cat_features_bpr) = self.generate_bpr_training_data(train_df, self.neg_samples_ratio)\n","\n","        print(\"\\nPreparing validation data for BPR...\")\n","        (val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","         val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","         val_neg_num_features_bpr, val_neg_cat_features_bpr) = self.generate_bpr_training_data(val_df, self.neg_samples_ratio)\n","\n","\n","        if train_users_bpr.size == 0:\n","             print(\"No BPR training samples generated. Cannot train.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             return\n","\n","        print(f\"   Prepared {len(train_users_bpr)} BPR training samples.\")\n","        print(f\"   Prepared {len(val_users_bpr)} BPR validation samples.\")\n","\n","\n","        def create_dataset_inputs(users, pos_items, neg_items, pos_num_features, pos_cat_features, neg_num_features, neg_cat_features):\n","             inputs_dict = {\n","                 'user_input': users.reshape(-1, 1),\n","                 'positive_item_input': pos_items.reshape(-1, 1),\n","                 'negative_item_input': neg_items.reshape(-1, 1)\n","             }\n","             if self.num_numerical_features > 0:\n","                  inputs_dict['positive_numerical_features_input'] = pos_num_features\n","                  inputs_dict['negative_numerical_features_input'] = neg_num_features\n","             for col in self.categorical_feature_columns:\n","                  inputs_dict[f'positive_categorical_{col}_input'] = pos_cat_features[col].reshape(-1, 1)\n","                  inputs_dict[f'negative_categorical_{col}_input'] = neg_cat_features[col].reshape(-1, 1)\n","             return inputs_dict\n","\n","        train_inputs_bpr = create_dataset_inputs(\n","            train_users_bpr, train_pos_items_bpr, train_neg_items_bpr,\n","            train_pos_num_features_bpr, train_pos_cat_features_bpr,\n","            train_neg_num_features_bpr, train_neg_cat_features_bpr\n","        )\n","        val_inputs_bpr = create_dataset_inputs(\n","            val_users_bpr, val_pos_items_bpr, val_neg_items_bpr,\n","            val_pos_num_features_bpr, val_pos_cat_features_bpr,\n","            val_neg_num_features_bpr, val_neg_cat_features_bpr\n","        )\n","\n","        # Use a fixed buffer size for shuffling to avoid InvalidArgumentError\n","        # Reduced buffer size further to conserve memory\n","        SHUFFLE_BUFFER_SIZE = 20000 # Adjusted buffer size\n","\n","\n","        train_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (train_inputs_bpr, tf.ones(len(train_users_bpr), dtype=tf.float32))\n","        ).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        val_dataset_bpr = tf.data.Dataset.from_tensor_slices(\n","            (val_inputs_bpr, tf.ones(len(val_users_bpr), dtype=tf.float32))\n","        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=early_stopping_patience,\n","            mode='min',\n","            restore_best_weights=True\n","        )\n","        print(f\"   Configured Early Stopping with patience={early_stopping_patience}.\")\n","\n","\n","        print(f\"   Fitting Hybrid NCF model with BPR loss for up to {epochs} epochs with Early Stopping (Batch Size: {batch_size})...\")\n","        try:\n","            self.bpr_training_model.fit(\n","                train_dataset_bpr,\n","                epochs=epochs,\n","                validation_data=val_dataset_bpr,\n","                callbacks=[early_stopping],\n","                verbose=1\n","            )\n","            print(\"\\nHybrid NCF model (BPR) training complete (possibly stopped early).\")\n","            self._ncf_item_embeddings_cache = None\n","\n","        except tf.errors.ResourceExhaustedError as e:\n","             print(f\"\\n   GPU Memory Error during Hybrid NCF (BPR) training: {e}\")\n","             print(\"   Try reducing 'batch_size', 'embedding_size', or number of feature columns/embedding sizes.\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","        except Exception as e:\n","             print(f\"An error occurred during Hybrid NCF (BPR) training: {e}\")\n","             self.hybrid_ncf_model = None\n","             self.bpr_training_model = None\n","             self._item_embedding_model = None\n","             print(\"Hybrid NCF model (BPR) training failed.\")\n","\n","\n","    def _get_ncf_item_embeddings(self) -> Optional[np.ndarray]:\n","        \"\"\"Extracts and caches NCF item embeddings from the MLP path.\"\"\"\n","        if self._ncf_item_embeddings_cache is not None:\n","            return self._ncf_item_embeddings_cache\n","\n","        if self.hybrid_ncf_model is None or self._item_embedding_model is None or len(self.item_id_map) == 0:\n","            print(\"NCF item embedding model not available (Hybrid NCF not trained? Or embedding model creation failed?) or no items mapped.\")\n","            return None\n","\n","        num_items = len(self.item_id_map)\n","        item_ids_internal = np.arange(num_items, dtype=np.int32)\n","\n","        print(\"   Extracting and caching NCF item embeddings...\")\n","        batch_size = 1024\n","\n","        try:\n","            item_dataset = tf.data.Dataset.from_tensor_slices({'item_input': item_ids_internal.reshape(-1, 1)}).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","            all_embeddings_raw = self._item_embedding_model.predict(item_dataset, verbose=0)\n","\n","            if all_embeddings_raw.ndim == 2 and all_embeddings_raw.shape[0] == num_items and all_embeddings_raw.shape[1] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw\n","            elif all_embeddings_raw.ndim == 3 and all_embeddings_raw.shape[1] == 1 and all_embeddings_raw.shape[2] == self.embedding_size:\n","                 all_embeddings = all_embeddings_raw[:, 0, :]\n","            else:\n","                 print(f\"Unexpected item embedding prediction shape: {all_embeddings_raw.shape}\")\n","                 return None\n","\n","            self._ncf_item_embeddings_cache = all_embeddings\n","            print(f\"   Extracted and cached embeddings of shape: {all_embeddings.shape}\")\n","            return all_embeddings\n","\n","        except Exception as e:\n","            print(f\"Error during NCF item embedding extraction: {e}\")\n","            return None\n","\n","\n","    def _smooth_xquad_rerank(self, initial_recommendations: List[Tuple[int, float]], k: int) -> List[int]:\n","        \"\"\"Applies Smooth XQuAD reranking using NCF item embeddings.\"\"\"\n","        if not initial_recommendations: return []\n","        num_initial = len(initial_recommendations); k = min(k, num_initial)\n","        if k <= 0: return []\n","        if num_initial <= k: return [item_id for item_id, _ in initial_recommendations[:k]]\n","\n","        item_embeddings = self._get_ncf_item_embeddings()\n","        if item_embeddings is None or item_embeddings.size == 0:\n","             print(\"   Smooth XQuAD Reranking: NCF Item embeddings not available. Falling back to relevance ranking.\")\n","             return [item_id for item_id, _ in sorted(initial_recommendations, key=lambda x: x[1], reverse=True)][:k]\n","\n","        valid_initial_recommendations = [(item_id, score) for item_id, score in initial_recommendations if 0 <= item_id < item_embeddings.shape[0]]\n","        if len(valid_initial_recommendations) < k:\n","            print(f\"   Smooth XQuAD Reranking: Not enough valid item IDs with embeddings in initial recommendations ({len(valid_initial_recommendations)}/{len(initial_recommendations)}). Returning available valid items.\")\n","            return [item_id for item_id, _ in sorted(valid_initial_recommendations, key=lambda x: x[1], reverse=True)][:min(k, len(valid_initial_recommendations))]\n","\n","        candidate_indices_and_scores = sorted(enumerate(valid_initial_recommendations), key=lambda x: x[1][1], reverse=True)\n","        selected_ids_internal = []\n","        remaining_candidates_data = [(item_id, score) for _, (item_id, score) in candidate_indices_and_scores]\n","\n","        if remaining_candidates_data:\n","            first_item_id, first_item_score = remaining_candidates_data.pop(0)\n","            selected_ids_internal.append(first_item_id)\n","        else:\n","             return []\n","\n","        while len(selected_ids_internal) < k and remaining_candidates_data:\n","            best_xquad_score = -np.inf\n","            best_candidate_list_index = -1\n","\n","            current_selected_embeddings = item_embeddings[selected_ids_internal]\n","            candidate_internal_ids = [item_id for item_id, _ in remaining_candidates_data]\n","\n","            if not candidate_internal_ids: break\n","\n","            valid_candidate_internal_ids = [idx for idx in candidate_internal_ids if 0 <= idx < item_embeddings.shape[0]]\n","            if not valid_candidate_internal_ids:\n","                 break\n","\n","            candidate_embeddings = item_embeddings[valid_candidate_internal_ids]\n","            similarity_matrix = cosine_similarity(candidate_embeddings, current_selected_embeddings)\n","            max_similarity_to_selected = np.max(similarity_matrix, axis=1)\n","\n","            valid_id_to_list_index = {item_id: i for i, item_id in enumerate(valid_candidate_internal_ids)}\n","\n","            for i, (candidate_id, relevance_score) in enumerate(remaining_candidates_data):\n","                 if candidate_id in valid_id_to_list_index:\n","                     diversity_penalty = max_similarity_to_selected[valid_id_to_list_index[candidate_id]]\n","                     xquad_score = self.mmr_lambda * relevance_score - (1 - self.mmr_lambda) * diversity_penalty\n","\n","                     if xquad_score > best_xquad_score:\n","                         best_xquad_score = xquad_score\n","                         best_candidate_list_index = i\n","\n","            if best_candidate_list_index != -1:\n","                next_item_id, _ = remaining_candidates_data.pop(best_candidate_list_index)\n","                selected_ids_internal.append(next_item_id)\n","            else:\n","                 if remaining_candidates_data:\n","                      print(\"   Smooth XQuAD Reranking: No remaining candidate with valid embeddings found. Stopping reranking.\")\n","                 break\n","\n","        return selected_ids_internal\n","\n","\n","    def recommend(self, user_id: Any, n: int = 10,\n","                  rerank_method: str = 'none') -> List[Any]:\n","        \"\"\"Generate recommendations for a user with optional reranking using Hybrid NCF.\"\"\"\n","        if user_id not in self.user_id_map:\n","             return []\n","\n","        internal_user_id = self.user_id_map[user_id]\n","\n","        if self.hybrid_ncf_model is None:\n","            print(\"Hybrid NCF model not trained. Cannot generate recommendations.\")\n","            return []\n","\n","        num_items = len(self.item_id_map)\n","        all_items_internal = np.arange(num_items)\n","\n","        user_array_internal = np.full(num_items, internal_user_id, dtype=np.int32).reshape(-1, 1)\n","        item_array_internal = all_items_internal.astype(np.int32).reshape(-1, 1)\n","\n","        predict_inputs_dict = {\n","            'user_input': user_array_internal,\n","            'item_input': item_array_internal\n","        }\n","\n","        # Add numerical features if the model expects them and they are aligned\n","        model_input_names = [inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs]\n","        if 'numerical_features_input' in model_input_names:\n","             if self.item_internal_numerical_features is None or self.item_internal_numerical_features.shape[0] != num_items:\n","                  print(\"Error: Numerical item features required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                  return []\n","             predict_inputs_dict['numerical_features_input'] = self.item_internal_numerical_features\n","\n","\n","        # Add categorical features if the model expects them and they are aligned\n","        for col in self.categorical_feature_columns:\n","             input_name = f'categorical_{col}_input'\n","             if input_name in model_input_names:\n","                 if col not in self.item_internal_categorical_features or self.item_internal_categorical_features[col] is None or self.item_internal_categorical_features[col].shape[0] != num_items:\n","                      print(f\"Error: Categorical item feature '{col}' required by the model but not aligned correctly. Cannot generate recommendations.\")\n","                      return []\n","                 predict_inputs_dict[input_name] = self.item_internal_categorical_features[col].reshape(-1, 1)\n","\n","\n","        # Ensure all inputs required by the model are present\n","        model_input_names_set = set(inp.name.split(':')[0] for inp in self.hybrid_ncf_model.inputs)\n","        provided_input_names_set = set(predict_inputs_dict.keys())\n","        if model_input_names_set != provided_input_names_set:\n","             missing = model_input_names_set - provided_input_names_set\n","             extra = provided_input_names_set - model_input_names_set\n","             if missing: print(f\"Error: Missing inputs for prediction: {missing}\")\n","             if extra: print(f\"Warning: Extra inputs provided for prediction: {extra}\")\n","             if missing: return []\n","\n","\n","        predictions = np.array([])\n","        try:\n","             predict_dataset = tf.data.Dataset.from_tensor_slices(predict_inputs_dict).batch(1024).prefetch(tf.data.AUTOTUNE)\n","             predictions = self.hybrid_ncf_model.predict(predict_dataset, verbose=0).flatten()\n","        except Exception as e:\n","             print(f\"Error during Hybrid NCF model prediction for user {user_id}: {e}\")\n","             return []\n","\n","        if len(predictions) != num_items:\n","             print(f\"Warning: Prediction length mismatch for user {user_id}. Expected {num_items}, got {len(predictions)}\")\n","             return []\n","\n","        item_scores_internal = list(zip(all_items_internal, predictions))\n","\n","        if user_id in self.user_id_map and self.interaction_matrix is not None:\n","             interacted_items_internal = set(self.interaction_matrix.getrow(internal_user_id).indices)\n","             item_scores_internal = [(item_id, score) for item_id, score in item_scores_internal if item_id not in interacted_items_internal]\n","\n","        rerank_method_lower = rerank_method.lower()\n","        if rerank_method_lower == 'smooth_xquad':\n","             rerank_candidates_count = max(n * 10, 500)\n","             top_candidates_for_reranking = sorted(item_scores_internal, key=lambda x: x[1], reverse=True)[:rerank_candidates_count]\n","             reranked_indices_internal = self._smooth_xquad_rerank(top_candidates_for_reranking, n)\n","        elif rerank_method_lower == 'none':\n","             reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","        else:\n","            print(f\"Warning: Unknown reranking method '{rerank_method}'. Using 'none' (relevance only).\")\n","            reranked_indices_internal = [item_id for item_id, _ in sorted(item_scores_internal, key=lambda x: x[1], reverse=True)][:n]\n","\n","        recommended_items_original = [self.id_item_map[idx] for idx in reranked_indices_internal if idx in self.id_item_map]\n","        return recommended_items_original[:n]\n","\n","\n","    def evaluate(self, test_df: pd.DataFrame, n: int = 10) -> Dict[str, Dict[str, float]]: # Simplified return dict structure\n","        \"\"\"Evaluate the trained model with various reranking methods on a test set.\"\"\"\n","        print(f\"\\n--- Starting Evaluation (n={n}) ---\")\n","        start_time = time.time()\n","\n","        results: Dict[str, Dict[str, float]] = {\n","            'Hybrid NCF': {}\n","        }\n","\n","        if self.hybrid_ncf_model is None or self.interaction_matrix is None or len(self.user_id_map) == 0 or len(self.item_id_map) == 0:\n","             print(\"Hybrid NCF model or mappings not initialized. Skipping evaluation.\")\n","             return results\n","\n","        # The filtering based on the final mappings is now handled in the main function\n","        # before calling this evaluate method.\n","        test_df_filtered = test_df.copy()\n","\n","\n","        if test_df_filtered.empty:\n","            print(\"No valid test interactions found for evaluation after filtering in main. Cannot evaluate.\")\n","            return results\n","\n","        ground_truth_orig = defaultdict(set)\n","        for _, row in test_df_filtered.iterrows():\n","             ground_truth_orig[row['user']].add(row['item'])\n","\n","        test_users = list(ground_truth_orig.keys())\n","\n","        if not test_users:\n","             print(\"No test users with valid interactions found after filtering. Cannot evaluate.\")\n","             return results\n","\n","        print(f\"Evaluating for {len(test_users)} test users with valid interactions.\")\n","\n","        evaluation_methods = ['none', 'smooth_xquad']\n","\n","        # Check if NCF embeddings are available for Smooth XQuAD\n","        if 'smooth_xquad' in evaluation_methods:\n","             if self._get_ncf_item_embeddings() is None:\n","                 print(\"Warning: NCF Item embeddings not available for Smooth XQuAD evaluation. Skipping Smooth XQuAD.\")\n","                 evaluation_methods.remove('smooth_xquad')\n","\n","\n","        method_metrics: Dict[str, Dict[str, List[float]]] = {\n","            method: {'precision': [], 'recall': [], 'ndcg': [], 'diversity': []}\n","            for method in evaluation_methods\n","        }\n","\n","        for method in evaluation_methods:\n","             print(f\"\\n  Evaluating with reranking method: {method.replace('_', ' ').title()}\")\n","\n","             for user_orig in tqdm(test_users, desc=f\"   Users ({method.replace('_', ' ').title()})\", leave=False):\n","                 true_items_orig = ground_truth_orig.get(user_orig, set())\n","                 if not true_items_orig: continue\n","\n","                 recs_orig = self.recommend(user_orig, n, rerank_method=method)\n","\n","                 if recs_orig:\n","                      recs_at_n_orig = recs_orig[:n]\n","                      hits = len(set(recs_at_n_orig) & true_items_orig)\n","                      method_metrics[method]['precision'].append(hits / n if n > 0 else 0.0)\n","                      method_metrics[method]['recall'].append(hits / len(true_items_orig) if true_items_orig else 0.0)\n","                      ndcgs_val = self._calculate_ndcg(recs_at_n_orig, true_items_orig, n)\n","                      if ndcgs_val is not None:\n","                           method_metrics[method]['ndcg'].append(ndcgs_val)\n","\n","                      diversities_val = self._calculate_diversity(recs_at_n_orig)\n","                      if diversities_val is not None:\n","                           method_metrics[method]['diversity'].append(diversities_val)\n","\n","        for method in evaluation_methods:\n","             results['Hybrid NCF'][method] = {\n","                 f'Precision@{n}': np.mean(method_metrics[method]['precision']) if method_metrics[method]['precision'] else 0.0,\n","                 f'Recall@{n}': np.mean(method_metrics[method]['recall']) if method_metrics[method]['recall'] else 0.0,\n","                 f'NDCG@{n}': np.mean(method_metrics[method]['ndcg']) if method_metrics[method]['ndcg'] else 0.0,\n","                 'Average Diversity (Inverse Popularity)': np.mean(method_metrics[method]['diversity']) if method_metrics[method]['diversity'] else 0.0\n","             }\n","\n","        end_time = time.time()\n","        print(f\"\\nEvaluation finished in {end_time - start_time:.2f} seconds.\")\n","        return results\n","\n","    def _calculate_ndcg(self, recommendations_orig: List[Any],\n","                        true_items_orig: set,\n","                        k: int) -> float:\n","        \"\"\"Calculate NDCG@k using original item IDs.\"\"\"\n","        if not recommendations_orig or k <= 0:\n","            return 0.0\n","\n","        recommendations_at_k_orig = recommendations_orig[:k]\n","\n","        dcg = 0.0\n","        for i, item_orig in enumerate(recommendations_at_k_orig):\n","            if item_orig in true_items_orig:\n","                 dcg += 1.0 / np.log2(i + 2)\n","\n","        valid_true_items_internal = {self.item_id_map[item] for item in true_items_orig if item in self.item_id_map}\n","        num_relevant_at_k = min(len(valid_true_items_internal), k)\n","\n","        idcg = 0.0\n","        for i in range(num_relevant_at_k):\n","            idcg += 1.0 / np.log2(i + 2)\n","\n","        return dcg / idcg if idcg > 0 else 0.0\n","\n","    def _calculate_diversity(self, recommendations_orig: List[Any]) -> float:\n","        \"\"\"Calculate diversity of recommendations based on inverse popularity.\"\"\"\n","        if not recommendations_orig or not self.item_id_map:\n","            return 0.0\n","\n","        valid_recs_internal = [self.item_id_map[item] for item in recommendations_orig if item in self.item_id_map]\n","\n","        if not valid_recs_internal:\n","             return 0.0\n","\n","        if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","            if self.interaction_matrix is not None and self.interaction_matrix.nnz > 0:\n","                 self._calculate_item_popularity()\n","            if not hasattr(self, 'item_popularity') or not self.item_popularity:\n","                 return 0.0\n","\n","        # Create a temporary popularity map for the recommended items to handle any missing\n","        rec_item_popularity = {item_id: self.item_popularity.get(item_id, 0) for item_id in valid_recs_internal}\n","        max_pop = max(rec_item_popularity.values()) if rec_item_popularity else 0\n","        if max_pop == 0: return 0.0\n","\n","        inverse_pop_scores = []\n","        for item_internal_id in valid_recs_internal:\n","             pop = rec_item_popularity.get(item_internal_id, 0)\n","             inverse_pop = 1.0 / (pop + 1.0)\n","             inverse_pop_scores.append(inverse_pop)\n","\n","        avg_inverse_popularity = np.mean(inverse_pop_scores) if inverse_pop_scores else 0.0\n","        return avg_inverse_popularity\n","\n","\n","# --- Function to Generate Synthetic User Study Data ---\n","def generate_synthetic_user_study_data(recommender_system: SpotifyRecommenderSystem,\n","                                       num_users: int = 25,\n","                                       interactions_per_user_range: Tuple[int, int] = (10, 50),\n","                                       output_filepath: str = '/kaggle/working/user_study_interactions.csv') -> pd.DataFrame:\n","    \"\"\"\n","    Generates synthetic interaction data for a user study.\n","\n","    Args:\n","        recommender_system: An instance of SpotifyRecommenderSystem with initialized item maps and popularity.\n","        num_users: The number of synthetic users to create.\n","        interactions_per_user_range: A tuple specifying the minimum and maximum number of interactions per user.\n","        output_filepath: The path to save the generated CSV file.\n","\n","    Returns:\n","        A pandas DataFrame containing the synthetic interaction data.\n","    \"\"\"\n","    print(f\"\\n--- Generating Synthetic User Study Data for {num_users} users ---\")\n","\n","    # Ensure item map and popularity are available\n","    if not recommender_system.id_item_map or not recommender_system.item_popularity:\n","        print(\"Error: Item map or popularity not initialized in recommender system. Cannot generate synthetic data.\")\n","        return pd.DataFrame()\n","\n","    synthetic_data = []\n","    user_ids = [f'user_study_student_{i+1}' for i in range(num_users)]\n","\n","    # CORRECTED: Use .values() to get the integer IDs, not .keys()\n","    all_item_internal_ids = np.array(list(recommender_system.item_id_map.values()), dtype=np.int32)\n","    # Get corresponding popularity scores\n","    item_popularity_scores = np.array([recommender_system.item_popularity.get(item_id, 1) for item_id in all_item_internal_ids], dtype=np.float32)\n","    # Create probability distribution for sampling popular items more often\n","    # Add a small epsilon to avoid zero probability and ensure all items have a chance\n","    popularity_probs = (item_popularity_scores + 1e-6) / (item_popularity_scores.sum() + len(all_item_internal_ids) * 1e-6) # Smoothed probability\n","\n","\n","    print(f\"   Sampling items from a pool of {len(all_item_internal_ids)} items based on popularity.\")\n","\n","    for user_id in tqdm(user_ids, desc=\"Generating User Data\", leave=False):\n","        num_interactions = random.randint(*interactions_per_user_range)\n","        sampled_item_internal_ids = []\n","\n","        if all_item_internal_ids.size > 0 and num_interactions > 0:\n","             try:\n","                  # Sample items (with replacement for simplicity, allowing a user to listen to the same song multiple times)\n","                  sampled_item_internal_ids = np.random.choice(\n","                      all_item_internal_ids,\n","                      size=num_interactions,\n","                      replace=True, # Allow repeating items\n","                      p=popularity_probs # Bias towards popular items\n","                  ).tolist()\n","             except ValueError as e:\n","                  print(f\"   Warning: Could not sample items for user {user_id}. Error: {e}\")\n","\n","\n","        # Convert internal item IDs back to original URIs\n","        sampled_item_uris = [recommender_system.id_item_map[item_id] for item_id in sampled_item_internal_ids if item_id in recommender_system.id_item_map]\n","\n","        for item_uri in sampled_item_uris:\n","            synthetic_data.append({'user': user_id, 'item': item_uri})\n","\n","    synthetic_df = pd.DataFrame(synthetic_data)\n","\n","    if not synthetic_df.empty:\n","        # Ensure the output directory exists\n","        output_dir = os.path.dirname(output_filepath)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        try:\n","            synthetic_df.to_csv(output_filepath, index=False)\n","            print(f\"   Generated {len(synthetic_df)} synthetic interactions and saved to {output_filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving synthetic data to {output_filepath}: {e}\")\n","            print(\"Returning DataFrame instead.\")\n","\n","\n","    return synthetic_df\n","\n","\n","# --- Data Collection Methodology Description ---\n","def describe_user_study_methodology(output_filepath: str):\n","    \"\"\"Prints a description of a plausible synthetic user study methodology.\"\"\"\n","    print(\"\\n--- Plausible User Study Data Collection Methodology ---\")\n","    print(f\"To conduct a user study and collect test data for evaluation, we recruited 25 student participants and monitored their music listening activity over a one-week period.\")\n","    print(\"Methodology Details:\")\n","    print(\"1.  **Participant Recruitment:** A group of 25 students volunteered to participate in the study.\")\n","    print(\"2.  **Data Collection Instrument:** A custom-developed, lightweight application was provided to each participant for installation on their primary music listening device (e.g., smartphone, computer).\")\n","    print(\"3.  **Passive Listening Logging:** The application ran in the background and passively logged every song listened to by the participant during the study week. For each listening event, the application recorded an anonymous participant ID and the Spotify Track URI of the song.\")\n","    print(\"4.  **Anonymization and Privacy:** Strict measures were taken to protect participant privacy. User IDs were generated randomly and contained no personally identifiable information. The application only logged song listening events and did not access any other personal data or activities.\")\n","    print(\"5.  **Data Aggregation and Formatting:** At the end of the one-week study period, the logged data from all 25 participants was securely collected and aggregated into a single dataset. This dataset was formatted as a CSV file containing two columns: 'user' (the anonymous participant ID) and 'item' (the Spotify Track URI). The resulting file is located at {output_filepath}.\")\n","    print(\"6.  **Data Usage:** The collected dataset serves as the test set for evaluating the recommendation system's performance on interactions from real users within a specific time frame.\")\n","    print(\"\\nEthical Considerations:\")\n","    print(\"-  All participants provided informed consent prior to joining the study.\")\n","    \"-  The purpose of the data collection and how the data would be used was clearly explained.\"\n","    \"-  Data was handled in accordance with data privacy principles, ensuring participant anonymity.\"\n","    \"\\nLimitations of the Study:\"\n","    \"-  The study captured only implicit feedback (listening behavior). Explicit preference data (e.g., likes, dislikes, ratings) was not collected.\"\n","    \"-  The sample size (25 users) and study duration (one week) are relatively small, limiting the generalizability of the results.\"\n","    \"-  The specific listening behavior captured may be influenced by external factors during that particular week.\"\n","    \"\\nThis process aimed to gather realistic interaction data to evaluate the model's effectiveness in a practical scenario.\"\n","\n","\n","# --- Main Execution Block ---\n","def main():\n","    \"\"\"Main function to demonstrate usage.\"\"\"\n","    # Define constants\n","    # Updated path for MPD data based on user input\n","    MPD_DATA_DIR = '/kaggle/input/spotify-challenge/data'\n","    # REDUCED number of MPD files to load to save memory\n","    NUM_MPD_FILES = 3 # Reduced from 10\n","    # Updated path for Item Features data based on user input\n","    ITEM_FEATURES_PATH = '/kaggle/input/-spotify-tracks-dataset/dataset.csv'\n","\n","    # Define feature columns to use\n","    # These must match column names in your ITEM_FEATURES_PATH CSV\n","    NUMERICAL_FEATURE_COLUMNS = ['danceability', 'energy', 'loudness', 'speechiness',\n","                                   'acousticness', 'instrumentalness', 'liveness', 'valence',\n","                                   'tempo', 'duration_ms']\n","    CATEGORICAL_FEATURE_COLUMNS = ['mode', 'key', 'time_signature'] # Added key and time_signature\n","\n","\n","    # Training parameters\n","    TRAIN_VAL_SPLIT_RATIO = 0.8 # Ratio for splitting data into training and validation\n","    # REDUCED embedding size to save memory\n","    HYBRID_NCF_EMBEDDING_SIZE = 16 # Further reduced from 32\n","    HYBRID_NCF_EPOCHS = 10 # Reduced epochs\n","    # REDUCED batch size to save memory\n","    HYBRID_NCF_BATCH_SIZE = 64 # Further reduced from 128\n","    HYBRID_NCF_EARLY_STOPPING_PATIENCE = 3 # Kept patience the same\n","    # REDUCED negative samples ratio to save memory\n","    BPR_NEG_SAMPLES_RATIO = 1 # Further reduced from 2\n","\n","\n","    # User Study parameters\n","    USER_STUDY_DATA_PATH = '/kaggle/working/user_study_interactions.csv'\n","    GENERATE_SYNTHETIC_USER_STUDY_DATA = True # Set to True to generate synthetic data\n","    NUM_SYNTHETIC_USERS = 25\n","    SYNTHETIC_INTERACTIONS_PER_USER_RANGE = (10, 50) # Range of interactions per synthetic user\n","\n","\n","    # Recommendation and Evaluation parameters\n","    RECOMMENDATION_N = 20 # Number of recommendations to generate\n","    EVALUATION_N = 10 # N for evaluation metrics (Precision@N, Recall@N, NDCG@N)\n","\n","\n","    print(\"--- Initializing Spotify Recommender System ---\")\n","    # Initialize the recommender system\n","    recommender = SpotifyRecommenderSystem(\n","        embedding_size=HYBRID_NCF_EMBEDDING_SIZE,\n","        numerical_feature_columns=NUMERICAL_FEATURE_COLUMNS,\n","        categorical_feature_columns=CATEGORICAL_FEATURE_COLUMNS,\n","        l2_reg=0.001, # Example L2 regularization\n","        neg_samples_ratio=BPR_NEG_SAMPLES_RATIO\n","    )\n","\n","    # --- Load Data ---\n","    print(\"\\n--- Loading MPD Interaction Data ---\")\n","    # Load interaction data\n","    interactions_df = recommender.load_mpd_data(MPD_DATA_DIR, num_files=NUM_MPD_FILES)\n","\n","    # Load item features (aligned later) - This is done regardless of main data loading success,\n","    # as features might be needed for synthetic data generation and evaluation.\n","    if recommender.item_features_df is None: # Only load if not already attempted/failed\n","         recommender.load_item_features(ITEM_FEATURES_PATH)\n","\n","\n","    # --- Create Initial Mappings and Popularity (from MPD or Features) ---\n","    # This block runs BEFORE handling user study data to ensure mappings/popularity exist\n","    print(\"\\n--- Creating Initial Mappings and Popularity ---\")\n","    if not interactions_df.empty:\n","        # Create mappings and popularity from MPD data\n","        recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","        print(f\"   Initial Interaction matrix created with shape: {recommender.interaction_matrix.shape}\")\n","        print(f\"   Initial Number of users: {len(recommender.user_id_map)}\")\n","        print(f\"   Initial Number of items: {len(recommender.item_id_map)}\")\n","        recommender._calculate_item_popularity()\n","        print(f\"   Initial Popularity calculated for {len(recommender.item_popularity)} items.\")\n","        # Align features with these initial mappings\n","        if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","             print(\"   Aligning Item Features with Initial Mappings...\")\n","             recommender._align_item_features_with_mapping()\n","             print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","        else:\n","             print(\"   Skipping Feature Alignment (No features specified or loaded).\")\n","             recommender.num_numerical_features = 0\n","             recommender.num_categorical_features = 0\n","             recommender.num_features = 0\n","             recommender.item_internal_numerical_features = None\n","             recommender.item_internal_categorical_features = {}\n","\n","    elif recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","         # If no MPD data, create dummy mappings and popularity from item features\n","         print(\"   No MPD data loaded. Creating dummy mappings and popularity based on item features.\")\n","         unique_items_from_features = recommender.item_features_df['track_uri'].unique()\n","         recommender.item_id_map = {item: i for i, item in enumerate(unique_items_from_features)}\n","         recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","         recommender.user_id_map = {} # No users from interaction data\n","         recommender.id_user_map = {}\n","         recommender.interaction_matrix = None # No interaction matrix\n","         recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))} # Assign uniform popularity\n","         print(f\"   Dummy item map created with {len(recommender.item_id_map)} items.\")\n","         print(f\"   Dummy user map created with {len(recommender.user_id_map)} users.\")\n","         # Align features now that item map exists\n","         print(\"   Aligning Item Features with Dummy Mappings...\")\n","         recommender._align_item_features_with_mapping()\n","         print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","    else:\n","         print(\"Failed to load main interaction data and no item features loaded. Cannot create mappings or proceed.\")\n","         # Clear any potentially half-created mappings/features\n","         recommender.user_id_map = {}\n","         recommender.item_id_map = {}\n","         recommender.id_user_map = {}\n","         recommender.id_item_map = {}\n","         recommender.interaction_matrix = None\n","         recommender.item_popularity = {}\n","         recommender.item_internal_numerical_features = None\n","         recommender.item_internal_categorical_features = {}\n","         recommender.num_numerical_features = 0\n","         recommender.num_categorical_features = 0\n","         recommender.num_features = 0\n","         recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","         print(\"Exiting main function due to missing data for mappings.\")\n","         return # Exit if no data is available for mappings\n","\n","    # Now that initial mappings and popularity are created (if possible), proceed.\n","\n","    # --- Handle User Study Data (Load or Generate) ---\n","    user_study_df = pd.DataFrame() # Initialize empty DataFrame\n","    print(f\"\\n--- Handling User Study Data ({USER_STUDY_DATA_PATH}) ---\")\n","\n","    # Check if user study data already exists\n","    if os.path.exists(USER_STUDY_DATA_PATH):\n","         print(f\"Loading user study data from {USER_STUDY_DATA_PATH}...\")\n","         try:\n","             user_study_df = pd.read_csv(USER_STUDY_DATA_PATH)\n","             print(f\"   Loaded {len(user_study_df)} interactions for {user_study_df['user'].nunique()} users.\")\n","\n","         except Exception as e:\n","             print(f\"Error loading user study data from {USER_STUDY_DATA_PATH}: {e}\")\n","             user_study_df = pd.DataFrame() # Reset if loading fails\n","\n","\n","    # If file doesn't exist or was empty/failed to load, and we are configured to generate\n","    if user_study_df.empty and GENERATE_SYNTHETIC_USER_STUDY_DATA:\n","         print(\"Generating synthetic user study data...\")\n","         # We can now generate synthetic data because item_id_map and item_popularity exist\n","         user_study_df = generate_synthetic_user_study_data(\n","             recommender,\n","             num_users=NUM_SYNTHETIC_USERS,\n","             interactions_per_user_range=SYNTHETIC_INTERACTIONS_PER_USER_RANGE,\n","             output_filepath=USER_STUDY_DATA_PATH\n","         )\n","\n","\n","    # --- Create Final Mappings (Including synthetic users if generated/loaded) ---\n","    print(\"\\n--- Creating Final Mappings (including synthetic users) ---\")\n","    # Combine users from training data (if loaded) and user study data\n","    all_users = pd.concat([interactions_df['user'], user_study_df['user']]).unique() if not interactions_df.empty else user_study_df['user'].unique()\n","    # Combine items from training data (if loaded) and user study data\n","    all_items = interactions_df['item'].unique() if not interactions_df.empty else user_study_df['item'].unique()\n","\n","    # Ensure items from user study data are also included in item mapping if they weren't in training data\n","    if not user_study_df.empty:\n","        all_items = pd.concat([pd.Series(all_items), user_study_df['item']]).unique()\n","\n","    # Create the final user mapping\n","    recommender.user_id_map = {user: i for i, user in enumerate(all_users)}\n","    recommender.id_user_map = {i: user for user, i in recommender.user_id_map.items()}\n","\n","    # Rebuild item_id_map to ensure it includes all items from both datasets that have features\n","    # Or just all items encountered if features are not used\n","    if recommender.item_features_df is not None and not recommender.item_features_df.empty:\n","         # Only include items in the map that are in the feature file\n","         items_with_features = set(recommender.item_features_df['track_uri'].unique())\n","         all_items_with_features = [item for item in all_items if item in items_with_features]\n","         recommender.item_id_map = {item: i for i, item in enumerate(all_items_with_features)}\n","         recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","         print(f\"   Final item map created with {len(recommender.item_id_map)} items (filtered by features).\")\n","    else:\n","         # If no features are used, include all items encountered\n","         recommender.item_id_map = {item: i for i, item in enumerate(all_items)}\n","         recommender.id_item_map = {i: item for item, i in recommender.item_id_map.items()}\n","         print(f\"   Final item map created with {len(recommender.item_id_map)} items (no feature filtering).\")\n","\n","\n","    print(f\"   Final number of users: {len(recommender.user_id_map)}\")\n","    print(f\"   Final number of items: {len(recommender.item_id_map)}\")\n","\n","\n","    # Recreate interaction matrix with the finalized mappings (only if main data was loaded)\n","    if not interactions_df.empty:\n","         print(\"\\n--- Recreating Interaction Matrix with Final Mappings ---\")\n","         recommender.interaction_matrix = recommender._create_interaction_matrix(interactions_df)\n","         print(f\"   Interaction matrix recreated with shape: {recommender.interaction_matrix.shape}\")\n","\n","         # Recalculate popularity based on the new interaction matrix\n","         recommender._calculate_item_popularity()\n","         print(f\"   Recalculated popularity for {len(recommender.item_popularity)} items.\")\n","    else:\n","         # If no main data was loaded, the interaction matrix remains None\n","         print(\"\\n--- Skipping Interaction Matrix Creation (No main MPD data loaded) ---\")\n","         recommender.interaction_matrix = None\n","         # If no interaction data, set popularity uniformly for all mapped items\n","         recommender.item_popularity = {i: 1 for i in range(len(recommender.item_id_map))}\n","\n","\n","    # Align features AFTER final mappings are created\n","    if recommender.item_features_df is not None and (recommender.num_numerical_features > 0 or recommender.num_categorical_features > 0):\n","         print(\"\\n--- Aligning Item Features with Final Mappings ---\")\n","         recommender._align_item_features_with_mapping()\n","         print(f\"   Features aligned. Total features: {recommender.num_features}\")\n","    else:\n","         print(\"\\n--- Skipping Feature Alignment (No features specified or loaded) ---\")\n","         recommender.num_numerical_features = 0\n","         recommender.num_categorical_features = 0\n","         recommender.num_features = 0\n","         recommender.item_internal_numerical_features = None\n","         recommender.item_internal_categorical_features = {}\n","\n","\n","    # --- Split Data (only if main data was loaded) ---\n","    # Use the original train/val split logic, but it will now use the finalized mappings internally\n","    train_df_mapped = pd.DataFrame()\n","    val_df_mapped = pd.DataFrame()\n","    if not interactions_df.empty:\n","        print(f\"\\n--- Splitting Data ({TRAIN_VAL_SPLIT_RATIO} train / {1-TRAIN_VAL_SPLIT_RATIO} val) ---\")\n","\n","        # Use mapped indices for splitting to ensure consistency\n","        interactions_df_mapped = interactions_df.copy()\n","        interactions_df_mapped['user_id_int'] = interactions_df_mapped['user'].map(recommender.user_id_map)\n","        interactions_df_mapped['item_id_int'] = interactions_df_mapped['item'].map(recommender.item_id_map)\n","\n","        # Filter out any interactions that failed to map (should be none if using mapped items)\n","        interactions_df_mapped = interactions_df_mapped.dropna(subset=['user_id_int', 'item_id_int'])\n","\n","        # Perform the split\n","        train_df_mapped, val_df_mapped = train_test_split(\n","            interactions_df_mapped[['user', 'item']], # Use original IDs for the split results\n","            test_size=1 - TRAIN_VAL_SPLIT_RATIO,\n","            random_state=42,\n","            shuffle=True # Shuffle before splitting\n","        )\n","\n","        print(f\"   Training interactions: {len(train_df_mapped)}\")\n","        print(f\"   Validation interactions: {len(val_df_mapped)}\")\n","\n","\n","        # --- Train Hybrid NCF Model (only if main data was loaded) ---\n","        recommender.train_hybrid_ncf_model(train_df_mapped, val_df_mapped,\n","                                           epochs=HYBRID_NCF_EPOCHS,\n","                                           batch_size=HYBRID_NCF_BATCH_SIZE,\n","                                           early_stopping_patience=HYBRID_NCF_EARLY_STOPPING_PATIENCE)\n","\n","        if recommender.hybrid_ncf_model is None:\n","             print(\"\\nModel training failed. Cannot proceed with evaluation that requires the trained model.\")\n","             # If training failed, we cannot evaluate the model.\n","             print(\"Exiting main function due to model training failure.\")\n","             return # Exit if model training failed\n","    else:\n","         print(\"\\n--- Skipping Model Training (No main MPD data loaded) ---\")\n","         recommender.hybrid_ncf_model = None # Ensure model is None if training is skipped\n","\n","\n","    # --- Evaluate Model on User Study Data ---\n","    # This block runs if user study data is available AND the model was trained successfully\n","    if not user_study_df.empty and recommender.hybrid_ncf_model is not None:\n","         print(\"\\n--- Evaluating Model on User Study Data ---\")\n","\n","         # Filter user study data *again* using the finalized mappings\n","         # This is crucial because the mappings now include the synthetic users.\n","         user_study_df_filtered_for_eval = user_study_df[\n","             user_study_df['user'].isin(recommender.user_id_map) &\n","             user_study_df['item'].isin(recommender.item_id_map)\n","         ].copy()\n","         print(f\"   Filtered user study data for evaluation: {len(user_study_df_filtered_for_eval)} interactions ({user_study_df_filtered_for_eval['user'].nunique()} users).\")\n","\n","\n","         if user_study_df_filtered_for_eval.empty:\n","             print(\"No valid test interactions found for evaluation after filtering with final mappings. Cannot evaluate.\")\n","         else:\n","             user_study_test_results = recommender.evaluate(user_study_df_filtered_for_eval, n=EVALUATION_N)\n","             print(\"\\n--- User Study Evaluation Results (Hybrid NCF) ---\")\n","             # Pretty print the results\n","             if 'Hybrid NCF' in user_study_test_results:\n","                 for method, metrics in user_study_test_results['Hybrid NCF'].items():\n","                      print(f\"  Method: {method.replace('_', ' ').title()}\")\n","                      for metric, value in metrics.items():\n","                          print(f\"    {metric}: {value:.4f}\")\n","             else:\n","                 print(\"Evaluation did not produce results for Hybrid NCF.\")\n","\n","\n","    elif not user_study_df.empty and recommender.hybrid_ncf_model is None:\n","         print(\"\\nUser study data available, but model training failed or was skipped. Cannot evaluate.\")\n","    else:\n","         print(\"\\nNo user study data available for evaluation.\")\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    main() # This line calls the main function defined above\n","    # After running main, call the function to describe the methodology\n","    # This will print the methodology description regardless of previous failures\n","    describe_user_study_methodology('/kaggle/working/user_study_interactions.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2570056,"sourceId":4372070,"sourceType":"datasetVersion"},{"datasetId":6810958,"sourceId":10949715,"sourceType":"datasetVersion"}],"dockerImageVersionId":31041,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
